{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773debcd",
   "metadata": {},
   "source": [
    "# Évaluation d'un système de recommandation My Content\n",
    "\n",
    "Notebook pour entraîner et comparer plusieurs approches de recommandation sur le dataset Kaggle **news-portal-user-interactions-by-globocom**. L'objectif est de montrer clairement chaque étape (du chargement des données jusqu'au choix final du modèle).\n",
    "\n",
    "> Ce notebook aligne désormais **toutes les approches de recommandation sur la bibliothèque Surprise** (https://surprise.readthedocs.io/) afin de bénéficier d'algorithmes collaboratifs standardisés et faciles à déployer."
   ]
  },
  {
   "cell_type": "code",
   "id": "43aa8131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:53.682258Z",
     "iopub.status.busy": "2025-12-17T10:07:53.681987Z",
     "iopub.status.idle": "2025-12-17T10:07:55.146794Z",
     "shell.execute_reply": "2025-12-17T10:07:55.145979Z"
    }
   },
   "source": [
    "# Imports & Config\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Ensure the project root is importable\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"clicks_dir\": \"../data/news-portal-user-interactions-by-globocom/clicks\",\n",
    "    \"metadata_path\": \"../data/news-portal-user-interactions-by-globocom/articles_metadata.csv\",\n",
    "    \"embeddings_path\": \"../data/news-portal-user-interactions-by-globocom/articles_embeddings.pickle\",\n",
    "    \"max_click_files\": 30,\n",
    "    \"artifacts_dir\": \"../artifacts/evaluation\",\n",
    "    \"k\": 5,\n",
    "    \"random_seed\": 42,\n",
    "    \"min_user_interactions\": 3,\n",
    "}\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "Path(CONFIG[\"artifacts_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config ready\", CONFIG)\n",
    "\n",
    "# Context columns provided in the clicks dataset\n",
    "CONTEXT_COLUMNS = [\n",
    "    \"click_environment\",\n",
    "    \"click_deviceGroup\",\n",
    "    \"click_os\",\n",
    "    \"click_country\",\n",
    "    \"click_region\",\n",
    "    \"click_referrer_type\",\n",
    "]\n",
    "\n",
    "from surprise import Dataset, Reader, KNNBasic, NormalPredictor, SVDpp\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc6be6a7",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Nous voulons proposer à chaque lecteur un Top-5 d'articles susceptibles de l'intéresser. Le notebook illustre la démarche de A à Z : préparation des données, construction de différentes familles de modèles puis comparaison à l'aide de métriques de ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8cfa",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Les fichiers attendus sont situés dans `/data/*`."
   ]
  },
  {
   "cell_type": "code",
   "id": "116743e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.150007Z",
     "iopub.status.busy": "2025-12-17T10:07:55.149650Z",
     "iopub.status.idle": "2025-12-17T10:07:55.178749Z",
     "shell.execute_reply": "2025-12-17T10:07:55.177740Z"
    }
   },
   "source": [
    "\n",
    "# Load data utilities\n",
    "\n",
    "\n",
    "def detect_timestamp_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the timestamp-like column name.\"\"\"\n",
    "    candidates = [\"click_timestamp\", \"timestamp\", \"event_time\", \"ts\", \"time\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates or col.lower() in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No timestamp-like column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def detect_article_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the article/item column name.\"\"\"\n",
    "    candidates = [\"click_article_id\", \"clicked_article_id\", \"article_id\", \"item_id\", \"content_id\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No article id column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def infer_unix_unit(values: pd.Series) -> str:\n",
    "    numeric = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
    "    if numeric.empty:\n",
    "        return \"s\"\n",
    "    max_abs = numeric.abs().max()\n",
    "    if max_abs >= 1e14:\n",
    "        return \"ns\"\n",
    "    if max_abs >= 1e11:\n",
    "        return \"ms\"\n",
    "    return \"s\"\n",
    "\n",
    "\n",
    "def to_timestamp(series: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return pd.to_datetime(series)\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        unit = infer_unix_unit(series)\n",
    "        return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "    converted = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if converted.notna().any():\n",
    "        return converted\n",
    "\n",
    "    unit = infer_unix_unit(series)\n",
    "    return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def list_click_files(path: Union[str, Path]) -> List[Path]:\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.is_file():\n",
    "        return [path_obj]\n",
    "    if path_obj.is_dir():\n",
    "        return sorted(path_obj.glob(\"clicks_hour_*.csv\"))\n",
    "    return []\n",
    "\n",
    "\n",
    "def ensure_context_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure session_size and context columns exist with safe defaults.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"session_size\" not in df.columns:\n",
    "        df[\"session_size\"] = 1\n",
    "    for col in CONTEXT_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_synthetic_clicks(path: str, n_users: int = 50, n_items: int = 120, days: int = 30, interactions_per_user: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"Create a small synthetic clicks dataset to keep the notebook runnable.\"\"\"\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "    start = pd.Timestamp(\"2022-01-01\")\n",
    "    envs = [\"web\", \"app\"]\n",
    "    devices = [\"mobile\", \"desktop\"]\n",
    "    oss = [\"ios\", \"android\", \"linux\"]\n",
    "    referrers = [\"direct\", \"search\", \"social\"]\n",
    "    records = []\n",
    "    for user in range(1, n_users + 1):\n",
    "        offsets = rng.integers(0, days, size=interactions_per_user)\n",
    "        timestamps = [start + pd.Timedelta(int(o), unit=\"D\") for o in sorted(offsets.tolist())]\n",
    "        articles = rng.integers(1, n_items + 1, size=interactions_per_user)\n",
    "        for ts, art in zip(timestamps, articles):\n",
    "            records.append({\n",
    "                \"user_id\": int(user),\n",
    "                \"article_id\": int(art),\n",
    "                \"timestamp\": ts,\n",
    "                \"session_size\": int(rng.integers(1, 6)),\n",
    "                \"click_environment\": rng.choice(envs),\n",
    "                \"click_deviceGroup\": rng.choice(devices),\n",
    "                \"click_os\": rng.choice(oss),\n",
    "                \"click_country\": rng.choice([\"fr\", \"us\", \"br\"]),\n",
    "                \"click_region\": rng.choice([\"idf\", \"sp\", \"ca\"]),\n",
    "                \"click_referrer_type\": rng.choice(referrers),\n",
    "            })\n",
    "    df = pd.DataFrame(records).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\n",
    "        f\"Synthetic clicks dataset created at {path} \"\n",
    "        f\"(users={n_users}, items={n_items}, interactions={len(df)})\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_clicks(path: str, max_files: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load clicks data from the Globo hourly files, with a safety cap.\"\"\"\n",
    "    files = list_click_files(path)\n",
    "    total_files = len(files)\n",
    "    if not files:\n",
    "        print(f\"Clicks directory not found at {path}. Generating a synthetic sample for demonstration.\")\n",
    "        return ensure_context_columns(create_synthetic_clicks(Path(path) / \"clicks_hour_000.csv\"))\n",
    "\n",
    "    if max_files is not None:\n",
    "        print(f\"Limite explicite max_files={max_files}, total détecté={total_files}\")\n",
    "        files = files[:max_files]\n",
    "\n",
    "    print(f\"Chargement de {len(files)} fichiers clicks (total détecté={total_files}, limite={max_files if max_files is not None else 'aucune'})\")\n",
    "    frames = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        ts_col = detect_timestamp_column(df)\n",
    "        article_col = detect_article_column(df)\n",
    "        df[ts_col] = to_timestamp(df[ts_col])\n",
    "        df = df.rename(columns={ts_col: \"timestamp\", article_col: \"article_id\"})\n",
    "        df = ensure_context_columns(df)\n",
    "        keep_cols = [col for col in [\n",
    "            \"user_id\",\n",
    "            \"article_id\",\n",
    "            \"timestamp\",\n",
    "            \"session_size\",\n",
    "            *CONTEXT_COLUMNS,\n",
    "        ] if col in df.columns]\n",
    "        frames.append(df[keep_cols])\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined = combined.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"Clicks agrégés : {len(combined)} lignes, {combined['user_id'].nunique()} utilisateurs uniques, {combined['article_id'].nunique()} articles uniques.\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_metadata(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load article metadata if available.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Metadata file not found at {path}. Utilisation du pipeline Surprise uniquement si les métadonnées sont absentes.\")\n",
    "        return None\n",
    "    meta = pd.read_csv(path)\n",
    "    if \"article_id\" not in meta.columns:\n",
    "        print(\"Metadata missing 'article_id' column. Ignoring metadata.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "\n",
    "clicks = load_clicks(CONFIG[\"clicks_dir\"], max_files=CONFIG[\"max_click_files\"])\n",
    "metadata = load_metadata(CONFIG[\"metadata_path\"])\n",
    "print(clicks.head())\n",
    "print(\"Metadata loaded:\", metadata is not None)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eab18f0f",
   "metadata": {},
   "source": [
    "## Analyse exploratoire des données\n",
    "\n",
    "Courte photographie des fichiers sources immédiatement après le chargement :\n",
    "- nombre de lignes et noms de colonnes des clics\n",
    "- volumes et intégrité des métadonnées articles\n",
    "- dimensions et structure du fichier d'`articles_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "id": "45108db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.182450Z",
     "iopub.status.busy": "2025-12-17T10:07:55.182242Z",
     "iopub.status.idle": "2025-12-17T10:07:55.260458Z",
     "shell.execute_reply": "2025-12-17T10:07:55.259678Z"
    }
   },
   "source": [
    "# EDA rapide sur les données sources\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "\n",
    "\n",
    "def summarize_timestamps(series: pd.Series):\n",
    "    series = pd.to_datetime(series)\n",
    "    daily = series.dt.date.value_counts().sort_index().rename_axis(\"date\").reset_index(name=\"nb_clicks\")\n",
    "    hourly = series.dt.hour.value_counts().sort_index().rename_axis(\"hour\").reset_index(name=\"nb_clicks\")\n",
    "    return series.min(), series.max(), daily, hourly\n",
    "\n",
    "\n",
    "def describe_structure(obj, prefix=\"embeddings\", max_depth=4):\n",
    "    entries = []\n",
    "\n",
    "    def add_entry(path, value, note=None):\n",
    "        entry = {\"chemin\": path, \"type\": type(value).__name__}\n",
    "        if hasattr(value, \"shape\"):\n",
    "            entry[\"shape\"] = tuple(getattr(value, \"shape\"))\n",
    "        elif hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)):\n",
    "            entry[\"len\"] = len(value)\n",
    "        if hasattr(value, \"dtype\"):\n",
    "            entry[\"dtype\"] = str(getattr(value, \"dtype\"))\n",
    "        if note:\n",
    "            entry[\"note\"] = note\n",
    "        if isinstance(value, np.ndarray) and value.dtype.names:\n",
    "            entry[\"dtype_fields\"] = list(value.dtype.names)\n",
    "        if isinstance(value, np.ndarray) and value.ndim == 1 and len(value) > 0 and not isinstance(value[0], (np.ndarray, list, tuple, Mapping)):\n",
    "            entry[\"exemple\"] = repr(value[:3].tolist())\n",
    "        entries.append(entry)\n",
    "\n",
    "    def walk(value, path, depth):\n",
    "        add_entry(path, value)\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        if isinstance(value, Mapping):\n",
    "            for k, v in value.items():\n",
    "                walk(v, f\"{path}.{k}\", depth + 1)\n",
    "        elif isinstance(value, (list, tuple, np.ndarray)) and not isinstance(value, (str, bytes)):\n",
    "            if len(value) > 0:\n",
    "                walk(value[0], f\"{path}[0]\", depth + 1)\n",
    "\n",
    "    walk(obj, prefix, 0)\n",
    "    return entries\n",
    "\n",
    "\n",
    "click_files = list_click_files(CONFIG[\"clicks_dir\"])\n",
    "print(f\"Nombre total de fichiers clicks détectés: {len(click_files)}\")\n",
    "if not click_files:\n",
    "    print(\"Aucun fichier clicks trouvé au chemin configuré. Vérifiez le téléchargement des données.\")\n",
    "\n",
    "files_for_eda = click_files[:2]\n",
    "per_file_stats = []\n",
    "for file in files_for_eda:\n",
    "    df_file = pd.read_csv(file)\n",
    "    ts_col = detect_timestamp_column(df_file)\n",
    "    article_col = detect_article_column(df_file)\n",
    "    timestamps = to_timestamp(df_file[ts_col])\n",
    "    per_file_stats.append(\n",
    "        {\n",
    "            \"fichier\": file.name,\n",
    "            \"nb_lignes\": len(df_file),\n",
    "            \"colonnes\": \", \".join(df_file.columns),\n",
    "            \"articles_uniques\": df_file[article_col].nunique(),\n",
    "            \"horodatage_min\": timestamps.min(),\n",
    "            \"horodatage_max\": timestamps.max(),\n",
    "        }\n",
    "    )\n",
    "if per_file_stats:\n",
    "    display(pd.DataFrame(per_file_stats))\n",
    "else:\n",
    "    print(\"Pas assez de fichiers pour réaliser une EDA détaillée par fichier.\")\n",
    "\n",
    "print(\"=== Clicks (agrégés) ===\")\n",
    "if clicks.empty:\n",
    "    print(\"Aucun clic chargé. Vérifier le chemin ou augmenter max_click_files.\")\n",
    "else:\n",
    "    clicks_summary = {\n",
    "        \"nb_lignes\": len(clicks),\n",
    "        \"colonnes\": \", \".join(clicks.columns),\n",
    "        \"utilisateurs_uniques\": clicks['user_id'].nunique() if 'user_id' in clicks else None,\n",
    "        \"articles_uniques\": clicks['article_id'].nunique() if 'article_id' in clicks else None,\n",
    "    }\n",
    "    display(pd.DataFrame([clicks_summary]))\n",
    "\n",
    "    total_articles = None\n",
    "    if metadata is not None and 'article_id' in metadata:\n",
    "        total_articles = metadata['article_id'].nunique()\n",
    "    elif 'article_id' in clicks:\n",
    "        total_articles = clicks['article_id'].nunique()\n",
    "\n",
    "    total_clients = clicks['user_id'].nunique() if 'user_id' in clicks else None\n",
    "    print(\"Synthèse globale (articles / clients)\")\n",
    "    display(pd.DataFrame([{\n",
    "        'nombre_total_articles': total_articles,\n",
    "        'nombre_total_clients': total_clients,\n",
    "    }]))\n",
    "\n",
    "    ts_min, ts_max, daily, hourly = summarize_timestamps(clicks['timestamp'])\n",
    "    display(pd.DataFrame([\n",
    "        {\n",
    "            'horodatage_min': ts_min,\n",
    "            'horodatage_max': ts_max,\n",
    "            'fenetre_jours': (ts_max - ts_min).days + 1,\n",
    "        }\n",
    "    ]))\n",
    "    print(\"Répartition par jour (jusqu'à 10 premières valeurs)\")\n",
    "    display(daily.head(10))\n",
    "    print(\"Répartition par heure (0-23)\")\n",
    "    display(hourly)\n",
    "\n",
    "print(\"=== Métadonnées des articles ===\")\n",
    "if metadata is None:\n",
    "    print(\"Aucun fichier metadata chargé.\")\n",
    "else:\n",
    "    meta_summary = {\n",
    "        \"nb_articles\": len(metadata),\n",
    "        \"colonnes\": \", \".join(metadata.columns),\n",
    "        \"articles_uniques\": metadata['article_id'].nunique() if 'article_id' in metadata else None,\n",
    "    }\n",
    "    display(pd.DataFrame([meta_summary]))\n",
    "    missing = metadata.isna().sum().sort_values(ascending=False)\n",
    "    display(missing.to_frame('valeurs_manquantes'))\n",
    "    if 'created_at_ts' in metadata.columns:\n",
    "        created = to_timestamp(metadata['created_at_ts'])\n",
    "        display(pd.DataFrame([{'premier_article': created.min(), 'dernier_article': created.max()}]))\n",
    "    if 'article_id' in metadata.columns:\n",
    "        overlap = set(clicks['article_id'].unique()) if 'article_id' in clicks.columns else set()\n",
    "        coverage = len(overlap & set(metadata['article_id'].unique()))\n",
    "        print(f\"Articles présents dans clicks et metadata: {coverage}\")\n",
    "\n",
    "\n",
    "print(\"=== Embeddings d'articles ===\")\n",
    "embeddings_path = Path(CONFIG['embeddings_path'])\n",
    "if embeddings_path.exists():\n",
    "    with embeddings_path.open('rb') as f:\n",
    "        embeddings_obj = pickle.load(f)\n",
    "    print(f\"Type chargé: {type(embeddings_obj)}\")\n",
    "\n",
    "    def summarize_matrix(mat):\n",
    "        stats = {\n",
    "            'shape': getattr(mat, 'shape', None),\n",
    "            'dtype': getattr(mat, 'dtype', None),\n",
    "        }\n",
    "\n",
    "        dim_values = []\n",
    "        shape = getattr(mat, 'shape', None)\n",
    "        if shape is not None and len(shape) >= 2:\n",
    "            dim_values.append(shape[1])\n",
    "        elif isinstance(mat, (list, tuple, np.ndarray)):\n",
    "            for row in mat:\n",
    "                if hasattr(row, '__len__') and not isinstance(row, (str, bytes)):\n",
    "                    try:\n",
    "                        dim_values.append(len(row))\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "\n",
    "        if dim_values:\n",
    "            stats.update({\n",
    "                'profondeur_min': min(dim_values),\n",
    "                'profondeur_moyenne': float(np.mean(dim_values)),\n",
    "                'profondeur_max': max(dim_values),\n",
    "            })\n",
    "\n",
    "        if hasattr(mat, 'shape') and len(getattr(mat, 'shape', [])) == 2:\n",
    "            norms = np.linalg.norm(mat, axis=1)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'nb_vectors': mat.shape[0],\n",
    "                    'dim': mat.shape[1],\n",
    "                    'norm_min': norms.min(),\n",
    "                    'norm_max': norms.max(),\n",
    "                    'norm_moyenne': norms.mean(),\n",
    "                }\n",
    "            )\n",
    "        return stats\n",
    "\n",
    "    base_structure = describe_structure(embeddings_obj, max_depth=4)\n",
    "\n",
    "    if isinstance(embeddings_obj, dict):\n",
    "        keys = list(embeddings_obj.keys())\n",
    "        print(f\"Clés disponibles: {keys}\")\n",
    "        matrix = embeddings_obj.get('embeddings')\n",
    "        ids = embeddings_obj.get('articles_ids') or embeddings_obj.get('article_ids')\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_ids',\n",
    "                'type': type(ids).__name__,\n",
    "                'len': len(ids),\n",
    "                'note': \"Identifiants d'articles fournis dans le fichier\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if matrix is not None:\n",
    "            stats = summarize_matrix(matrix)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'colonnes': \", \".join(keys),\n",
    "                    'nb_articles_ids': len(ids) if ids is not None else None,\n",
    "                    'ids_uniques': len(set(ids)) if ids is not None else None,\n",
    "                    'couverture_metadata': len(set(ids) & set(metadata['article_id']))\n",
    "                    if (metadata is not None and ids is not None and 'article_id' in metadata)\n",
    "                    else None,\n",
    "                    'couverture_clicks': len(set(ids) & set(clicks['article_id']))\n",
    "                    if (not clicks.empty and ids is not None and 'article_id' in clicks)\n",
    "                    else None,\n",
    "                }\n",
    "            )\n",
    "            display(pd.DataFrame([stats]))\n",
    "\n",
    "            if ids is not None:\n",
    "                sample_ids = ids[:5] if len(ids) >= 5 else ids\n",
    "                print(\"Aperçu des premiers article_id liés aux embeddings:\")\n",
    "                display(pd.DataFrame({'article_id': sample_ids}))\n",
    "\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, matrix.shape[1] if hasattr(matrix, 'shape') else 0))]\n",
    "            if preview_cols:\n",
    "                preview = pd.DataFrame(matrix[:5, : len(preview_cols)], columns=preview_cols)\n",
    "                if ids is not None:\n",
    "                    preview.insert(0, 'article_id', ids[: len(preview)])\n",
    "                print(\"Aperçu des embeddings (quelques colonnes et premières lignes):\")\n",
    "                display(preview)\n",
    "                print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "                print(\", \".join(preview.columns))\n",
    "\n",
    "                if ids is not None and metadata is not None and 'article_id' in metadata:\n",
    "                    meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                    meta_sample = (\n",
    "                        preview[['article_id']]\n",
    "                        .merge(metadata[['article_id'] + meta_cols], on='article_id', how='left')\n",
    "                    )\n",
    "                    if 'created_at_ts' in meta_sample.columns:\n",
    "                        meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                    print(\"Exemple de liaison embedding -> metadata sur article_id (5 premières lignes):\")\n",
    "                    display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Aucune matrice d'embeddings explicite trouvée dans l'objet chargé.\")\n",
    "    elif hasattr(embeddings_obj, 'shape'):\n",
    "        stats = summarize_matrix(embeddings_obj)\n",
    "\n",
    "        inferred_ids = None\n",
    "        mapping_note = None\n",
    "        if metadata is not None and 'article_id' in metadata and hasattr(embeddings_obj, 'shape'):\n",
    "            if embeddings_obj.shape[0] == len(metadata):\n",
    "                inferred_ids = metadata['article_id'].reset_index(drop=True)\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id explicite fourni ; association supposée alignée sur l'ordre des metadata.\"\n",
    "                )\n",
    "            else:\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id dans le fichier d'embeddings et la taille ne correspond pas aux metadata : \"\n",
    "                    f\"{embeddings_obj.shape[0]} vecteurs vs {len(metadata)} lignes de metadata.\"\n",
    "                )\n",
    "        else:\n",
    "            mapping_note = (\n",
    "                \"Aucun identifiant d'article n'est présent dans le fichier d'embeddings (mapping externe requis).\"\n",
    "            )\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if inferred_ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_id (inféré)',\n",
    "                'type': type(inferred_ids).__name__,\n",
    "                'len': len(inferred_ids),\n",
    "                'note': \"Alignement supposé sur metadata.article_id (index identique).\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if mapping_note:\n",
    "            print(mapping_note)\n",
    "\n",
    "        if inferred_ids is not None:\n",
    "            stats.update(\n",
    "                {\n",
    "                    'ids_source': 'metadata.article_id (alignement par index)',\n",
    "                    'ids_uniques': inferred_ids.nunique(),\n",
    "                    'couverture_metadata': len(set(inferred_ids) & set(metadata['article_id'])),\n",
    "                    'couverture_clicks': len(set(inferred_ids) & set(clicks['article_id'])) if not clicks.empty else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        display(pd.DataFrame([stats]))\n",
    "        if len(getattr(embeddings_obj, 'shape', [])) >= 2 and embeddings_obj.shape[1] > 0:\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, embeddings_obj.shape[1]))]\n",
    "            preview = pd.DataFrame(embeddings_obj[:5, : len(preview_cols)], columns=preview_cols)\n",
    "            if inferred_ids is not None:\n",
    "                preview.insert(0, 'article_id', inferred_ids.iloc[: len(preview)].values)\n",
    "            print(\"Aperçu direct de la matrice d'embeddings:\")\n",
    "            display(preview)\n",
    "            print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "            print(\", \".join(preview.columns))\n",
    "\n",
    "            if inferred_ids is not None and metadata is not None:\n",
    "                meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                meta_sample = preview[['article_id']].merge(\n",
    "                    metadata[['article_id'] + meta_cols], on='article_id', how='left'\n",
    "                )\n",
    "                if 'created_at_ts' in meta_sample.columns:\n",
    "                    meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                print(\"Exemple de liaison embedding -> metadata sur article_id (inféré):\")\n",
    "                display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Objet chargé non structuré, utilisez type/len pour investiguer.\")\n",
    "else:\n",
    "    print(f\"Fichier d'embeddings introuvable à {embeddings_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bac08d578ddc9073",
   "metadata": {},
   "source": [
    "# Article Embeddings\n",
    "\n",
    "Ce fichier contient les **embeddings des articles**, c’est-à-dire une **représentation numérique du contenu textuel** permettant de comparer les articles entre eux sur le plan sémantique.\n",
    "\n",
    "* **Format** : matrice NumPy `(N, 250)` en `float32`\n",
    "* **1 ligne = 1 article**\n",
    "* **250 colonnes = dimensions latentes**\n",
    "* Les valeurs individuelles n’ont pas de signification directe\n",
    "\n",
    "L’`article_id` n’est **pas stocké explicitement** : il est **déduit de l’ordre des lignes**, qui doit rester aligné avec les métadonnées des articles.\n",
    "\n",
    "La variable `words_count` indique le **nombre de mots du texte source** et sert uniquement d’indicateur de qualité du contenu.\n",
    "\n",
    "Les embeddings **ne sont pas normalisés** : la **similarité cosinus** est la mesure recommandée pour comparer les articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données et split temporel\n",
    "\n",
    "Les splits sont réalisés **par utilisateur**, en conservant l'ordre chronologique :\n",
    "- historique utilisateur ordonné\n",
    "- 1 interaction la plus récente pour le test\n",
    "- 1 interaction juste avant pour la validation\n",
    "- le reste pour l'entraînement\n",
    "\n",
    "Cette stratégie garantit l'absence de fuite d'information tout en restant robuste pour des historiques courts.\n"
   ],
   "id": "c856514aa2797fb1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data preparation utilities\n",
    "\n",
    "def clean_interactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"user_id\"] = pd.to_numeric(df[\"user_id\"], errors=\"coerce\")\n",
    "    df[\"article_id\"] = pd.to_numeric(df[\"article_id\"], errors=\"coerce\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"user_id\", \"article_id\", \"timestamp\"])\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(int)\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_user_histories(df: pd.DataFrame) -> Dict[int, List[int]]:\n",
    "    return (\n",
    "        df.sort_values([\"user_id\", \"timestamp\"])\n",
    "        .groupby(\"user_id\")[\"article_id\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "def temporal_split_per_user(df: pd.DataFrame, min_interactions: int = 3) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_rows = []\n",
    "    val_rows = []\n",
    "    test_rows = []\n",
    "    for user_id, group in df.groupby(\"user_id\"):\n",
    "        group = group.sort_values(\"timestamp\")\n",
    "        if len(group) < min_interactions:\n",
    "            continue\n",
    "        test_rows.append(group.iloc[-1])\n",
    "        val_rows.append(group.iloc[-2])\n",
    "        if len(group) > 2:\n",
    "            train_rows.append(group.iloc[:-2])\n",
    "    train_df = pd.concat(train_rows, ignore_index=True) if train_rows else pd.DataFrame(columns=df.columns)\n",
    "    val_df = pd.DataFrame(val_rows) if val_rows else pd.DataFrame(columns=df.columns)\n",
    "    test_df = pd.DataFrame(test_rows) if test_rows else pd.DataFrame(columns=df.columns)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "clicks_clean = clean_interactions(clicks)\n",
    "user_histories = build_user_histories(clicks_clean)\n",
    "train_df, val_df, test_df = temporal_split_per_user(clicks_clean, CONFIG[\"min_user_interactions\"])\n",
    "\n",
    "print(f\"Interactions totales après nettoyage: {len(clicks_clean)}\")\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Candidate items are derived from training data only\n",
    "candidate_items = sorted(train_df[\"article_id\"].unique().tolist())\n",
    "print(f\"Catalog (train only): {len(candidate_items)} items\")\n",
    "\n",
    "# Leakage checks\n",
    "def assert_no_leakage(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Ensure temporal ordering per user without forbidding repeated items.\"\"\"\n",
    "    latest_train = train_df.groupby(\"user_id\")[\"timestamp\"].max()\n",
    "    earliest_val = val_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "    earliest_test = test_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "    for user_id in latest_train.index:\n",
    "        if user_id in earliest_val.index:\n",
    "            assert latest_train[user_id] <= earliest_val[user_id], \"Temporal leakage train->val\"\n",
    "        if user_id in earliest_test.index:\n",
    "            assert latest_train[user_id] <= earliest_test[user_id], \"Temporal leakage train->test\"\n",
    "\n",
    "    if not val_df.empty and not test_df.empty:\n",
    "        latest_val = val_df.groupby(\"user_id\")[\"timestamp\"].max()\n",
    "        earliest_test = test_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "        for user_id in latest_val.index:\n",
    "            if user_id in earliest_test.index:\n",
    "                assert latest_val[user_id] <= earliest_test[user_id], \"Temporal leakage val->test\"\n",
    "\n",
    "assert_no_leakage(train_df, val_df, test_df)\n",
    "\n",
    "train_user_items = train_df.groupby(\"user_id\")[\"article_id\"].apply(set).to_dict()\n",
    "\n",
    "print(f\"Users in train: {len(train_user_items)}\")\n",
    "\n",
    "display(train_df.head())\n"
   ],
   "id": "78caefc0d42e539a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles et métriques de ranking\n",
    "\n",
    "Les modèles sont entraînés **indépendamment** et évalués sur le même jeu de candidats (articles vus en train).\n",
    "Pour le modèle avec pondération de session, nous transformons le signal implicite en :\n",
    "`rating = rating_base + log1p(session_size)` afin de renforcer légèrement les sessions plus longues.\n",
    "\n",
    "Le **rating_base** est défini comme `1 + 0.01 * rang_normalisé` (rang temporel par utilisateur),\n",
    "ce qui introduit une légère variance nécessaire au calcul des prédictions sans changer la logique implicite.\n",
    "\n",
    "**Note** : les modèles SVD++ exploitent à la fois les notes implicites et l'historique d'interactions\n",
    "pour apprendre des facteurs latents (pas de similarité de contenu textuel direct).\n",
    "\n",
    "---\n"
   ],
   "id": "f40da61d7585693"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Modeling and evaluation\n",
    "\n",
    "\n",
    "def build_surprise_trainset(df: pd.DataFrame, rating_col: str) -> Tuple[Dataset, object]:\n",
    "    rating_min = float(df[rating_col].min())\n",
    "    rating_max = float(df[rating_col].max())\n",
    "    reader = Reader(rating_scale=(rating_min, rating_max))\n",
    "    data = Dataset.load_from_df(df[[\"user_id\", \"article_id\", rating_col]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    return data, trainset\n",
    "\n",
    "\n",
    "def recommend_random(rng: np.random.Generator, user_id: int, k: int) -> List[int]:\n",
    "    seen = train_user_items.get(user_id, set())\n",
    "    candidates = [item for item in candidate_items if item not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "    rng.shuffle(candidates)\n",
    "    return candidates[:k]\n",
    "\n",
    "\n",
    "def recommend_surprise(model, user_id: int, k: int) -> List[int]:\n",
    "    seen = train_user_items.get(user_id, set())\n",
    "    candidates = [item for item in candidate_items if item not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "    preds = []\n",
    "    for item_id in candidates:\n",
    "        pred = model.predict(user_id, item_id).est\n",
    "        preds.append((item_id, pred))\n",
    "    preds.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in preds[:k]]\n",
    "\n",
    "class HybridSvdppKnnModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        svdpp_weighted_model,\n",
    "        knn_model,\n",
    "        svdpp_weight: float = 0.6,\n",
    "        knn_weight: float = 0.4,\n",
    "    ) -> None:\n",
    "        self.svdpp_weighted_model = svdpp_weighted_model\n",
    "        self.knn_model = knn_model\n",
    "        self.svdpp_weight = svdpp_weight\n",
    "        self.knn_weight = knn_weight\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        prediction = self.svdpp_weighted_model.predict(user_id, item_id)\n",
    "        knn_score = self.knn_model.predict(user_id, item_id).est\n",
    "        blended_score = self.svdpp_weight * prediction.est + self.knn_weight * knn_score\n",
    "        return prediction._replace(est=blended_score)\n",
    "\n",
    "\n",
    "def precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not recommended:\n",
    "        return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for item in recommended_k if item in relevant_set)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for item in recommended_k if item in relevant_set)\n",
    "    return hits / len(relevant_set)\n",
    "\n",
    "\n",
    "def average_precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    relevant_set = set(relevant)\n",
    "    ap = 0.0\n",
    "    hits = 0\n",
    "    for idx, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant_set:\n",
    "            hits += 1\n",
    "            ap += hits / idx\n",
    "    return ap / min(len(relevant_set), k)\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    relevant_set = set(relevant)\n",
    "    dcg = 0.0\n",
    "    for idx, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant_set:\n",
    "            dcg += 1.0 / np.log2(idx + 1)\n",
    "    ideal_hits = min(len(relevant_set), k)\n",
    "    idcg = sum(1.0 / np.log2(idx + 1) for idx in range(1, ideal_hits + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model_name: str, recommend_fn, eval_df: pd.DataFrame, k: int) -> Dict[str, float]:\n",
    "    start_time = time.perf_counter()\n",
    "    per_user_metrics = []\n",
    "    all_recommended = []\n",
    "    users = eval_df[\"user_id\"].unique()\n",
    "    for user_id in users:\n",
    "        relevant = eval_df.loc[eval_df[\"user_id\"] == user_id, \"article_id\"].tolist()\n",
    "        user_start = time.perf_counter()\n",
    "        recommended = recommend_fn(user_id, k)\n",
    "        latency = time.perf_counter() - user_start\n",
    "        all_recommended.extend(recommended)\n",
    "        per_user_metrics.append(\n",
    "            {\n",
    "                \"precision\": precision_at_k(recommended, relevant, k),\n",
    "                \"recall\": recall_at_k(recommended, relevant, k),\n",
    "                \"map\": average_precision_at_k(recommended, relevant, k),\n",
    "                \"ndcg\": ndcg_at_k(recommended, relevant, k),\n",
    "                \"latency\": latency,\n",
    "            }\n",
    "        )\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    if not per_user_metrics:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"precision@k\": 0.0,\n",
    "            \"recall@k\": 0.0,\n",
    "            \"map@k\": 0.0,\n",
    "            \"ndcg@k\": 0.0,\n",
    "            \"coverage@k\": 0.0,\n",
    "            \"latency_per_user_s\": 0.0,\n",
    "            \"total_eval_time_s\": total_time,\n",
    "        }\n",
    "    metrics_df = pd.DataFrame(per_user_metrics)\n",
    "    coverage = len(set(all_recommended)) / max(len(candidate_items), 1)\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"precision@k\": metrics_df[\"precision\"].mean(),\n",
    "        \"recall@k\": metrics_df[\"recall\"].mean(),\n",
    "        \"map@k\": metrics_df[\"map\"].mean(),\n",
    "        \"ndcg@k\": metrics_df[\"ndcg\"].mean(),\n",
    "        \"coverage@k\": coverage,\n",
    "        \"latency_per_user_s\": metrics_df[\"latency\"].mean(),\n",
    "        \"total_eval_time_s\": total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare implicit ratings\n",
    "train_df = train_df.copy()\n",
    "train_df[\"interaction_rank\"] = train_df.groupby(\"user_id\").cumcount()\n",
    "max_rank = train_df[\"interaction_rank\"].max()\n",
    "if max_rank > 0:\n",
    "    train_df[\"rating\"] = 1.0 + 0.01 * (train_df[\"interaction_rank\"] / max_rank)\n",
    "else:\n",
    "    train_df[\"rating\"] = 1.0\n",
    "train_df[\"rating_weighted\"] = train_df[\"rating\"] + np.log1p(train_df[\"session_size\"].astype(float))\n",
    "\n",
    "_, trainset_basic = build_surprise_trainset(train_df, rating_col=\"rating\")\n",
    "_, trainset_weighted = build_surprise_trainset(train_df, rating_col=\"rating_weighted\")\n",
    "\n",
    "# Models\n",
    "rng = np.random.default_rng(CONFIG[\"random_seed\"])\n"
   ],
   "id": "a4db9eac36403577",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train NormalPredictor baseline\n",
    "normal_model = NormalPredictor()\n",
    "normal_model.fit(trainset_basic)\n"
   ],
   "id": "32b98c6f3759c0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train item-item cosine model\n",
    "content_model = KNNBasic(sim_options={\"name\": \"cosine\", \"user_based\": False})\n",
    "content_model.fit(trainset_basic)\n"
   ],
   "id": "1e51bc31ec92816e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train SVD++ collaborative filtering model\n",
    "svdpp_model = SVDpp(random_state=CONFIG[\"random_seed\"])\n",
    "svdpp_model.fit(trainset_basic)\n"
   ],
   "id": "aa7ea9cc0096e36e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train SVD++ model with session weighting\n",
    "svdpp_weighted_model = SVDpp(random_state=CONFIG[\"random_seed\"])\n",
    "svdpp_weighted_model.fit(trainset_weighted)"
   ],
   "id": "d4be10d435fdc09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train hybrid model (SVD++ session-weighted + item-item KNN)\n",
    "hybrid_svdpp_knn_model = HybridSvdppKnnModel(\n",
    "    svdpp_weighted_model,\n",
    "    content_model,\n",
    "    svdpp_weight=0.6,\n",
    "    knn_weight=0.4,\n",
    ")"
   ],
   "id": "eb8ed6f5e6ef0b2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = []\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Random recommender\",\n",
    "        lambda user_id, k: recommend_random(rng, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "479f12570074f4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"NormalPredictor\",\n",
    "        lambda user_id, k: recommend_surprise(normal_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "6e342ac8c299c3d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"KNNBasic cosine (item-item)\",\n",
    "        lambda user_id, k: recommend_surprise(content_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "a20a4727cfdbada2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"SVD++\",\n",
    "        lambda user_id, k: recommend_surprise(svdpp_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "cb89ebdb8b24d3e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"SVD++ + session weighting\",\n",
    "        lambda user_id, k: recommend_surprise(svdpp_weighted_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "776567c3b7a5dde5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Hybrid SVD++ (session weighting) + KNNBasic cosine (item-item)\",\n",
    "        lambda user_id, k: recommend_surprise(hybrid_svdpp_knn_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "    )\n",
    ")"
   ],
   "id": "38a81e0ec4f751bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(by=\"ndcg@k\", ascending=False))"
   ],
   "id": "752707c5fc3b6593",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétation rapide\n",
    "\n",
    "- **Random recommender** : sert de base sanity-check ; il ignore les préférences et affiche les métriques les plus faibles.\n",
    "- **NormalPredictor** : exploite la distribution globale des interactions, utile comme baseline non personnalisée mais limitée en couverture personnelle.\n",
    "- **KNNBasic cosine (item-item)** : capte des co-occurrences simples, efficace quand la popularité locale est un bon signal.\n",
    "- **SVD++** : modèle factoriel collaboratif qui combine notes implicites et historique d'interactions pour affiner les recommandations.\n",
    "- **SVD++ + session weighting** : renforce les sessions longues, ce qui peut mieux refléter l'engagement utilisateur pour le contexte news.\n",
    "\n",
    "---\n"
   ],
   "id": "e640b2f140ef5398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Debug harness: compare SVDpp vs KNN vs Hybrid (Surprise)\n",
    "# - checks if KNN scores are flat, if hybrid changes ranking\n",
    "# - prints per-user TopK comparisons and score statistics\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assumes you already have in memory:\n",
    "# - trainset_basic, trainset_weighted (Surprise Trainset)\n",
    "# - candidate_items: List[int]\n",
    "# - train_user_items: Dict[int, set]  (user_id -> seen items in train)\n",
    "# - test_df: pd.DataFrame with columns [\"user_id\",\"article_id\"]\n",
    "# - CONFIG[\"k\"], CONFIG[\"random_seed\"]\n",
    "# - content_model already trained (KNNBasic cosine item-item) OR we re-train below\n",
    "#\n",
    "# If content_model is not trained yet, uncomment the training block below.\n",
    "\n",
    "from surprise import KNNBasic, SVDpp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Optional) Train KNN cosine if not already trained\n",
    "# ------------------------------------------------------------\n",
    "# content_model = KNNBasic(sim_options={\"name\": \"cosine\", \"user_based\": False})\n",
    "# content_model.fit(trainset_basic)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Train SVDpp on weighted ratings (as in your hybrid)\n",
    "# ------------------------------------------------------------\n",
    "svdpp_weighted_model = SVDpp(random_state=CONFIG[\"random_seed\"])\n",
    "svdpp_weighted_model.fit(trainset_weighted)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Hybrid wrapper\n",
    "# ------------------------------------------------------------\n",
    "class HybridSvdppKnnModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        svdpp_weighted_model: Any,\n",
    "        knn_model: Any,\n",
    "        svdpp_weight: float = 0.6,\n",
    "        knn_weight: float = 0.4,\n",
    "    ) -> None:\n",
    "        self.svdpp_weighted_model = svdpp_weighted_model\n",
    "        self.knn_model = knn_model\n",
    "        self.svdpp_weight = float(svdpp_weight)\n",
    "        self.knn_weight = float(knn_weight)\n",
    "\n",
    "    def predict(self, user_id: int, item_id: int):\n",
    "        pred = self.svdpp_weighted_model.predict(user_id, item_id)\n",
    "        knn_est = self.knn_model.predict(user_id, item_id).est\n",
    "        blended = self.svdpp_weight * pred.est + self.knn_weight * knn_est\n",
    "        return pred._replace(est=blended)\n",
    "\n",
    "\n",
    "hybrid_06_04 = HybridSvdppKnnModel(svdpp_weighted_model, content_model, 0.6, 0.4)\n",
    "hybrid_knn_only = HybridSvdppKnnModel(svdpp_weighted_model, content_model, 0.0, 1.0)\n",
    "hybrid_svd_only = HybridSvdppKnnModel(svdpp_weighted_model, content_model, 1.0, 0.0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Recommendation helpers (same semantics as your pipeline)\n",
    "# ------------------------------------------------------------\n",
    "def recommend_surprise(model: Any, user_id: int, k: int) -> List[int]:\n",
    "    seen = train_user_items.get(user_id, set())\n",
    "    candidates = [item for item in candidate_items if item not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "    preds: List[Tuple[int, float]] = []\n",
    "    for item_id in candidates:\n",
    "        est = model.predict(user_id, item_id).est\n",
    "        preds.append((item_id, float(est)))\n",
    "    preds.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in preds[:k]]\n",
    "\n",
    "\n",
    "def recommend_with_scores(model: Any, user_id: int, n_items: int = 500) -> Tuple[List[int], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns: (candidates_used, svd_or_model_scores, None)\n",
    "    Here we just return model scores for a candidate subset for debug.\n",
    "    \"\"\"\n",
    "    seen = train_user_items.get(user_id, set())\n",
    "    candidates = [item for item in candidate_items if item not in seen]\n",
    "    if not candidates:\n",
    "        return [], np.array([]), np.array([])\n",
    "    # Take a deterministic subset for speed\n",
    "    candidates = candidates[: min(n_items, len(candidates))]\n",
    "    scores = np.array([float(model.predict(user_id, item).est) for item in candidates], dtype=float)\n",
    "    return candidates, scores, np.array([])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Debug metrics about score spread + ranking equality\n",
    "# ------------------------------------------------------------\n",
    "@dataclass\n",
    "class ScoreStats:\n",
    "    std: float\n",
    "    min: float\n",
    "    max: float\n",
    "    uniq_rounded_4: int\n",
    "\n",
    "\n",
    "def score_stats(x: np.ndarray) -> ScoreStats:\n",
    "    if x.size == 0:\n",
    "        return ScoreStats(0.0, 0.0, 0.0, 0)\n",
    "    xr = np.round(x, 4)\n",
    "    return ScoreStats(\n",
    "        std=float(np.std(x)),\n",
    "        min=float(np.min(x)),\n",
    "        max=float(np.max(x)),\n",
    "        uniq_rounded_4=int(len(np.unique(xr))),\n",
    "    )\n",
    "\n",
    "\n",
    "def jaccard(a: List[int], b: List[int]) -> float:\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb:\n",
    "        return 1.0\n",
    "    if not sa or not sb:\n",
    "        return 0.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main test runner\n",
    "# ------------------------------------------------------------\n",
    "def run_debug(\n",
    "    users: List[int],\n",
    "    k: int,\n",
    "    n_score_items: int = 500,\n",
    ") -> pd.DataFrame:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for u in users:\n",
    "        # TopK comparisons\n",
    "        top_svd = recommend_surprise(svdpp_weighted_model, u, k)\n",
    "        top_knn = recommend_surprise(content_model, u, k)\n",
    "        top_hyb = recommend_surprise(hybrid_06_04, u, k)\n",
    "        top_hyb_knn_only = recommend_surprise(hybrid_knn_only, u, k)\n",
    "        top_hyb_svd_only = recommend_surprise(hybrid_svd_only, u, k)\n",
    "\n",
    "        # Score spreads on same candidate subset\n",
    "        cand, svd_scores, _ = recommend_with_scores(svdpp_weighted_model, u, n_items=n_score_items)\n",
    "        _, knn_scores, _ = recommend_with_scores(content_model, u, n_items=n_score_items)\n",
    "        _, hyb_scores, _ = recommend_with_scores(hybrid_06_04, u, n_items=n_score_items)\n",
    "\n",
    "        ss = score_stats(svd_scores)\n",
    "        ks = score_stats(knn_scores)\n",
    "        hs = score_stats(hyb_scores)\n",
    "\n",
    "        corr = float(np.corrcoef(svd_scores, knn_scores)[0, 1]) if svd_scores.size and knn_scores.size else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"user_id\": u,\n",
    "                \"cand_used_for_stats\": len(cand),\n",
    "                \"svd_std\": ss.std,\n",
    "                \"svd_min\": ss.min,\n",
    "                \"svd_max\": ss.max,\n",
    "                \"svd_uniq4\": ss.uniq_rounded_4,\n",
    "                \"knn_std\": ks.std,\n",
    "                \"knn_min\": ks.min,\n",
    "                \"knn_max\": ks.max,\n",
    "                \"knn_uniq4\": ks.uniq_rounded_4,\n",
    "                \"hyb_std\": hs.std,\n",
    "                \"hyb_min\": hs.min,\n",
    "                \"hyb_max\": hs.max,\n",
    "                \"hyb_uniq4\": hs.uniq_rounded_4,\n",
    "                \"corr_svd_knn\": corr,\n",
    "                \"topk_equal_svd_vs_hybrid\": top_svd == top_hyb,\n",
    "                \"topk_jacc_svd_vs_hybrid\": jaccard(top_svd, top_hyb),\n",
    "                \"topk_equal_svd_vs_hyb_svdonly\": top_svd == top_hyb_svd_only,\n",
    "                \"topk_equal_knn_vs_hyb_knnonly\": top_knn == top_hyb_knn_only,\n",
    "                \"top_svd\": top_svd,\n",
    "                \"top_knn\": top_knn,\n",
    "                \"top_hybrid_06_04\": top_hyb,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pick a user sample to test\n",
    "# ------------------------------------------------------------\n",
    "# 1) First N users from test_df (deterministic)\n",
    "unique_users = test_df[\"user_id\"].dropna().astype(int).unique().tolist()\n",
    "N_USERS = min(10, len(unique_users))\n",
    "users_to_test = unique_users[:N_USERS]\n",
    "\n",
    "debug_df = run_debug(users_to_test, k=CONFIG[\"k\"], n_score_items=500)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Summary prints (what I need from you)\n",
    "# ------------------------------------------------------------\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "display(debug_df)\n",
    "\n",
    "print(\"\\n=== Aggregate summary ===\")\n",
    "print(\"Users tested:\", len(debug_df))\n",
    "print(\"TopK identical (SVDpp vs Hybrid 0.6/0.4):\", int(debug_df[\"topk_equal_svd_vs_hybrid\"].sum()), \"/\", len(debug_df))\n",
    "print(\"Mean Jaccard@K (SVDpp vs Hybrid):\", float(debug_df[\"topk_jacc_svd_vs_hybrid\"].mean()))\n",
    "print(\"Mean corr(SVD,KNN) on candidate subset:\", float(np.nanmean(debug_df[\"corr_svd_knn\"])))\n",
    "\n",
    "print(\"\\n=== Quick diagnosis hints ===\")\n",
    "print(\"- If knn_std is ~0 and knn_uniq4 is tiny => KNN scores are effectively constant -> hybrid can't change ranking.\")\n",
    "print(\"- If corr_svd_knn is ~1 and both have similar ordering => hybrid likely preserves SVD ranking.\")\n",
    "print(\"- If topk_equal_svd_vs_hyb_svdonly is True (should be) and topk_equal_knn_vs_hyb_knnonly is True (should be) -> wrapper is consistent.\")\n",
    "\n",
    "# Optional: save to CSV for sharing\n",
    "# debug_df.to_csv(\"hybrid_debug.csv\", index=False)\n"
   ],
   "id": "4e3c575fa4e242e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d08ada1b0ce34061",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
