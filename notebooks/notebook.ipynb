{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773debcd",
   "metadata": {},
   "source": [
    "# Évaluation d'un système de recommandation My Content\n",
    "\n",
    "Notebook pour entraîner et comparer plusieurs approches de recommandation sur le dataset Kaggle **news-portal-user-interactions-by-globocom**. L'objectif est de montrer clairement chaque étape (du chargement des données jusqu'au choix final du modèle).\n",
    "\n",
    "> Ce notebook aligne désormais **toutes les approches de recommandation sur la bibliothèque Surprise** (https://surprise.readthedocs.io/) afin de bénéficier d'algorithmes collaboratifs standardisés et faciles à déployer."
   ]
  },
  {
   "cell_type": "code",
   "id": "43aa8131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:53.682258Z",
     "iopub.status.busy": "2025-12-17T10:07:53.681987Z",
     "iopub.status.idle": "2025-12-17T10:07:55.146794Z",
     "shell.execute_reply": "2025-12-17T10:07:55.145979Z"
    }
   },
   "source": [
    "# Imports & Config\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Ensure the project root is importable\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.models.lightfm_item2item import (\n",
    "    CONTEXT_COLUMNS,\n",
    "    LightFMApproximator,\n",
    "    build_interaction_matrices,\n",
    "    precompute_item_neighbors,\n",
    "    score_from_neighbors,\n",
    "    session_weight_from_size,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"clicks_dir\": \"../data/news-portal-user-interactions-by-globocom/clicks\",\n",
    "    \"metadata_path\": \"../data/news-portal-user-interactions-by-globocom/articles_metadata.csv\",\n",
    "    \"embeddings_path\": \"../data/news-portal-user-interactions-by-globocom/articles_embeddings.pickle\",\n",
    "    \"max_click_files\": None,\n",
    "    \"artifacts_dir\": \"../artifacts/evaluation\",\n",
    "    \"k\": 5,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"recent_window_days\": 7,\n",
    "    \"random_seed\": 42,\n",
    "    \"svd_components\": 64,\n",
    "    \"content_pca_components\": None,\n",
    "    \"covisit_top_n_neighbors\": 20,\n",
    "    \"covisit_similarity\": \"cosine\",\n",
    "    \"covisit_hybrid_alpha\": 0.7350738721058192,\n",
    "    \"svd_hazard_ndcg\": 0.02,\n",
    "    \"min_user_interactions\": 3,\n",
    "    \"min_item_interactions\": 5,\n",
    "    \"svd_use_session_rating\": True,\n",
    "    \"lightfm_use_user_features\": True,\n",
    "    \"lightfm_components\": 48,\n",
    "    \"lightfm_item_neighbors\": 200,\n",
    "    \"hybrid_weights\": (0.6, 0.4),\n",
    "}\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "Path(CONFIG[\"artifacts_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config ready\", CONFIG)\n",
    "\n",
    "from surprise import Dataset, Reader, KNNBasic, NormalPredictor, SVD\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc6be6a7",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Nous voulons proposer à chaque lecteur un Top-5 d'articles susceptibles de l'intéresser. Le notebook illustre la démarche de A à Z : préparation des données, construction de différentes familles de modèles puis comparaison à l'aide de métriques de ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8cfa",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Les fichiers attendus sont situés dans `/data/*`."
   ]
  },
  {
   "cell_type": "code",
   "id": "116743e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.150007Z",
     "iopub.status.busy": "2025-12-17T10:07:55.149650Z",
     "iopub.status.idle": "2025-12-17T10:07:55.178749Z",
     "shell.execute_reply": "2025-12-17T10:07:55.177740Z"
    }
   },
   "source": [
    "\n",
    "# Load data utilities\n",
    "\n",
    "\n",
    "def detect_timestamp_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the timestamp-like column name.\"\"\"\n",
    "    candidates = [\"click_timestamp\", \"timestamp\", \"event_time\", \"ts\", \"time\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates or col.lower() in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No timestamp-like column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def detect_article_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the article/item column name.\"\"\"\n",
    "    candidates = [\"click_article_id\", \"clicked_article_id\", \"article_id\", \"item_id\", \"content_id\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No article id column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def infer_unix_unit(values: pd.Series) -> str:\n",
    "    numeric = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
    "    if numeric.empty:\n",
    "        return \"s\"\n",
    "    max_abs = numeric.abs().max()\n",
    "    if max_abs >= 1e14:\n",
    "        return \"ns\"\n",
    "    if max_abs >= 1e11:\n",
    "        return \"ms\"\n",
    "    return \"s\"\n",
    "\n",
    "\n",
    "def to_timestamp(series: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return pd.to_datetime(series)\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        unit = infer_unix_unit(series)\n",
    "        return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "    converted = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if converted.notna().any():\n",
    "        return converted\n",
    "\n",
    "    unit = infer_unix_unit(series)\n",
    "    return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def list_click_files(path: Union[str, Path]) -> List[Path]:\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.is_file():\n",
    "        return [path_obj]\n",
    "    if path_obj.is_dir():\n",
    "        return sorted(path_obj.glob(\"clicks_hour_*.csv\"))\n",
    "    return []\n",
    "\n",
    "\n",
    "def ensure_context_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure session_size and context columns exist with safe defaults.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"session_size\" not in df.columns:\n",
    "        df[\"session_size\"] = 1\n",
    "    for col in CONTEXT_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_synthetic_clicks(path: str, n_users: int = 50, n_items: int = 120, days: int = 30, interactions_per_user: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"Create a small synthetic clicks dataset to keep the notebook runnable.\"\"\"\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "    start = pd.Timestamp(\"2022-01-01\")\n",
    "    envs = [\"web\", \"app\"]\n",
    "    devices = [\"mobile\", \"desktop\"]\n",
    "    oss = [\"ios\", \"android\", \"linux\"]\n",
    "    referrers = [\"direct\", \"search\", \"social\"]\n",
    "    records = []\n",
    "    for user in range(1, n_users + 1):\n",
    "        offsets = rng.integers(0, days, size=interactions_per_user)\n",
    "        timestamps = [start + pd.Timedelta(int(o), unit=\"D\") for o in sorted(offsets.tolist())]\n",
    "        articles = rng.integers(1, n_items + 1, size=interactions_per_user)\n",
    "        for ts, art in zip(timestamps, articles):\n",
    "            records.append({\n",
    "                \"user_id\": int(user),\n",
    "                \"article_id\": int(art),\n",
    "                \"timestamp\": ts,\n",
    "                \"session_size\": int(rng.integers(1, 6)),\n",
    "                \"click_environment\": rng.choice(envs),\n",
    "                \"click_deviceGroup\": rng.choice(devices),\n",
    "                \"click_os\": rng.choice(oss),\n",
    "                \"click_country\": rng.choice([\"fr\", \"us\", \"br\"]),\n",
    "                \"click_region\": rng.choice([\"idf\", \"sp\", \"ca\"]),\n",
    "                \"click_referrer_type\": rng.choice(referrers),\n",
    "            })\n",
    "    df = pd.DataFrame(records).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\n",
    "        f\"Synthetic clicks dataset created at {path} \"\n",
    "        f\"(users={n_users}, items={n_items}, interactions={len(df)})\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_clicks(path: str, max_files: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load clicks data from the Globo hourly files, with a safety cap.\"\"\"\n",
    "    files = list_click_files(path)\n",
    "    total_files = len(files)\n",
    "    if not files:\n",
    "        print(f\"Clicks directory not found at {path}. Generating a synthetic sample for demonstration.\")\n",
    "        return ensure_context_columns(create_synthetic_clicks(Path(path) / \"clicks_hour_000.csv\"))\n",
    "\n",
    "    if max_files is not None:\n",
    "        print(f\"Limite explicite max_files={max_files}, total détecté={total_files}\")\n",
    "        files = files[:max_files]\n",
    "\n",
    "    print(f\"Chargement de {len(files)} fichiers clicks (total détecté={total_files}, limite={max_files if max_files is not None else 'aucune'})\")\n",
    "    frames = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        ts_col = detect_timestamp_column(df)\n",
    "        article_col = detect_article_column(df)\n",
    "        df[ts_col] = to_timestamp(df[ts_col])\n",
    "        df = df.rename(columns={ts_col: \"timestamp\", article_col: \"article_id\"})\n",
    "        df = ensure_context_columns(df)\n",
    "        keep_cols = [col for col in [\n",
    "            \"user_id\",\n",
    "            \"article_id\",\n",
    "            \"timestamp\",\n",
    "            \"session_size\",\n",
    "            *CONTEXT_COLUMNS,\n",
    "        ] if col in df.columns]\n",
    "        frames.append(df[keep_cols])\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined = combined.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"Clicks agrégés : {len(combined)} lignes, {combined['user_id'].nunique()} utilisateurs uniques, {combined['article_id'].nunique()} articles uniques.\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_metadata(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load article metadata if available.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Metadata file not found at {path}. Utilisation du pipeline Surprise uniquement si les métadonnées sont absentes.\")\n",
    "        return None\n",
    "    meta = pd.read_csv(path)\n",
    "    if \"article_id\" not in meta.columns:\n",
    "        print(\"Metadata missing 'article_id' column. Ignoring metadata.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "\n",
    "clicks = load_clicks(CONFIG[\"clicks_dir\"], max_files=CONFIG[\"max_click_files\"])\n",
    "metadata = load_metadata(CONFIG[\"metadata_path\"])\n",
    "print(clicks.head())\n",
    "print(\"Metadata loaded:\", metadata is not None)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eab18f0f",
   "metadata": {},
   "source": [
    "## Analyse exploratoire des données\n",
    "\n",
    "Courte photographie des fichiers sources immédiatement après le chargement :\n",
    "- nombre de lignes et noms de colonnes des clics\n",
    "- volumes et intégrité des métadonnées articles\n",
    "- dimensions et structure du fichier d'`articles_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "id": "45108db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.182450Z",
     "iopub.status.busy": "2025-12-17T10:07:55.182242Z",
     "iopub.status.idle": "2025-12-17T10:07:55.260458Z",
     "shell.execute_reply": "2025-12-17T10:07:55.259678Z"
    }
   },
   "source": [
    "# EDA rapide sur les données sources\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "\n",
    "\n",
    "def summarize_timestamps(series: pd.Series):\n",
    "    series = pd.to_datetime(series)\n",
    "    daily = series.dt.date.value_counts().sort_index().rename_axis(\"date\").reset_index(name=\"nb_clicks\")\n",
    "    hourly = series.dt.hour.value_counts().sort_index().rename_axis(\"hour\").reset_index(name=\"nb_clicks\")\n",
    "    return series.min(), series.max(), daily, hourly\n",
    "\n",
    "\n",
    "def describe_structure(obj, prefix=\"embeddings\", max_depth=4):\n",
    "    entries = []\n",
    "\n",
    "    def add_entry(path, value, note=None):\n",
    "        entry = {\"chemin\": path, \"type\": type(value).__name__}\n",
    "        if hasattr(value, \"shape\"):\n",
    "            entry[\"shape\"] = tuple(getattr(value, \"shape\"))\n",
    "        elif hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)):\n",
    "            entry[\"len\"] = len(value)\n",
    "        if hasattr(value, \"dtype\"):\n",
    "            entry[\"dtype\"] = str(getattr(value, \"dtype\"))\n",
    "        if note:\n",
    "            entry[\"note\"] = note\n",
    "        if isinstance(value, np.ndarray) and value.dtype.names:\n",
    "            entry[\"dtype_fields\"] = list(value.dtype.names)\n",
    "        if isinstance(value, np.ndarray) and value.ndim == 1 and len(value) > 0 and not isinstance(value[0], (np.ndarray, list, tuple, Mapping)):\n",
    "            entry[\"exemple\"] = repr(value[:3].tolist())\n",
    "        entries.append(entry)\n",
    "\n",
    "    def walk(value, path, depth):\n",
    "        add_entry(path, value)\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        if isinstance(value, Mapping):\n",
    "            for k, v in value.items():\n",
    "                walk(v, f\"{path}.{k}\", depth + 1)\n",
    "        elif isinstance(value, (list, tuple, np.ndarray)) and not isinstance(value, (str, bytes)):\n",
    "            if len(value) > 0:\n",
    "                walk(value[0], f\"{path}[0]\", depth + 1)\n",
    "\n",
    "    walk(obj, prefix, 0)\n",
    "    return entries\n",
    "\n",
    "\n",
    "click_files = list_click_files(CONFIG[\"clicks_dir\"])\n",
    "print(f\"Nombre total de fichiers clicks détectés: {len(click_files)}\")\n",
    "if not click_files:\n",
    "    print(\"Aucun fichier clicks trouvé au chemin configuré. Vérifiez le téléchargement des données.\")\n",
    "\n",
    "files_for_eda = click_files[:2]\n",
    "per_file_stats = []\n",
    "for file in files_for_eda:\n",
    "    df_file = pd.read_csv(file)\n",
    "    ts_col = detect_timestamp_column(df_file)\n",
    "    article_col = detect_article_column(df_file)\n",
    "    timestamps = to_timestamp(df_file[ts_col])\n",
    "    per_file_stats.append(\n",
    "        {\n",
    "            \"fichier\": file.name,\n",
    "            \"nb_lignes\": len(df_file),\n",
    "            \"colonnes\": \", \".join(df_file.columns),\n",
    "            \"articles_uniques\": df_file[article_col].nunique(),\n",
    "            \"horodatage_min\": timestamps.min(),\n",
    "            \"horodatage_max\": timestamps.max(),\n",
    "        }\n",
    "    )\n",
    "if per_file_stats:\n",
    "    display(pd.DataFrame(per_file_stats))\n",
    "else:\n",
    "    print(\"Pas assez de fichiers pour réaliser une EDA détaillée par fichier.\")\n",
    "\n",
    "print(\"=== Clicks (agrégés) ===\")\n",
    "if clicks.empty:\n",
    "    print(\"Aucun clic chargé. Vérifier le chemin ou augmenter max_click_files.\")\n",
    "else:\n",
    "    clicks_summary = {\n",
    "        \"nb_lignes\": len(clicks),\n",
    "        \"colonnes\": \", \".join(clicks.columns),\n",
    "        \"utilisateurs_uniques\": clicks['user_id'].nunique() if 'user_id' in clicks else None,\n",
    "        \"articles_uniques\": clicks['article_id'].nunique() if 'article_id' in clicks else None,\n",
    "    }\n",
    "    display(pd.DataFrame([clicks_summary]))\n",
    "\n",
    "    total_articles = None\n",
    "    if metadata is not None and 'article_id' in metadata:\n",
    "        total_articles = metadata['article_id'].nunique()\n",
    "    elif 'article_id' in clicks:\n",
    "        total_articles = clicks['article_id'].nunique()\n",
    "\n",
    "    total_clients = clicks['user_id'].nunique() if 'user_id' in clicks else None\n",
    "    print(\"Synthèse globale (articles / clients)\")\n",
    "    display(pd.DataFrame([{\n",
    "        'nombre_total_articles': total_articles,\n",
    "        'nombre_total_clients': total_clients,\n",
    "    }]))\n",
    "\n",
    "    ts_min, ts_max, daily, hourly = summarize_timestamps(clicks['timestamp'])\n",
    "    display(pd.DataFrame([\n",
    "        {\n",
    "            'horodatage_min': ts_min,\n",
    "            'horodatage_max': ts_max,\n",
    "            'fenetre_jours': (ts_max - ts_min).days + 1,\n",
    "        }\n",
    "    ]))\n",
    "    print(\"Répartition par jour (jusqu'à 10 premières valeurs)\")\n",
    "    display(daily.head(10))\n",
    "    print(\"Répartition par heure (0-23)\")\n",
    "    display(hourly)\n",
    "\n",
    "print(\"=== Métadonnées des articles ===\")\n",
    "if metadata is None:\n",
    "    print(\"Aucun fichier metadata chargé.\")\n",
    "else:\n",
    "    meta_summary = {\n",
    "        \"nb_articles\": len(metadata),\n",
    "        \"colonnes\": \", \".join(metadata.columns),\n",
    "        \"articles_uniques\": metadata['article_id'].nunique() if 'article_id' in metadata else None,\n",
    "    }\n",
    "    display(pd.DataFrame([meta_summary]))\n",
    "    missing = metadata.isna().sum().sort_values(ascending=False)\n",
    "    display(missing.to_frame('valeurs_manquantes'))\n",
    "    if 'created_at_ts' in metadata.columns:\n",
    "        created = to_timestamp(metadata['created_at_ts'])\n",
    "        display(pd.DataFrame([{'premier_article': created.min(), 'dernier_article': created.max()}]))\n",
    "    if 'article_id' in metadata.columns:\n",
    "        overlap = set(clicks['article_id'].unique()) if 'article_id' in clicks.columns else set()\n",
    "        coverage = len(overlap & set(metadata['article_id'].unique()))\n",
    "        print(f\"Articles présents dans clicks et metadata: {coverage}\")\n",
    "\n",
    "\n",
    "print(\"=== Embeddings d'articles ===\")\n",
    "embeddings_path = Path(CONFIG['embeddings_path'])\n",
    "if embeddings_path.exists():\n",
    "    with embeddings_path.open('rb') as f:\n",
    "        embeddings_obj = pickle.load(f)\n",
    "    print(f\"Type chargé: {type(embeddings_obj)}\")\n",
    "\n",
    "    def summarize_matrix(mat):\n",
    "        stats = {\n",
    "            'shape': getattr(mat, 'shape', None),\n",
    "            'dtype': getattr(mat, 'dtype', None),\n",
    "        }\n",
    "\n",
    "        dim_values = []\n",
    "        shape = getattr(mat, 'shape', None)\n",
    "        if shape is not None and len(shape) >= 2:\n",
    "            dim_values.append(shape[1])\n",
    "        elif isinstance(mat, (list, tuple, np.ndarray)):\n",
    "            for row in mat:\n",
    "                if hasattr(row, '__len__') and not isinstance(row, (str, bytes)):\n",
    "                    try:\n",
    "                        dim_values.append(len(row))\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "\n",
    "        if dim_values:\n",
    "            stats.update({\n",
    "                'profondeur_min': min(dim_values),\n",
    "                'profondeur_moyenne': float(np.mean(dim_values)),\n",
    "                'profondeur_max': max(dim_values),\n",
    "            })\n",
    "\n",
    "        if hasattr(mat, 'shape') and len(getattr(mat, 'shape', [])) == 2:\n",
    "            norms = np.linalg.norm(mat, axis=1)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'nb_vectors': mat.shape[0],\n",
    "                    'dim': mat.shape[1],\n",
    "                    'norm_min': norms.min(),\n",
    "                    'norm_max': norms.max(),\n",
    "                    'norm_moyenne': norms.mean(),\n",
    "                }\n",
    "            )\n",
    "        return stats\n",
    "\n",
    "    base_structure = describe_structure(embeddings_obj, max_depth=4)\n",
    "\n",
    "    if isinstance(embeddings_obj, dict):\n",
    "        keys = list(embeddings_obj.keys())\n",
    "        print(f\"Clés disponibles: {keys}\")\n",
    "        matrix = embeddings_obj.get('embeddings')\n",
    "        ids = embeddings_obj.get('articles_ids') or embeddings_obj.get('article_ids')\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_ids',\n",
    "                'type': type(ids).__name__,\n",
    "                'len': len(ids),\n",
    "                'note': \"Identifiants d'articles fournis dans le fichier\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if matrix is not None:\n",
    "            stats = summarize_matrix(matrix)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'colonnes': \", \".join(keys),\n",
    "                    'nb_articles_ids': len(ids) if ids is not None else None,\n",
    "                    'ids_uniques': len(set(ids)) if ids is not None else None,\n",
    "                    'couverture_metadata': len(set(ids) & set(metadata['article_id']))\n",
    "                    if (metadata is not None and ids is not None and 'article_id' in metadata)\n",
    "                    else None,\n",
    "                    'couverture_clicks': len(set(ids) & set(clicks['article_id']))\n",
    "                    if (not clicks.empty and ids is not None and 'article_id' in clicks)\n",
    "                    else None,\n",
    "                }\n",
    "            )\n",
    "            display(pd.DataFrame([stats]))\n",
    "\n",
    "            if ids is not None:\n",
    "                sample_ids = ids[:5] if len(ids) >= 5 else ids\n",
    "                print(\"Aperçu des premiers article_id liés aux embeddings:\")\n",
    "                display(pd.DataFrame({'article_id': sample_ids}))\n",
    "\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, matrix.shape[1] if hasattr(matrix, 'shape') else 0))]\n",
    "            if preview_cols:\n",
    "                preview = pd.DataFrame(matrix[:5, : len(preview_cols)], columns=preview_cols)\n",
    "                if ids is not None:\n",
    "                    preview.insert(0, 'article_id', ids[: len(preview)])\n",
    "                print(\"Aperçu des embeddings (quelques colonnes et premières lignes):\")\n",
    "                display(preview)\n",
    "                print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "                print(\", \".join(preview.columns))\n",
    "\n",
    "                if ids is not None and metadata is not None and 'article_id' in metadata:\n",
    "                    meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                    meta_sample = (\n",
    "                        preview[['article_id']]\n",
    "                        .merge(metadata[['article_id'] + meta_cols], on='article_id', how='left')\n",
    "                    )\n",
    "                    if 'created_at_ts' in meta_sample.columns:\n",
    "                        meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                    print(\"Exemple de liaison embedding -> metadata sur article_id (5 premières lignes):\")\n",
    "                    display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Aucune matrice d'embeddings explicite trouvée dans l'objet chargé.\")\n",
    "    elif hasattr(embeddings_obj, 'shape'):\n",
    "        stats = summarize_matrix(embeddings_obj)\n",
    "\n",
    "        inferred_ids = None\n",
    "        mapping_note = None\n",
    "        if metadata is not None and 'article_id' in metadata and hasattr(embeddings_obj, 'shape'):\n",
    "            if embeddings_obj.shape[0] == len(metadata):\n",
    "                inferred_ids = metadata['article_id'].reset_index(drop=True)\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id explicite fourni ; association supposée alignée sur l'ordre des metadata.\"\n",
    "                )\n",
    "            else:\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id dans le fichier d'embeddings et la taille ne correspond pas aux metadata : \"\n",
    "                    f\"{embeddings_obj.shape[0]} vecteurs vs {len(metadata)} lignes de metadata.\"\n",
    "                )\n",
    "        else:\n",
    "            mapping_note = (\n",
    "                \"Aucun identifiant d'article n'est présent dans le fichier d'embeddings (mapping externe requis).\"\n",
    "            )\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if inferred_ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_id (inféré)',\n",
    "                'type': type(inferred_ids).__name__,\n",
    "                'len': len(inferred_ids),\n",
    "                'note': \"Alignement supposé sur metadata.article_id (index identique).\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if mapping_note:\n",
    "            print(mapping_note)\n",
    "\n",
    "        if inferred_ids is not None:\n",
    "            stats.update(\n",
    "                {\n",
    "                    'ids_source': 'metadata.article_id (alignement par index)',\n",
    "                    'ids_uniques': inferred_ids.nunique(),\n",
    "                    'couverture_metadata': len(set(inferred_ids) & set(metadata['article_id'])),\n",
    "                    'couverture_clicks': len(set(inferred_ids) & set(clicks['article_id'])) if not clicks.empty else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        display(pd.DataFrame([stats]))\n",
    "        if len(getattr(embeddings_obj, 'shape', [])) >= 2 and embeddings_obj.shape[1] > 0:\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, embeddings_obj.shape[1]))]\n",
    "            preview = pd.DataFrame(embeddings_obj[:5, : len(preview_cols)], columns=preview_cols)\n",
    "            if inferred_ids is not None:\n",
    "                preview.insert(0, 'article_id', inferred_ids.iloc[: len(preview)].values)\n",
    "            print(\"Aperçu direct de la matrice d'embeddings:\")\n",
    "            display(preview)\n",
    "            print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "            print(\", \".join(preview.columns))\n",
    "\n",
    "            if inferred_ids is not None and metadata is not None:\n",
    "                meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                meta_sample = preview[['article_id']].merge(\n",
    "                    metadata[['article_id'] + meta_cols], on='article_id', how='left'\n",
    "                )\n",
    "                if 'created_at_ts' in meta_sample.columns:\n",
    "                    meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                print(\"Exemple de liaison embedding -> metadata sur article_id (inféré):\")\n",
    "                display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Objet chargé non structuré, utilisez type/len pour investiguer.\")\n",
    "else:\n",
    "    print(f\"Fichier d'embeddings introuvable à {embeddings_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bac08d578ddc9073",
   "metadata": {},
   "source": [
    "# Article Embeddings\n",
    "\n",
    "Ce fichier contient les **embeddings des articles**, c’est-à-dire une **représentation numérique du contenu textuel** permettant de comparer les articles entre eux sur le plan sémantique.\n",
    "\n",
    "* **Format** : matrice NumPy `(N, 250)` en `float32`\n",
    "* **1 ligne = 1 article**\n",
    "* **250 colonnes = dimensions latentes**\n",
    "* Les valeurs individuelles n’ont pas de signification directe\n",
    "\n",
    "L’`article_id` n’est **pas stocké explicitement** : il est **déduit de l’ordre des lignes**, qui doit rester aligné avec les métadonnées des articles.\n",
    "\n",
    "La variable `words_count` indique le **nombre de mots du texte source** et sert uniquement d’indicateur de qualité du contenu.\n",
    "\n",
    "Les embeddings **ne sont pas normalisés** : la **similarité cosinus** est la mesure recommandée pour comparer les articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26424a2",
   "metadata": {},
   "source": [
    "## Protocole\n",
    "\n",
    "1. Tri des interactions par horodatage pour respecter la chronologie.\n",
    "2. Split temporel train/test selon `train_ratio` afin d'éviter toute fuite du futur.\n",
    "3. Construction d'un profil utilisateur à partir des interactions de train.\n",
    "4. Définition du *ground truth* : articles cliqués en test pour chaque utilisateur (au moins un).\n",
    "5. Génération de recommandations Top-5 en excluant les articles déjà vus en train.\n",
    "6. Calcul des métriques de ranking (Precision@5, Recall@5, MAP@5, NDCG@5, Coverage@5) et estimation de la latence moyenne sur un échantillon de 500 utilisateurs max.\n",
    "\n",
    "Cette démarche imite un scénario de production : d'abord on respecte le temps, puis on mesure simultanément la qualité des suggestions et le coût de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b13ef4",
   "metadata": {},
   "source": [
    "## Préparation minimale des interactions"
   ]
  },
  {
   "cell_type": "code",
   "id": "45b62c76",
   "metadata": {},
   "source": [
    "\n",
    "# Filtrage k-core itératif pour limiter la sparsité avant le split train/test\n",
    "\n",
    "def iterative_k_core_filter(\n",
    "    df: pd.DataFrame, min_user_interactions: int, min_item_interactions: int\n",
    ") -> pd.DataFrame:\n",
    "    filtered = df.copy()\n",
    "    previous_size = -1\n",
    "    while previous_size != len(filtered):\n",
    "        previous_size = len(filtered)\n",
    "        user_counts = filtered[\"user_id\"].value_counts()\n",
    "        item_counts = filtered[\"article_id\"].value_counts()\n",
    "        filtered = filtered[\n",
    "            filtered[\"user_id\"].isin(user_counts[user_counts >= min_user_interactions].index)\n",
    "            & filtered[\"article_id\"].isin(item_counts[item_counts >= min_item_interactions].index)\n",
    "        ]\n",
    "    return filtered\n",
    "\n",
    "if clicks.empty:\n",
    "    print(\"Dataset clicks vide : saut du filtrage k-core.\")\n",
    "else:\n",
    "    before = (\n",
    "        len(clicks),\n",
    "        clicks[\"user_id\"].nunique(),\n",
    "        clicks[\"article_id\"].nunique(),\n",
    "    )\n",
    "    clicks = iterative_k_core_filter(\n",
    "        clicks,\n",
    "        CONFIG[\"min_user_interactions\"],\n",
    "        CONFIG[\"min_item_interactions\"],\n",
    "    ).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    after = (\n",
    "        len(clicks),\n",
    "        clicks[\"user_id\"].nunique(),\n",
    "        clicks[\"article_id\"].nunique(),\n",
    "    )\n",
    "    print(\n",
    "        \"Filtrage k-core terminé: \"\n",
    "        f\"interactions {before[0]} -> {after[0]}, \"\n",
    "        f\"utilisateurs {before[1]} -> {after[1]}, \"\n",
    "        f\"articles {before[2]} -> {after[2]}\"\n",
    "    )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a99221f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.263614Z",
     "iopub.status.busy": "2025-12-17T10:07:55.263340Z",
     "iopub.status.idle": "2025-12-17T10:07:55.299869Z",
     "shell.execute_reply": "2025-12-17T10:07:55.298572Z"
    }
   },
   "source": [
    "# Split and utility functions\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "MIN_HISTORY = 2\n",
    "\n",
    "\n",
    "def _assert_no_temporal_leakage(train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Vectorized leakage check:\n",
    "    For each user that has both train and test rows, ensure max(train_ts) <= min(test_ts).\n",
    "    \"\"\"\n",
    "    if train_df.empty or test_df.empty:\n",
    "        return\n",
    "\n",
    "    train_max = train_df.groupby(\"user_id\")[\"timestamp\"].max()\n",
    "    test_min = test_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "\n",
    "    joined = pd.concat([train_max.rename(\"train_max\"), test_min.rename(\"test_min\")], axis=1).dropna()\n",
    "    bad = joined[joined[\"train_max\"] > joined[\"test_min\"]]\n",
    "    assert bad.empty, f\"Temporal leakage detected for users: {bad.index.tolist()[:5]}\"\n",
    "\n",
    "\n",
    "def temporal_train_test_split_per_user(df: pd.DataFrame, train_ratio: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split interactions chronologically per user to avoid leakage.\n",
    "\n",
    "    Retro-compatible:\n",
    "    - Same function name/signature and return types.\n",
    "    - Same semantics: per user, earliest interactions go to train, remaining to test.\n",
    "    - Keeps a leakage assert (but vectorized, not O(users * rows)).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        empty = df.iloc[0:0].copy()\n",
    "        return empty, empty\n",
    "\n",
    "    # Sort once globally instead of sorting inside each group\n",
    "    df2 = df.sort_values([\"user_id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # Position inside each user (0..n-1) and per-user cutoff\n",
    "    pos = df2.groupby(\"user_id\").cumcount()\n",
    "    n = df2.groupby(\"user_id\")[\"user_id\"].transform(\"size\")\n",
    "    cutoff = (n * float(train_ratio)).astype(int)\n",
    "\n",
    "    train_mask = pos < cutoff\n",
    "    train_df = df2[train_mask].copy()\n",
    "    test_df = df2[~train_mask].copy()\n",
    "\n",
    "    _assert_no_temporal_leakage(train_df, test_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def build_user_histories(df: pd.DataFrame) -> Dict[int, List[int]]:\n",
    "    \"\"\"Create mapping user -> unique list of articles in chronological order.\n",
    "\n",
    "    Retro-compatible:\n",
    "    - Same output: {user_id: [article_id1, article_id2, ...]} with order preserved and duplicates removed.\n",
    "    - Faster: sort once and drop duplicates per (user_id, article_id) while keeping first occurrence.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    df2 = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "\n",
    "    # Keep first time a user saw an article (preserves chronological order per user)\n",
    "    df2 = df2.drop_duplicates(subset=[\"user_id\", \"article_id\"], keep=\"first\")\n",
    "\n",
    "    # Build dict user -> list[article_id]\n",
    "    # (astype(int) for retro-compat with your previous behavior)\n",
    "    grouped = df2.groupby(\"user_id\")[\"article_id\"].apply(lambda s: [int(x) for x in s.tolist()])\n",
    "    return {int(uid): items for uid, items in grouped.items()}\n",
    "\n",
    "\n",
    "def make_ground_truth(\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    min_history: int,\n",
    ") -> Tuple[Dict[int, List[int]], Dict[int, Set[int]], List[int], float]:\n",
    "    \"\"\"Build user histories and ground truth for evaluation.\n",
    "\n",
    "    Test items not observed in train are filtered out to keep candidates consistent.\n",
    "    Returns the unknown test item rate (fraction of test interactions filtered out).\n",
    "\n",
    "    Retro-compatible:\n",
    "    - Same signature and returned tuple structure.\n",
    "    - Same filtering rules.\n",
    "    \"\"\"\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "\n",
    "    # Preserve your int coercions (important for joins/sets and Surprise adapters)\n",
    "    train[\"user_id\"] = train[\"user_id\"].astype(int)\n",
    "    train[\"article_id\"] = train[\"article_id\"].astype(int)\n",
    "    test[\"user_id\"] = test[\"user_id\"].astype(int)\n",
    "    test[\"article_id\"] = test[\"article_id\"].astype(int)\n",
    "\n",
    "    train_hist = build_user_histories(train)\n",
    "\n",
    "    # Candidate pool from train items\n",
    "    candidate_items_global = [int(i) for i in train[\"article_id\"].dropna().unique().tolist()]\n",
    "    candidate_set = set(candidate_items_global)\n",
    "\n",
    "    unknown_mask = ~test[\"article_id\"].isin(candidate_set)\n",
    "    unknown_test_items_rate = float(unknown_mask.mean()) if len(test) else 0.0\n",
    "\n",
    "    test_hist = build_user_histories(test)\n",
    "\n",
    "    filtered_gt: Dict[int, Set[int]] = {}\n",
    "    for user_id, items in test_hist.items():\n",
    "        filtered_items = {int(i) for i in items if int(i) in candidate_set}\n",
    "        if not filtered_items:\n",
    "            continue\n",
    "        if len(train_hist.get(int(user_id), [])) < int(min_history):\n",
    "            continue\n",
    "        filtered_gt[int(user_id)] = filtered_items\n",
    "\n",
    "    return train_hist, filtered_gt, candidate_items_global, unknown_test_items_rate\n",
    "\n",
    "\n",
    "# --- Usage (unchanged) ---\n",
    "\n",
    "train_df, test_df = temporal_train_test_split_per_user(clicks, CONFIG[\"train_ratio\"])\n",
    "train_histories, ground_truth, candidate_items_global, unknown_test_items_rate = make_ground_truth(\n",
    "    train_df, test_df, MIN_HISTORY\n",
    ")\n",
    "eval_users = sorted(ground_truth.keys())\n",
    "candidate_items = candidate_items_global\n",
    "print(\n",
    "    \"Train size: \"\n",
    "    f\"{len(train_df)}, Test size: {len(test_df)}, Users for eval: {len(eval_users)}, \"\n",
    "    f\"Unknown test items rate (filtered): {unknown_test_items_rate:.4f}\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e35d6c96",
   "metadata": {},
   "source": [
    "## Métriques utilisées\n",
    "\n",
    "* **Precision@5** : part des recommandations top-5 qui sont réellement cliquées (plus c'est haut, plus le Top-5 est précis).\n",
    "* **Recall@5** : part des clics test retrouvés dans le Top-5 (mesure la couverture de ce que l'utilisateur aime).\n",
    "* **MAP@5** : moyenne de la précision cumulée à chaque clic retrouvé ; récompense les bonnes positions dans la liste.\n",
    "* **NDCG@5** : pondère chaque clic par sa position (gain décroissant) et normalise par le meilleur score possible ; idéal pour comparer des classements.\n",
    "* **Coverage@5** : proportion d'articles différents recommandés sur l'ensemble des utilisateurs (diversité du catalogue).\n",
    "* **Latence par utilisateur** : temps moyen pour produire le Top-5 (important pour une API temps réel).\n",
    "* **RMSE** : erreur quadratique moyenne sur les prédictions de note ; résume l'écart global entre les estimations du modèle et les clics réels.\n",
    "* **MAE** : erreur absolue moyenne ; met en avant l'erreur moyenne sans amplifier les grands écarts."
   ]
  },
  {
   "cell_type": "code",
   "id": "2bb3c598",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.302388Z",
     "iopub.status.busy": "2025-12-17T10:07:55.302132Z",
     "iopub.status.idle": "2025-12-17T10:07:55.309858Z",
     "shell.execute_reply": "2025-12-17T10:07:55.308995Z"
    }
   },
   "source": [
    "# Metrics\n",
    "\n",
    "def precision_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
    "    \"\"\"Precision@k for a single user (binary relevance).\"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    rec_k = recommended[:k]\n",
    "    hits = sum(1 for item in rec_k if item in relevant)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
    "    \"\"\"Recall@k for a single user (binary relevance).\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    rec_k = recommended[:k]\n",
    "    hits = sum(1 for item in rec_k if item in relevant)\n",
    "    return hits / len(relevant)\n",
    "\n",
    "\n",
    "def average_precision_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
    "    \"\"\"Average precision@k for a single user (binary relevance).\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k)\n",
    "\n",
    "\n",
    "def dcg_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
    "    \"\"\"Discounted cumulative gain (binary relevance).\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            dcg += 1 / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
    "    \"\"\"Normalized DCG with binary relevance and capped ideal DCG.\"\"\"\n",
    "    ideal_hits = min(len(relevant), k)\n",
    "    if ideal_hits == 0:\n",
    "        return 0.0\n",
    "    ideal_dcg = sum(1 / np.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
    "    return dcg_at_k(recommended, relevant, k) / ideal_dcg\n",
    "\n",
    "\n",
    "def coverage_at_k(all_recommendations: List[List[int]], candidate_items: List[int], k: int) -> float:\n",
    "    \"\"\"Coverage of unique recommended items over global candidates.\"\"\"\n",
    "    assert candidate_items, \"candidate_items_global must not be empty\"\n",
    "    rec_items = set()\n",
    "    for rec in all_recommendations:\n",
    "        rec_items.update(rec[:k])\n",
    "    assert rec_items.issubset(set(candidate_items)), \"Coverage union must be subset of candidates\"\n",
    "    return len(rec_items) / len(candidate_items)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33cb2d3e",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires pour les recommanders"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7a19e91",
   "metadata": {},
   "source": [
    "\n",
    "# Fonctions classiques (popularité, similarité, SVD léger) utilisées par les baselines\n",
    "\n",
    "def build_global_popularity(train: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Retourne les articles triés par nombre de clics.\"\"\"\n",
    "    return train.groupby(\"article_id\").size().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "def build_recent_popularity(train: pd.DataFrame, window_days: int) -> List[int]:\n",
    "    \"\"\"Retourne les articles populaires sur la dernière fenêtre glissante.\"\"\"\n",
    "    max_time = train[\"timestamp\"].max()\n",
    "    window_start = max_time - pd.Timedelta(days=window_days)\n",
    "    recent = train[train[\"timestamp\"] >= window_start]\n",
    "    if recent.empty:\n",
    "        return build_global_popularity(train)\n",
    "    counts = recent.groupby(\"article_id\")[\"timestamp\"].agg([\"size\", \"max\"])\n",
    "    ranked = counts.sort_values(by=[\"size\", \"max\"], ascending=[False, False])\n",
    "    return ranked.index.tolist()\n",
    "\n",
    "\n",
    "def build_covisit_graph(train: pd.DataFrame) -> Dict[int, Dict[int, int]]:\n",
    "    \"\"\"Construire un graphe de co-visitation basé sur l'historique utilisateur.\"\"\"\n",
    "    graph: Dict[int, Dict[int, int]] = {}\n",
    "    for _, group in train.groupby(\"user_id\"):\n",
    "        items = group.sort_values(\"timestamp\")[\"article_id\"].tolist()\n",
    "        unique_items = list(dict.fromkeys(items))\n",
    "        for i, item_i in enumerate(unique_items):\n",
    "            graph.setdefault(item_i, {})\n",
    "            for item_j in unique_items[i + 1 :]:\n",
    "                graph[item_i][item_j] = graph[item_i].get(item_j, 0) + 1\n",
    "                graph.setdefault(item_j, {})\n",
    "                graph[item_j][item_i] = graph[item_j].get(item_i, 0) + 1\n",
    "    return graph\n",
    "\n",
    "\n",
    "def build_content_embeddings(metadata: pd.DataFrame, pca_components: Optional[int] = None):\n",
    "    \"\"\"Crée des embeddings TF-IDF à partir des colonnes textuelles (avec PCA optionnel).\"\"\"\n",
    "    text_cols = [\n",
    "        c\n",
    "        for c in metadata.columns\n",
    "        if metadata[c].dtype == object and c not in {\"article_id\", \"clicks\"}\n",
    "    ]\n",
    "    non_id_cols = [c for c in metadata.columns if c != \"article_id\"]\n",
    "\n",
    "    if not text_cols and non_id_cols:\n",
    "        print(\"Aucune colonne textuelle : utilisation des colonnes non-ID comme tokens catégoriels.\")\n",
    "        text_cols = non_id_cols\n",
    "\n",
    "    if not text_cols:\n",
    "        raise ValueError(\"Aucune colonne utilisable dans les métadonnées pour construire des embeddings\")\n",
    "\n",
    "    corpus = metadata[text_cols].fillna(\"\")\n",
    "    corpus = corpus.apply(lambda row: \" \".join(f\"{col}_{val}\" for col, val in row.items()), axis=1)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    if pca_components and pca_components < tfidf.shape[1]:\n",
    "        svd = TruncatedSVD(n_components=pca_components, random_state=CONFIG[\"random_seed\"])\n",
    "        reduced = svd.fit_transform(tfidf)\n",
    "        embeddings = normalize(reduced)\n",
    "    else:\n",
    "        embeddings = normalize(tfidf)\n",
    "    ids = metadata[\"article_id\"].tolist()\n",
    "    return embeddings, ids\n",
    "\n",
    "\n",
    "def build_item_similarity(train: pd.DataFrame, metadata: Optional[pd.DataFrame]):\n",
    "    \"\"\"Construit une similarité article-article par contenu ou co-visitation.\"\"\"\n",
    "    if metadata is not None:\n",
    "        try:\n",
    "            embeddings, ids = build_content_embeddings(metadata, CONFIG[\"content_pca_components\"])\n",
    "            similarity: Dict[int, Dict[int, float]] = {}\n",
    "            for i, aid in enumerate(ids):\n",
    "                sims = embeddings @ embeddings[i].T\n",
    "                sims = np.asarray(sims).flatten()\n",
    "                top_idx = np.argsort(-sims)[1:51]\n",
    "                similarity[aid] = {ids[j]: float(sims[j]) for j in top_idx if sims[j] > 0}\n",
    "            return similarity, \"content\"\n",
    "        except Exception as exc:\n",
    "            print(f\"Embeddings de contenu impossibles ({exc}). Bascule sur la co-visitation.\")\n",
    "    graph = build_covisit_graph(train)\n",
    "    similarity = {item: {nbr: float(cnt) for nbr, cnt in neigh.items()} for item, neigh in graph.items()}\n",
    "    return similarity, \"covisitation\"\n",
    "\n",
    "\n",
    "def recommend_from_similarity(\n",
    "    user_id: int,\n",
    "    train_histories: Dict[int, List[int]],\n",
    "    similarity: Dict[int, Dict[int, float]],\n",
    "    candidate_items: List[int],\n",
    "    k: int,\n",
    ") -> List[int]:\n",
    "    \"\"\"Agrège les scores de similarité depuis l'historique utilisateur.\"\"\"\n",
    "    seen = set(train_histories.get(user_id, []))\n",
    "    scores: Dict[int, float] = {}\n",
    "    for item in seen:\n",
    "        for neighbor, sim in similarity.get(item, {}).items():\n",
    "            if neighbor in seen:\n",
    "                continue\n",
    "            scores[neighbor] = scores.get(neighbor, 0.0) + sim\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recs = [it for it, _ in ranked if it not in seen]\n",
    "    if len(recs) < k:\n",
    "        for c in candidate_items:\n",
    "            if c not in seen and c not in recs:\n",
    "                recs.append(c)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "    return recs[:k]\n",
    "\n",
    "\n",
    "def build_collaborative_svd(train: pd.DataFrame, n_components: int):\n",
    "    \"\"\"Entraîne un SVD implicite léger et retourne une fonction de recommandation.\"\"\"\n",
    "    user_codes, user_index = pd.factorize(train[\"user_id\"], sort=True)\n",
    "    item_codes, item_index = pd.factorize(train[\"article_id\"], sort=True)\n",
    "\n",
    "    interactions = pd.DataFrame({\"user_idx\": user_codes, \"item_idx\": item_codes}).drop_duplicates()\n",
    "    data = np.ones(len(interactions), dtype=np.float32)\n",
    "    mat = sparse.coo_matrix((data, (interactions[\"user_idx\"], interactions[\"item_idx\"])), shape=(len(user_index), len(item_index))).tocsr()\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=CONFIG[\"random_seed\"])\n",
    "    user_factors = svd.fit_transform(mat)\n",
    "    item_factors = svd.components_.T\n",
    "\n",
    "    user_to_idx = {int(uid): int(idx) for idx, uid in enumerate(user_index.tolist())}\n",
    "    items = [int(aid) for aid in item_index.tolist()]\n",
    "\n",
    "    def recommend(user_id: int, seen: set, k: int) -> List[int]:\n",
    "        if user_id not in user_to_idx:\n",
    "            popularity = build_global_popularity(train)\n",
    "            return [it for it in popularity if it not in seen][:k]\n",
    "\n",
    "        u_vec = user_factors[user_to_idx[user_id]]\n",
    "        scores = item_factors @ u_vec\n",
    "        ranked_items = [items[i] for i in np.argsort(-scores)]\n",
    "        return [it for it in ranked_items if it not in seen][:k]\n",
    "\n",
    "    meta = {\"users\": len(user_index), \"items\": len(item_index), \"components\": n_components}\n",
    "    return recommend, meta\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82f04b71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.312384Z",
     "iopub.status.busy": "2025-12-17T10:07:55.312125Z",
     "iopub.status.idle": "2025-12-17T10:07:55.341823Z",
     "shell.execute_reply": "2025-12-17T10:07:55.340786Z"
    }
   },
   "source": [
    "\n",
    "from surprise import Dataset, Reader, KNNBasic, NormalPredictor, SVD\n",
    "from surprise import accuracy\n",
    "\n",
    "\n",
    "def build_surprise_trainset(interactions: pd.DataFrame, *, use_session_rating: bool = False):\n",
    "    if use_session_rating:\n",
    "        weighted = interactions.copy()\n",
    "        weighted = ensure_context_columns(weighted)\n",
    "        weighted[\"rating\"] = session_weight_from_size(weighted.get(\"session_size\"))\n",
    "        aggregated = (\n",
    "            weighted.groupby([\"user_id\", \"article_id\"])\n",
    "            .agg(rating=(\"rating\", \"mean\"), last_ts=(\"timestamp\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        aggregated = (\n",
    "            interactions.groupby([\"user_id\", \"article_id\"])\n",
    "            .agg(clicks=(\"article_id\", \"size\"), last_ts=(\"timestamp\", \"max\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "        if aggregated.empty:\n",
    "            raise ValueError(\"Impossible de construire un trainset Surprise sans interactions\")\n",
    "\n",
    "        min_ts = aggregated[\"last_ts\"].min()\n",
    "        max_ts = aggregated[\"last_ts\"].max()\n",
    "        span_seconds = max((max_ts - min_ts).total_seconds(), 1.0)\n",
    "        recency = (aggregated[\"last_ts\"] - min_ts).dt.total_seconds() / span_seconds\n",
    "        aggregated[\"rating\"] = np.log1p(aggregated[\"clicks\"]) + 0.5 * recency\n",
    "\n",
    "    min_rating = float(aggregated[\"rating\"].min())\n",
    "    max_rating = float(aggregated[\"rating\"].max())\n",
    "    if max_rating == min_rating:\n",
    "        max_rating = min_rating + 1.0\n",
    "\n",
    "    reader = Reader(rating_scale=(min_rating, max_rating))\n",
    "    return Dataset.load_from_df(\n",
    "        aggregated[[\"user_id\", \"article_id\", \"rating\"]], reader\n",
    "    ).build_full_trainset()\n",
    "\n",
    "\n",
    "surprise_trainset = build_surprise_trainset(train_df, use_session_rating=False)\n",
    "surprise_items = [int(surprise_trainset.to_raw_iid(iid)) for iid in surprise_trainset.all_items()]\n",
    "popularity_order = build_global_popularity(train_df)\n",
    "popularity_rank = {int(aid): rank for rank, aid in enumerate(popularity_order)}\n",
    "\n",
    "\n",
    "# Adapters and baselines for evaluation\n",
    "\n",
    "def seen_recommender_adapter(recommend_func, k: int):\n",
    "    \"\"\"Wrap recommenders that expect (user_id, seen, k).\"\"\"\n",
    "    def wrapped(user_id: int, candidates_u: list[int], train_histories: Dict[int, List[int]]):\n",
    "        seen = set(train_histories.get(user_id, []))\n",
    "        return recommend_func(user_id, seen, k)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def random_recommender(user_id: int, candidates_u: list[int], train_histories: Dict[int, List[int]]):\n",
    "    \"\"\"Return a random ranking over the user candidate set.\"\"\"\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"] + int(user_id))\n",
    "    return [int(i) for i in rng.permutation(candidates_u)]\n",
    "\n",
    "\n",
    "def most_popular_recommender(user_id: int, candidates_u: list[int], train_histories: Dict[int, List[int]]):\n",
    "    \"\"\"Return candidates sorted by global popularity from train.\"\"\"\n",
    "    candidate_set = set(candidates_u)\n",
    "    return [int(i) for i in popularity_order if int(i) in candidate_set]\n",
    "# Chaque algorithme utilise un tie-breaker différent pour éviter des tops identiques en cas d'égalité\n",
    "\n",
    "\n",
    "def wrap_surprise_recommender(algo, label: str, *, tie_breaker=None, trainset=None, items=None):\n",
    "    current_trainset = trainset or surprise_trainset\n",
    "    current_items = items or surprise_items\n",
    "    algo.fit(current_trainset)\n",
    "\n",
    "    is_normal = isinstance(algo, NormalPredictor)\n",
    "    is_knn = hasattr(algo, \"get_neighbors\") and hasattr(algo, \"sim\")\n",
    "    neighbor_cache: dict[int, list[int]] = {}\n",
    "    sim_matrix = getattr(algo, \"sim\", None)\n",
    "\n",
    "    fallback_sorted_items = list(current_items)\n",
    "    if tie_breaker:\n",
    "        fallback_sorted_items = sorted(\n",
    "            fallback_sorted_items,\n",
    "            key=lambda iid: tie_breaker(iid),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "    if is_knn and sim_matrix is not None:\n",
    "        max_neighbors = getattr(algo, \"k\", 40)\n",
    "        for inner_iid in current_trainset.all_items():\n",
    "            raw_iid = int(current_trainset.to_raw_iid(inner_iid))\n",
    "            inner_neighbors = algo.get_neighbors(inner_iid, k=max_neighbors)\n",
    "            neighbor_cache[raw_iid] = [\n",
    "                int(current_trainset.to_raw_iid(neighbor))\n",
    "                for neighbor in inner_neighbors\n",
    "                if neighbor != inner_iid\n",
    "            ]\n",
    "\n",
    "    def recommend(user_id: int, seen: set, k: int) -> List[int]:\n",
    "        raw_uid = int(user_id)\n",
    "        scored: list[tuple[int, float]] = []\n",
    "\n",
    "        if is_knn:\n",
    "            if not seen:\n",
    "                return [iid for iid in fallback_sorted_items if iid not in seen][:k]\n",
    "\n",
    "            candidate_scores: Counter[int] = Counter()\n",
    "            inner_seen: dict[int, int] = {}\n",
    "            for seen_item in seen:\n",
    "                try:\n",
    "                    inner_seen[seen_item] = current_trainset.to_inner_iid(str(seen_item))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            for seen_item, inner_seen_id in inner_seen.items():\n",
    "                neighbors = neighbor_cache.get(seen_item, [])\n",
    "                for neighbor_raw in neighbors:\n",
    "                    if neighbor_raw in seen:\n",
    "                        continue\n",
    "                    try:\n",
    "                        neighbor_inner = current_trainset.to_inner_iid(str(neighbor_raw))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    sim = float(sim_matrix[inner_seen_id, neighbor_inner])\n",
    "                    if np.isfinite(sim):\n",
    "                        candidate_scores[neighbor_raw] += sim\n",
    "\n",
    "            if candidate_scores:\n",
    "                scored = list(candidate_scores.items())\n",
    "            else:\n",
    "                return [iid for iid in fallback_sorted_items if iid not in seen][:k]\n",
    "\n",
    "        if not scored:\n",
    "            if is_normal:\n",
    "                base_score = float(getattr(algo, \"mu\", 0.0))\n",
    "                scored = [(iid, base_score) for iid in current_items if iid not in seen]\n",
    "            else:\n",
    "                scored = []\n",
    "                for iid in current_items:\n",
    "                    if iid in seen:\n",
    "                        continue\n",
    "                    pred = algo.predict(raw_uid, int(iid), verbose=False)\n",
    "                    scored.append((iid, float(pred.est)))\n",
    "\n",
    "        if not scored:\n",
    "            return [it for it in current_items if it not in seen][:k]\n",
    "\n",
    "        def sort_key(item_score):\n",
    "            iid, score = item_score\n",
    "            tie = tie_breaker(iid) if tie_breaker else 0.0\n",
    "            return (score, tie)\n",
    "\n",
    "        scored.sort(key=sort_key, reverse=True)\n",
    "        return [it for it, _ in scored[:k]]\n",
    "\n",
    "    meta = {\"algo\": label, \"n_items\": len(current_items), \"estimator\": algo, \"trainset\": current_trainset}\n",
    "    return recommend, meta\n",
    "\n",
    "# Configuration commune\n",
    "K = CONFIG[\"k\"]\n",
    "\n",
    "# Modèles Surprise prêts à l'emploi\n",
    "popularity_recommender, pop_meta = wrap_surprise_recommender(\n",
    "    NormalPredictor(),\n",
    "    \"NormalPredictor (baseline)\",\n",
    "    tie_breaker=lambda iid: -popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "itemknn_recommender, itemknn_meta = wrap_surprise_recommender(\n",
    "    KNNBasic(\n",
    "        k=60,\n",
    "        min_k=2,\n",
    "        sim_options={\"name\": \"pearson_baseline\", \"user_based\": False, \"min_support\": 2, \"n_jobs\": -1},\n",
    "    ),\n",
    "    \"KNNBasic item-based (pearson baseline)\",\n",
    "    tie_breaker=lambda iid: -popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "svd_recommender, svd_meta = wrap_surprise_recommender(\n",
    "    SVD(\n",
    "        n_factors=CONFIG[\"svd_components\"],\n",
    "        n_epochs=35,\n",
    "        reg_all=0.06,\n",
    "        lr_all=0.004,\n",
    "        random_state=CONFIG[\"random_seed\"],\n",
    "    ),\n",
    "    \"SVD collaboratif (facteurs latents)\",\n",
    "    tie_breaker=lambda iid: popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5f8700d",
   "metadata": {},
   "source": [
    "# Mesures d'erreur pour les algorithmes Surprise\n",
    "\n",
    "def surprise_error_metrics(estimator, test_df: pd.DataFrame, candidate_pool=None) -> dict[str, float]:\n",
    "    \"\"\"Compute RMSE/MAE on the test split for a fitted Surprise estimator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : surprise.AlgoBase\n",
    "        Trained Surprise model with a ``predict`` method.\n",
    "    test_df : pd.DataFrame\n",
    "        Test interactions containing ``user_id`` and ``article_id``. If\n",
    "        ``session_size`` is available, it will be converted to a continuous\n",
    "        rating using ``session_weight_from_size`` (fallback 1.0).\n",
    "    candidate_pool : Iterable[int], optional\n",
    "        If provided, restrict the evaluated items to this candidate pool.\n",
    "    \"\"\"\n",
    "    if test_df.empty:\n",
    "        return {\"rmse\": float(\"nan\"), \"mae\": float(\"nan\")}\n",
    "\n",
    "    candidate_set = set(candidate_pool) if candidate_pool is not None else None\n",
    "    session_sizes = test_df.get(\"session_size\")\n",
    "    ratings = session_weight_from_size(session_sizes) if session_sizes is not None else np.ones(len(test_df), dtype=np.float32)\n",
    "\n",
    "    predictions = []\n",
    "    for (uid, iid, true_rating) in zip(test_df[\"user_id\"], test_df[\"article_id\"], ratings):\n",
    "        if candidate_set is not None and iid not in candidate_set:\n",
    "            continue\n",
    "        predictions.append(estimator.predict(int(uid), int(iid), r_ui=float(true_rating), verbose=False))\n",
    "\n",
    "    if not predictions:\n",
    "        return {\"rmse\": float(\"nan\"), \"mae\": float(\"nan\")}\n",
    "\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    mae = accuracy.mae(predictions, verbose=False)\n",
    "    return {\"rmse\": float(rmse), \"mae\": float(mae)}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2a633fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.344174Z",
     "iopub.status.busy": "2025-12-17T10:07:55.343953Z",
     "iopub.status.idle": "2025-12-17T10:07:55.351668Z",
     "shell.execute_reply": "2025-12-17T10:07:55.350647Z"
    }
   },
   "source": [
    "# Evaluation pipeline\n",
    "\n",
    "def deduplicate_preserve_order(items: List[int]) -> List[int]:\n",
    "    \"\"\"Remove duplicates while preserving the original order.\"\"\"\n",
    "    seen: set[int] = set()\n",
    "    deduped: List[int] = []\n",
    "    for item in items:\n",
    "        if item in seen:\n",
    "            continue\n",
    "        seen.add(item)\n",
    "        deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "def normalize_recommendations(raw_recs: List[Union[int, Tuple[int, float]]]) -> List[int]:\n",
    "    \"\"\"Normalize recommender output to a ranked list of int item ids.\"\"\"\n",
    "    if not raw_recs:\n",
    "        return []\n",
    "    first = raw_recs[0]\n",
    "    if isinstance(first, (tuple, list)):\n",
    "        ranked = sorted(raw_recs, key=lambda x: x[1], reverse=True)\n",
    "        return [int(item) for item, _ in ranked]\n",
    "    return [int(item) for item in raw_recs]\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    name: str,\n",
    "    recommend_func: Callable[[int, List[int], Dict[int, List[int]]], List[Union[int, Tuple[int, float]]]],\n",
    "    train_histories: Dict[int, List[int]],\n",
    "    ground_truth: Dict[int, set[int]],\n",
    "    candidate_items_global: List[int],\n",
    "    k: int,\n",
    "    latency_sample: int = 500,\n",
    "    progress_every: int = 500,\n",
    "    unknown_test_items_rate: float = 0.0,\n",
    "    popularity_fallback: Optional[List[int]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a recommender with ranking metrics and latency estimation.\"\"\"\n",
    "    assert candidate_items_global, \"candidate_items_global must not be empty\"\n",
    "    popularity_fallback = popularity_fallback or list(candidate_items_global)\n",
    "\n",
    "    precisions: List[float] = []\n",
    "    recalls: List[float] = []\n",
    "    maps: List[float] = []\n",
    "    ndcgs: List[float] = []\n",
    "    hitrates: List[int] = []\n",
    "    all_recs: List[List[int]] = []\n",
    "\n",
    "    users = sorted(ground_truth.keys())\n",
    "    total_users = len(users)\n",
    "    empty_candidates_count = 0\n",
    "    short_recs_count = 0\n",
    "    candidate_sizes: List[int] = []\n",
    "    users_evaluated_list: List[int] = []\n",
    "\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "    debug_users = set(rng.choice(users, size=min(5, len(users)), replace=False).tolist()) if users else set()\n",
    "    debug_examples: List[Dict[str, object]] = []\n",
    "\n",
    "    start_eval = time.perf_counter()\n",
    "    for idx, user_id in enumerate(users, start=1):\n",
    "        seen = set(train_histories.get(user_id, []))\n",
    "        candidates_u = [item for item in candidate_items_global if item not in seen]\n",
    "        if not candidates_u:\n",
    "            empty_candidates_count += 1\n",
    "            continue\n",
    "\n",
    "        candidate_sizes.append(len(candidates_u))\n",
    "        users_evaluated_list.append(user_id)\n",
    "        candidate_set_u = set(candidates_u)\n",
    "        raw_recs = recommend_func(user_id, candidates_u, train_histories)\n",
    "        recs = normalize_recommendations(raw_recs)\n",
    "        recs = deduplicate_preserve_order(recs)\n",
    "        recs = [item for item in recs if item in candidate_set_u]\n",
    "\n",
    "        if len(recs) < k:\n",
    "            short_recs_count += 1\n",
    "            fallback = [item for item in popularity_fallback if item in candidate_set_u and item not in recs]\n",
    "            recs.extend(fallback)\n",
    "\n",
    "        recs = recs[: min(k, len(candidates_u))]\n",
    "        all_recs.append(recs)\n",
    "        gt = ground_truth[user_id]\n",
    "\n",
    "        hits_u = sum(1 for item in recs[:k] if item in gt)\n",
    "        precisions.append(hits_u / k)\n",
    "        recalls.append(hits_u / len(gt))\n",
    "        hitrates.append(1 if hits_u > 0 else 0)\n",
    "        maps.append(average_precision_at_k(recs, gt, k))\n",
    "        ndcgs.append(ndcg_at_k(recs, gt, k))\n",
    "\n",
    "        if user_id in debug_users and len(debug_examples) < 5:\n",
    "            debug_examples.append(\n",
    "                {\n",
    "                    \"user_id\": user_id,\n",
    "                    \"train_size\": len(train_histories.get(user_id, [])),\n",
    "                    \"gt_size\": len(gt),\n",
    "                    \"candidate_size\": len(candidates_u),\n",
    "                    \"gt_items\": list(sorted(gt))[:10],\n",
    "                    \"recs\": recs[:k],\n",
    "                    \"hits\": hits_u,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if progress_every and idx % progress_every == 0:\n",
    "            elapsed = time.perf_counter() - start_eval\n",
    "            rate = elapsed / idx\n",
    "            eta = rate * max(total_users - idx, 0)\n",
    "            print(\n",
    "                f\"[{name}] {idx}/{total_users} users processed \"\n",
    "                f\"(elapsed {elapsed:.1f}s, ETA {eta:.1f}s)\"\n",
    "            )\n",
    "\n",
    "    users_evaluated = len(users_evaluated_list)\n",
    "    empty_candidates_rate = empty_candidates_count / max(1, total_users)\n",
    "    short_recs_rate = short_recs_count / max(1, users_evaluated)\n",
    "\n",
    "    if candidate_sizes:\n",
    "        candidate_min = int(np.min(candidate_sizes))\n",
    "        candidate_median = float(np.median(candidate_sizes))\n",
    "        candidate_max = int(np.max(candidate_sizes))\n",
    "    else:\n",
    "        candidate_min = candidate_median = candidate_max = 0\n",
    "\n",
    "    coverage = coverage_at_k(all_recs, candidate_items_global, k)\n",
    "    hitrate = float(np.mean(hitrates)) if hitrates else 0.0\n",
    "\n",
    "    sample_users = users[: min(latency_sample, len(users))]\n",
    "    start = time.perf_counter()\n",
    "    for user_id in sample_users:\n",
    "        seen = set(train_histories.get(user_id, []))\n",
    "        candidates_u = [item for item in candidate_items_global if item not in seen]\n",
    "        if not candidates_u:\n",
    "            continue\n",
    "        _ = recommend_func(user_id, candidates_u, train_histories)\n",
    "    latency = (time.perf_counter() - start) / max(1, len(sample_users))\n",
    "    total_eval_time = time.perf_counter() - start_eval\n",
    "\n",
    "    print(\n",
    "        f\"[{name}] users_evaluated={users_evaluated}, \"\n",
    "        f\"short_recs_rate={short_recs_rate:.4f}, \"\n",
    "        f\"empty_candidates_rate={empty_candidates_rate:.4f}, \"\n",
    "        f\"unknown_test_items_rate={unknown_test_items_rate:.4f}, \"\n",
    "        f\"candidate_size(min/median/max)={candidate_min}/{candidate_median:.1f}/{candidate_max}, \"\n",
    "        f\"precision@k={float(np.mean(precisions)) if precisions else 0.0:.4f}, \"\n",
    "        f\"recall@k={float(np.mean(recalls)) if recalls else 0.0:.4f}, \"\n",
    "        f\"map@k={float(np.mean(maps)) if maps else 0.0:.4f}, \"\n",
    "        f\"ndcg@k={float(np.mean(ndcgs)) if ndcgs else 0.0:.4f}, \"\n",
    "        f\"hitrate@k={hitrate:.4f}, \"\n",
    "        f\"coverage@k={coverage:.4f}\"\n",
    "    )\n",
    "\n",
    "    if debug_examples:\n",
    "        print(f\"[{name}] Debug examples (seeded):\")\n",
    "        for example in debug_examples:\n",
    "            print(\n",
    "                f\"user={example['user_id']} \"\n",
    "                f\"| train={example['train_size']} \"\n",
    "                f\"| gt={example['gt_size']} \"\n",
    "                f\"| candidates={example['candidate_size']} \"\n",
    "                f\"| gt_items={example['gt_items']} \"\n",
    "                f\"| recs={example['recs']} \"\n",
    "                f\"| hits={example['hits']}\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"users\": users_evaluated,\n",
    "        \"precision@k\": float(np.mean(precisions)) if precisions else 0.0,\n",
    "        \"recall@k\": float(np.mean(recalls)) if recalls else 0.0,\n",
    "        \"map@k\": float(np.mean(maps)) if maps else 0.0,\n",
    "        \"ndcg@k\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
    "        \"hitrate@k\": hitrate,\n",
    "        \"coverage@k\": coverage,\n",
    "        \"latency_per_user_s\": latency,\n",
    "        \"eval_time_s\": total_eval_time,\n",
    "        \"all_recommendations\": all_recs,\n",
    "        \"short_recs_rate\": short_recs_rate,\n",
    "        \"empty_candidates_rate\": empty_candidates_rate,\n",
    "        \"unknown_test_items_rate\": unknown_test_items_rate,\n",
    "        \"candidate_size_min\": candidate_min,\n",
    "        \"candidate_size_median\": candidate_median,\n",
    "        \"candidate_size_max\": candidate_max,\n",
    "        \"users_evaluated_list\": users_evaluated_list,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c71df81d",
   "metadata": {},
   "source": [
    "## Entraînement des systèmes de recommandation\n",
    "\n",
    "Chaque approche est entraînée séparément pour limiter le temps d'exécution de chaque cellule et mieux contextualiser le rôle de chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f3017",
   "metadata": {},
   "source": [
    "### Popularité globale\n",
    "La recommandation par popularité globale trie les articles par volume d'interactions dans l'ensemble d'entraînement. Elle est rapide à calculer (simple agrégation) et sert de baseline robuste pour comparer les modèles plus avancés."
   ]
  },
  {
   "cell_type": "code",
   "id": "77fe52f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.354112Z",
     "iopub.status.busy": "2025-12-17T10:07:55.353890Z",
     "iopub.status.idle": "2025-12-17T10:07:55.359023Z",
     "shell.execute_reply": "2025-12-17T10:07:55.358108Z"
    }
   },
   "source": [
    "\n",
    "# Configuration commune\n",
    "K = CONFIG[\"k\"]\n",
    "\n",
    "# Modèles Surprise prêts à l'emploi\n",
    "popularity_recommender, pop_meta = wrap_surprise_recommender(\n",
    "    NormalPredictor(),\n",
    "    \"NormalPredictor (baseline)\",\n",
    "    tie_breaker=lambda iid: -popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "itemknn_recommender, itemknn_meta = wrap_surprise_recommender(\n",
    "    KNNBasic(\n",
    "        k=60,\n",
    "        min_k=2,\n",
    "        sim_options={\"name\": \"pearson_baseline\", \"user_based\": False, \"min_support\": 2, \"n_jobs\": -1},\n",
    "    ),\n",
    "    \"KNNBasic item-based (pearson baseline)\",\n",
    "    tie_breaker=lambda iid: -popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "svd_recommender, svd_meta = wrap_surprise_recommender(\n",
    "    SVD(\n",
    "        n_factors=CONFIG[\"svd_components\"],\n",
    "        n_epochs=35,\n",
    "        reg_all=0.06,\n",
    "        lr_all=0.004,\n",
    "        random_state=CONFIG[\"random_seed\"],\n",
    "    ),\n",
    "    \"SVD collaboratif (facteurs latents)\",\n",
    "    tie_breaker=lambda iid: popularity_rank.get(int(iid), len(popularity_rank)),\n",
    ")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b431ac4",
   "metadata": {},
   "source": [
    "### Popularité récente\n",
    "Cette variante privilégie la fraîcheur en filtrant les interactions sur une fenêtre temporelle avant de trier les articles par fréquence. Utile pour capter les tendances du moment, au prix d'un recalcul plus fréquent de la fenêtre glissante."
   ]
  },
  {
   "cell_type": "code",
   "id": "13812520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.361539Z",
     "iopub.status.busy": "2025-12-17T10:07:55.361319Z",
     "iopub.status.idle": "2025-12-17T10:07:55.370933Z",
     "shell.execute_reply": "2025-12-17T10:07:55.370040Z"
    }
   },
   "source": [
    "# Popularité récente\n",
    "recent_rank = build_recent_popularity(train_df, CONFIG[\"recent_window_days\"])\n",
    "\n",
    "def recent_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return [it for it in recent_rank if it not in seen][:k]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8fa4833",
   "metadata": {},
   "source": [
    "### Collaborative (SVD)\n",
    "Le filtrage collaboratif factorise la matrice utilisateur-item (SVD) pour capturer des préférences latentes. L'entraînement est plus long que les méthodes de popularité ou de similarité de contenu, mais il modélise mieux les affinités implicites entre utilisateurs et articles."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ffed396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.373125Z",
     "iopub.status.busy": "2025-12-17T10:07:55.372900Z",
     "iopub.status.idle": "2025-12-17T10:07:55.394728Z",
     "shell.execute_reply": "2025-12-17T10:07:55.393807Z"
    }
   },
   "source": [
    "# Filtrage collaboratif (SVD)\n",
    "collab_recommend, collab_meta = build_collaborative_svd(train_df, CONFIG[\"svd_components\"])\n",
    "\n",
    "def collaborative_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return collab_recommend(user_id, seen, k)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79d92ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.398003Z",
     "iopub.status.busy": "2025-12-17T10:07:55.397764Z",
     "iopub.status.idle": "2025-12-17T10:07:55.439170Z",
     "shell.execute_reply": "2025-12-17T10:07:55.438405Z"
    }
   },
   "source": [
    "# Modèles co-visitation désactivés au profit de Surprise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f83ee1f5",
   "metadata": {},
   "source": [
    "### Contenu (similarité article-article)\n",
    "Un modèle basé contenu construit une matrice de similarité entre articles à partir des métadonnées. Les recommandations se font en projetant l'historique utilisateur vers les items proches dans cet espace. Ce calcul peut être plus coûteux car il nécessite la vectorisation et le produit croisé des articles."
   ]
  },
  {
   "cell_type": "code",
   "id": "dc3f20fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.442352Z",
     "iopub.status.busy": "2025-12-17T10:07:55.441747Z",
     "iopub.status.idle": "2025-12-17T10:07:55.445954Z",
     "shell.execute_reply": "2025-12-17T10:07:55.445409Z"
    }
   },
   "source": [
    "# Initialiser un conteneur de résultats pour chaque entraînement\n",
    "results = []\n",
    "step_results = []\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb98dec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.448407Z",
     "iopub.status.busy": "2025-12-17T10:07:55.448189Z",
     "iopub.status.idle": "2025-12-17T10:07:55.500220Z",
     "shell.execute_reply": "2025-12-17T10:07:55.498609Z"
    }
   },
   "source": [
    "# Recommandation basée contenu (désactivable)\n",
    "ENABLE_CONTENT_MODEL = True  # Passer à True pour activer le calcul de similarité contenu\n",
    "\n",
    "if ENABLE_CONTENT_MODEL:\n",
    "    item_similarity, sim_mode = build_item_similarity(train_df, metadata)\n",
    "\n",
    "    def content_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "        return recommend_from_similarity(user_id, train_histories, item_similarity, candidate_items, k)\n",
    "else:\n",
    "    sim_mode = \"désactivé\"\n",
    "    content_recommender = None\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18f18670",
   "metadata": {},
   "source": [
    "## Entraînements séparés\n",
    "\n",
    "Les trois stratégies Surprise sont exécutées dans des cellules distinctes afin de pouvoir lancer, arrêter ou relancer chaque bloc indépendamment. Cela évite d'attendre l'ensemble du pipeline quand un seul entraînement est nécessaire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a2100",
   "metadata": {},
   "source": [
    "### Entraînement 1 : Baseline Surprise (NormalPredictor)\n",
    "\n",
    "Ce bloc entraîne le modèle de base `NormalPredictor` de Surprise et calcule Precision@K, Recall@K, MAP@K, NDCG@K, couverture, latence moyenne ainsi que RMSE et MAE sur le jeu de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d861c1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.503322Z",
     "iopub.status.busy": "2025-12-17T10:07:55.502453Z",
     "iopub.status.idle": "2025-12-17T10:07:55.517553Z",
     "shell.execute_reply": "2025-12-17T10:07:55.516294Z"
    }
   },
   "source": [
    "random_result = evaluate_model(\n",
    "    \"Random (uniform)\",\n",
    "    random_recommender,\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "results.append(random_result)\n",
    "\n",
    "most_popular_result = evaluate_model(\n",
    "    \"Most-popular (train)\",\n",
    "    most_popular_recommender,\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "results.append(most_popular_result)\n",
    "\n",
    "if random_result[\"coverage@k\"] < 0.01 or most_popular_result[\"hitrate@k\"] < random_result[\"hitrate@k\"]:\n",
    "    print(\n",
    "        \"[WARN] Baseline sanity check failed: random coverage near 0 or \"\n",
    "        \"most-popular hitrate worse than random. Evaluation may be broken.\"\n",
    "    )\n",
    "\n",
    "popularity_result = evaluate_model(\n",
    "    \"Baseline Surprise - NormalPredictor\",\n",
    "    seen_recommender_adapter(popularity_recommender, K),\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "\n",
    "pop_meta_errors = surprise_error_metrics(\n",
    "    pop_meta[\"estimator\"], test_df, candidate_pool=candidate_items_global\n",
    ")\n",
    "popularity_result.update(pop_meta_errors)\n",
    "results.append(popularity_result)\n",
    "pd.DataFrame([popularity_result])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7d9c4fe",
   "metadata": {},
   "source": [
    "### Entraînement 2 : KNN item-based (Surprise)\n",
    "\n",
    "Ce bloc exécute `KNNBasic` en mode item-based avec une similarité **Pearson baseline**, 60 voisins\n",
    "(`k=60`, `min_k=2`, `min_support=2`). Cette configuration force le modèle à exploiter des co-cliques\n",
    "pour sortir des simples effets de popularité, afin d'obtenir des recommandations distinctes du SVD.\n",
    "\n",
    "Astuce performance : `n_jobs=-1` exploite tous les cœurs CPU pour la matrice de similarité Surprise, ce qui réduit nettement le temps de fit sur de gros catalogues (le modèle reste CPU-only)."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c9df9f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.535540Z",
     "iopub.status.busy": "2025-12-17T10:07:55.535328Z",
     "iopub.status.idle": "2025-12-17T10:07:55.566970Z",
     "shell.execute_reply": "2025-12-17T10:07:55.566381Z"
    }
   },
   "source": [
    "if False :\n",
    "    item2item_result = evaluate_model(\n",
    "        \"Modèle KNNBasic item-based (Pearson baseline)\",\n",
    "        seen_recommender_adapter(itemknn_recommender, K),\n",
    "        train_histories,\n",
    "        ground_truth,\n",
    "        candidate_items_global,\n",
    "        K,\n",
    "        unknown_test_items_rate=unknown_test_items_rate,\n",
    "        popularity_fallback=popularity_order,\n",
    "    )\n",
    "\n",
    "    itemknn_meta_errors = surprise_error_metrics(\n",
    "        itemknn_meta[\"estimator\"], test_df, candidate_pool=candidate_items_global\n",
    "    )\n",
    "    item2item_result.update(itemknn_meta_errors)\n",
    "    results.append(item2item_result)\n",
    "    pd.DataFrame([item2item_result])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "248cbed2",
   "metadata": {},
   "source": [
    "### Entraînement 3 : SVD Surprise\n",
    "\n",
    "Ce bloc entraîne un SVD implicite (facteurs latents) avec 64 dimensions, davantage d'itérations et une\n",
    "régularisation renforcée (`n_epochs=35`, `reg_all=0.06`, `lr_all=0.004`). L'objectif est d'obtenir un\n",
    "profil utilisateur/item plus contrasté que le KNN de voisinage.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73526056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.569369Z",
     "iopub.status.busy": "2025-12-17T10:07:55.569131Z",
     "iopub.status.idle": "2025-12-17T10:07:55.583727Z",
     "shell.execute_reply": "2025-12-17T10:07:55.583034Z"
    }
   },
   "source": [
    "svd_result = evaluate_model(\n",
    "    \"Modèle SVD Surprise (facteurs latents)\",\n",
    "    seen_recommender_adapter(svd_recommender, K),\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "\n",
    "svd_meta_errors = surprise_error_metrics(\n",
    "    svd_meta[\"estimator\"], test_df, candidate_pool=candidate_items_global\n",
    ")\n",
    "svd_result.update(svd_meta_errors)\n",
    "results.append(svd_result)\n",
    "pd.DataFrame([svd_result])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84a5dc70",
   "metadata": {},
   "source": [
    "\n",
    "### Session-size weighting and LightFM item-to-item\n",
    "\n",
    "`session_size` is turned into a relevance weight with **1 / log1p(session_size)** to dampen\n",
    "very long sessions while keeping short, focused sessions influential. The LightFM-style\n",
    "item-to-item model trains latent item vectors on these weighted interactions and enriches\n",
    "user representations with aggregated context features (environment, device, OS, country,\n",
    "region, referrer). Recommendations then come from cosine neighbors in that latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e59f7377",
   "metadata": {},
   "source": [
    "\n",
    "# Session-weighted Surprise SVD and LightFM item-to-item setup\n",
    "svd_session_trainset = build_surprise_trainset(\n",
    "    train_df, use_session_rating=CONFIG[\"svd_use_session_rating\"]\n",
    ")\n",
    "svd_session_items = [int(svd_session_trainset.to_raw_iid(iid)) for iid in svd_session_trainset.all_items()]\n",
    "svd_session_recommender, svd_session_meta = wrap_surprise_recommender(\n",
    "    SVD(n_factors=CONFIG[\"svd_components\"], n_epochs=35, reg_all=0.06, lr_all=0.004, random_state=CONFIG[\"random_seed\"]),\n",
    "    \"Modèle SVD Surprise (session_weighted)\",\n",
    "    tie_breaker=lambda iid: -popularity_rank.get(int(iid), len(popularity_rank)),\n",
    "    trainset=svd_session_trainset,\n",
    "    items=svd_session_items,\n",
    ")\n",
    "\n",
    "lightfm_interactions, lightfm_weights, lightfm_user_features, lightfm_item_ids = build_interaction_matrices(\n",
    "    train_df,\n",
    "    CONTEXT_COLUMNS,\n",
    "    use_user_features=CONFIG[\"lightfm_use_user_features\"],\n",
    ")\n",
    "lightfm_model = LightFMApproximator(\n",
    "    n_components=CONFIG[\"lightfm_components\"],\n",
    "    epochs=15,\n",
    "    random_state=CONFIG[\"random_seed\"],\n",
    ").fit(\n",
    "    lightfm_interactions,\n",
    "    sample_weight=lightfm_weights,\n",
    "    user_features=lightfm_user_features,\n",
    ")\n",
    "_, lightfm_item_embeddings = lightfm_model.get_item_representations()\n",
    "lightfm_neighbors = precompute_item_neighbors(\n",
    "    lightfm_item_embeddings, lightfm_item_ids, top_n=CONFIG[\"lightfm_item_neighbors\"]\n",
    ")\n",
    "\n",
    "\n",
    "def svd_score_candidates(user_id: int, estimator, candidates: list[int], seen: set) -> Dict[int, float]:\n",
    "    scores: Dict[int, float] = {}\n",
    "    for iid in candidates:\n",
    "        if iid in seen:\n",
    "            continue\n",
    "        pred = estimator.predict(int(user_id), int(iid), verbose=False)\n",
    "        scores[int(iid)] = float(pred.est)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def minmax_normalize(scores: Dict[int, float]) -> Dict[int, float]:\n",
    "    if not scores:\n",
    "        return {}\n",
    "    values = list(scores.values())\n",
    "    min_v, max_v = min(values), max(values)\n",
    "    if max_v == min_v:\n",
    "        return {i: 0.0 for i in scores}\n",
    "    return {i: (v - min_v) / (max_v - min_v) for i, v in scores.items()}\n",
    "\n",
    "\n",
    "def lightfm_item2item_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    scores = score_from_neighbors(train_histories.get(user_id, []), lightfm_neighbors, seen)\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recs = [it for it, _ in ranked if it not in seen][:k]\n",
    "    if len(recs) < k:\n",
    "        for cand in candidate_items:\n",
    "            if cand not in seen and cand not in recs:\n",
    "                recs.append(cand)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "    return recs[:k]\n",
    "\n",
    "\n",
    "def hybrid_svd_item2item(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    svd_scores = svd_score_candidates(user_id, svd_meta[\"estimator\"], candidate_items, seen)\n",
    "    item_scores = score_from_neighbors(train_histories.get(user_id, []), lightfm_neighbors, seen)\n",
    "    svd_norm = minmax_normalize(svd_scores)\n",
    "    item_norm = minmax_normalize(item_scores)\n",
    "    alpha, beta = CONFIG[\"hybrid_weights\"]\n",
    "    combined_items = set(list(svd_norm.keys()) + list(item_norm.keys()))\n",
    "    combined_scores = {\n",
    "        iid: alpha * svd_norm.get(iid, 0.0) + beta * item_norm.get(iid, 0.0)\n",
    "        for iid in combined_items\n",
    "    }\n",
    "    ranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recs = [it for it, _ in ranked if it not in seen][:k]\n",
    "    if len(recs) < k:\n",
    "        for cand in candidate_items:\n",
    "            if cand not in seen and cand not in recs:\n",
    "                recs.append(cand)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "    return recs[:k]\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e2b377a",
   "metadata": {},
   "source": [
    "\n",
    "# Entraînement 4 : variantes SVD session-weighted, LightFM item2item et hybride\n",
    "svd_session_result = evaluate_model(\n",
    "    \"SVD session_weighted\",\n",
    "    seen_recommender_adapter(svd_session_recommender, K),\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "svd_session_errors = surprise_error_metrics(\n",
    "    svd_session_meta[\"estimator\"], test_df, candidate_pool=candidate_items_global\n",
    ")\n",
    "svd_session_result.update(svd_session_errors)\n",
    "results.append(svd_session_result)\n",
    "\n",
    "lightfm_result = evaluate_model(\n",
    "    \"Item2Item LightFM (latent voisins)\",\n",
    "    seen_recommender_adapter(lightfm_item2item_recommender, K),\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "lightfm_result.update({\"rmse\": float(\"nan\"), \"mae\": float(\"nan\")})\n",
    "results.append(lightfm_result)\n",
    "\n",
    "hybrid_result = evaluate_model(\n",
    "    \"Hybrid SVD60 + Item2Item40\",\n",
    "    seen_recommender_adapter(hybrid_svd_item2item, K),\n",
    "    train_histories,\n",
    "    ground_truth,\n",
    "    candidate_items_global,\n",
    "    K,\n",
    "    unknown_test_items_rate=unknown_test_items_rate,\n",
    "    popularity_fallback=popularity_order,\n",
    ")\n",
    "hybrid_result.update({\"rmse\": float(\"nan\"), \"mae\": float(\"nan\")})\n",
    "results.append(hybrid_result)\n",
    "\n",
    "pd.DataFrame([svd_session_result, lightfm_result, hybrid_result])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04c77eb8",
   "metadata": {},
   "source": [
    "\n",
    "svd_native_label = \"Modèle SVD Surprise (facteurs latents)\"\n",
    "svd_session_label = \"SVD session_weighted\"\n",
    "lightfm_label = \"Item2Item LightFM (latent voisins)\"\n",
    "hybrid_label = \"Hybrid SVD60 + Item2Item40\"\n",
    "\n",
    "comparison_df = results_df[\n",
    "    results_df[\"model\"].isin(\n",
    "        [svd_native_label, svd_session_label, lightfm_label, hybrid_label]\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "comparison_df\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Modèles Surprise uniquement\n",
    "Les anciennes sections E* basées sur la co-visitation sont remplacées par des algorithmes Surprise (NormalPredictor, KNNBasic, SVD)."
   ],
   "id": "bd497f22f84ce254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Variantes co-visitation retirées\n",
    "Nous privilégions désormais les algorithmes Surprise pour assurer une cohérence entre expérimentation et déploiement."
   ],
   "id": "84ad9ce92ac33960"
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.585957Z",
     "iopub.status.busy": "2025-12-17T10:07:55.585654Z",
     "iopub.status.idle": "2025-12-17T10:07:55.616949Z",
     "shell.execute_reply": "2025-12-17T10:07:55.615926Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Les variantes de co-visitation sont remplacées par les modèles Surprise ci-dessus.\n",
   "id": "f83434f9666920c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section hybride supprimée\n",
    "L'hybridation co-visitation + popularité a été remplacée par le modèle SVD Surprise plus flexible."
   ],
   "id": "f8be7a02aeca5c49"
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.620010Z",
     "iopub.status.busy": "2025-12-17T10:07:55.619446Z",
     "iopub.status.idle": "2025-12-17T10:07:55.680096Z",
     "shell.execute_reply": "2025-12-17T10:07:55.679543Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Section hybride supprimée : la bibliothèque Surprise couvre les besoins collaboratifs.\n",
   "id": "ce95950e70d82a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Optuna n'est plus nécessaire pour ce notebook centré sur Surprise.\n",
   "id": "c3f1dfc99393d5ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Résultats consolidés\n",
    "\n",
    "Après exécution des trois blocs d'entraînement ci-dessus, les métriques sont agrégées pour comparer les approches. Chaque ligne du tableau récapitule la précision, le rappel, la MAP, le NDCG, la couverture et la latence moyenne par utilisateur, complétés par le RMSE et la MAE.\n"
   ],
   "id": "b89fe7434a12ff62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "candidate_items = candidate_items_global\n",
    "\n"
   ],
   "id": "739135b4439f08af"
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.683232Z",
     "iopub.status.busy": "2025-12-17T10:07:55.683015Z",
     "iopub.status.idle": "2025-12-17T10:07:55.690434Z",
     "shell.execute_reply": "2025-12-17T10:07:55.689949Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Agréger les métriques une fois les entraînements terminés\n",
    "clean_columns = [\n",
    "    \"model\",\n",
    "    \"users\",\n",
    "    \"precision@k\",\n",
    "    \"recall@k\",\n",
    "    \"map@k\",\n",
    "    \"ndcg@k\",\n",
    "    \"hitrate@k\",\n",
    "    \"coverage@k\",\n",
    "    \"latency_per_user_s\",\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = (\n",
    "    results_df[clean_columns]\n",
    "    .drop_duplicates(subset=[\"model\"])\n",
    "    .sort_values([\"ndcg@k\", \"map@k\"], ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "display(results_df)\n",
    "\n"
   ],
   "id": "314dd47cc1b940f0"
  },
  {
   "cell_type": "code",
   "id": "3bf504b8",
   "metadata": {},
   "source": [
    "# Comparaison rapide des top-5 pour un utilisateur\n",
    "sample_user = eval_users[0] if eval_users else None\n",
    "if sample_user is None:\n",
    "    print(\"Pas d'utilisateur pour comparer\")\n",
    "else:\n",
    "    seen = set(train_histories.get(sample_user, []))\n",
    "    print(f\"Utilisateur de test: {sample_user}\")\n",
    "    print(\"KNNBasic item-based:\", itemknn_recommender(sample_user, seen, 5))\n",
    "    print(\"SVD collaboratif:\", svd_recommender(sample_user, seen, 5))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88fe2d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.692949Z",
     "iopub.status.busy": "2025-12-17T10:07:55.692627Z",
     "iopub.status.idle": "2025-12-17T10:07:55.700350Z",
     "shell.execute_reply": "2025-12-17T10:07:55.699052Z"
    }
   },
   "source": [
    "results_steps = (\n",
    "    results_df\n",
    "    .sort_values([\"ndcg@k\", \"precision@k\"], ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"Tableau comparatif des modèles Surprise (trié sur ndcg@k puis precision@k) :\")\n",
    "results_steps\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b885ba7b7466a8d9",
   "metadata": {},
   "source": [
    "# Métriques détaillées : hitrate, lifts vs baseline et cohortes d'historique\n",
    "train_click_count = train_df.groupby(\"user_id\").size().to_dict()\n",
    "\n",
    "def assign_cohort(clicks: int) -> str:\n",
    "    if 1 <= clicks <= 2:\n",
    "        return \"1-2 clicks\"\n",
    "    if 3 <= clicks <= 9:\n",
    "        return \"3-9 clicks\"\n",
    "    return \"10+ clicks\"\n",
    "\n",
    "user_cohort = {user_id: assign_cohort(train_click_count.get(user_id, 0)) for user_id in train_histories}\n",
    "coverage_lookup = {res[\"model\"]: res.get(\"coverage@k\", np.nan) for res in results}\n",
    "recommendations_by_model = {res[\"model\"]: res.get(\"all_recommendations\", []) for res in results}\n",
    "users_by_model = {res[\"model\"]: res.get(\"users_evaluated_list\", eval_users) for res in results}\n",
    "baseline_label = svd_native_label if svd_native_label in results_df['model'].values else results_df['model'].iloc[0]\n",
    "\n",
    "def safe_lift(value: float, baseline: float) -> float:\n",
    "    if baseline is None or baseline == 0:\n",
    "        return np.nan\n",
    "    return value / baseline\n",
    "\n",
    "cohort_rows = []\n",
    "for model_name, recs in recommendations_by_model.items():\n",
    "    users_for_model = users_by_model.get(model_name, [])\n",
    "    buckets = {\n",
    "        \"ALL\": {\"precisions\": [], \"recalls\": [], \"ndcgs\": [], \"hits\": 0, \"users\": 0},\n",
    "        \"1-2 clicks\": {\"precisions\": [], \"recalls\": [], \"ndcgs\": [], \"hits\": 0, \"users\": 0},\n",
    "        \"3-9 clicks\": {\"precisions\": [], \"recalls\": [], \"ndcgs\": [], \"hits\": 0, \"users\": 0},\n",
    "        \"10+ clicks\": {\"precisions\": [], \"recalls\": [], \"ndcgs\": [], \"hits\": 0, \"users\": 0},\n",
    "    }\n",
    "\n",
    "    for user_id, recs_user in zip(users_for_model, recs):\n",
    "        gt = ground_truth[user_id]\n",
    "        metrics = {\n",
    "            \"precision\": precision_at_k(recs_user, gt, K),\n",
    "            \"recall\": recall_at_k(recs_user, gt, K),\n",
    "            \"ndcg\": ndcg_at_k(recs_user, gt, K),\n",
    "            \"hit\": 1 if set(recs_user[:K]) & set(gt) else 0,\n",
    "        }\n",
    "        labels = [\"ALL\", user_cohort.get(user_id, \"unknown\")]\n",
    "        for label in labels:\n",
    "            bucket = buckets[label]\n",
    "            bucket[\"precisions\"].append(metrics[\"precision\"])\n",
    "            bucket[\"recalls\"].append(metrics[\"recall\"])\n",
    "            bucket[\"ndcgs\"].append(metrics[\"ndcg\"])\n",
    "            bucket[\"hits\"] += metrics[\"hit\"]\n",
    "            bucket[\"users\"] += 1\n",
    "\n",
    "    for cohort, bucket in buckets.items():\n",
    "        users = bucket[\"users\"]\n",
    "        cohort_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"cohort\": cohort,\n",
    "                \"users\": users,\n",
    "                \"precision@k\": float(np.mean(bucket[\"precisions\"])) if users else 0.0,\n",
    "                \"recall@k\": float(np.mean(bucket[\"recalls\"])) if users else 0.0,\n",
    "                \"ndcg@k\": float(np.mean(bucket[\"ndcgs\"])) if users else 0.0,\n",
    "                \"hitrate@k\": bucket[\"hits\"] / users if users else 0.0,\n",
    "                \"coverage@k\": coverage_lookup.get(model_name, np.nan),\n",
    "            }\n",
    "        )\n",
    "\n",
    "cohort_df = pd.DataFrame(cohort_rows)\n",
    "baseline_rows = cohort_df[cohort_df[\"model\"] == baseline_label].set_index(\"cohort\")\n",
    "for metric in [\"precision@k\", \"recall@k\", \"ndcg@k\"]:\n",
    "    cohort_df[f\"lift_{metric}_vs_baseline\"] = cohort_df.apply(\n",
    "        lambda row: safe_lift(\n",
    "            row[metric],\n",
    "            float(baseline_rows.loc[row[\"cohort\"], metric])\n",
    "            if row[\"cohort\"] in baseline_rows.index\n",
    "            else np.nan,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "cohort_df = cohort_df.sort_values([\"cohort\", \"ndcg@k\", \"precision@k\"], ascending=[True, False, False]).reset_index(drop=True)\n",
    "cohort_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2deac6ae",
   "metadata": {},
   "source": [
    "## Analyse & choix du modèle MVP\n",
    "\n",
    "Le classement met en lumière des compromis :\n",
    "- **Pertinence** : la popularité globale obtient le meilleur NDCG@5/MAP@5, signe que trier par volume reste difficile à battre sur ce petit jeu synthétique.\n",
    "- **Diversité** : l'item2item couvre trois fois plus d'articles, ce qui réduit le risque d'effet tunnel.\n",
    "- **Latence** : toutes les approches sont très rapides (millisecondes), la popularité restant la plus simple.\n",
    "\n",
    "Le choix MVP bascule vers la popularité globale uniquement si l'on cherche la pertinence maximale et un déploiement express. Pour un produit, il serait pertinent de tester une hybridation : démarrer par la popularité pour les nouveaux utilisateurs puis basculer vers l'item2item dès que l'historique se construit afin d'augmenter la couverture sans sacrifier la qualité."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbe168bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.702573Z",
     "iopub.status.busy": "2025-12-17T10:07:55.702341Z",
     "iopub.status.idle": "2025-12-17T10:07:55.709380Z",
     "shell.execute_reply": "2025-12-17T10:07:55.708572Z"
    }
   },
   "source": [
    "\n",
    "best_row = results_df.iloc[0]\n",
    "justification = f\"\"\"\n",
    "## Choix du modèle MVP\n",
    "\n",
    "Modèle retenu : **{best_row['model']}**\n",
    "\n",
    "Motifs principaux :\n",
    "- NDCG@5 = {best_row['ndcg@k']:.4f}, MAP@5 = {best_row['map@k']:.4f}, Precision@5 = {best_row['precision@k']:.4f}, Recall@5 = {best_row['recall@k']:.4f}\n",
    "- Couverture = {best_row['coverage@k']:.4f} sur {len(candidate_items)} articles candidats.\n",
    "- Latence moyenne par utilisateur = {best_row['latency_per_user_s']:.6f} s (CPU).\n",
    "- Complexité : implémentation {'optimisée via Surprise (SVD/KNN)' if 'SVD' in best_row['model'] else 'basée sur Surprise'} compatible avec Azure Functions.\n",
    "- Gestion du cold-start utilisateur via popularité globale.\n",
    "\n",
    "Note : ajuster `content_pca_components` pour réduire la taille des embeddings en production si nécessaire.\n",
    "\"\"\"\n",
    "choice_path = Path(CONFIG[\"artifacts_dir\"]) / \"model_choice.md\"\n",
    "choice_path.write_text(justification)\n",
    "print(justification)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33525096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.711931Z",
     "iopub.status.busy": "2025-12-17T10:07:55.711736Z",
     "iopub.status.idle": "2025-12-17T10:07:55.718550Z",
     "shell.execute_reply": "2025-12-17T10:07:55.717809Z"
    }
   },
   "source": [
    "\n",
    "results_path_csv = Path(CONFIG[\"artifacts_dir\"]) / \"results.csv\"\n",
    "results_path_json = Path(CONFIG[\"artifacts_dir\"]) / \"results.json\"\n",
    "results_df.to_csv(results_path_csv, index=False)\n",
    "results_df.to_json(results_path_json, orient=\"records\", lines=True)\n",
    "print(f\"Résultats sauvegardés dans {results_path_csv} et {results_path_json}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7138f859",
   "metadata": {},
   "source": [
    "### Déploiement (application et Azure Functions)\n",
    "\n",
    "Le modèle **SVD Surprise** est exporté pour l'application Flask et la Function Azure. Les\n",
    "hyperparamètres reflètent la configuration du notebook (facteurs latents, lr_all, reg_all), tandis que\n",
    "le modèle KNN reste disponible pour comparaison locale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3854802",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook montre comment comparer des stratégies de recommandation avec une procédure reproductible : split temporel, entraînement, évaluation multi-métriques et sauvegarde des résultats. Les essais révèlent que la popularité globale reste une valeur sûre pour débuter, mais que des modèles plus personnalisés (item2item ou SVD) apportent de la diversité dès que l'on dispose d'historique. Les prochaines étapes naturelles sont d'exécuter les tests sur les vraies données Kaggle, d'ajouter des métriques business (taux de clic simulé, couverture par catégorie) et de prototyper une hybridation popularité + item2item dans une Azure Function pour valider le comportement en production."
   ]
  },
  {
   "cell_type": "code",
   "id": "bd8521c810d4bf26",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
