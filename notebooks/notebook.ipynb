{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773debcd",
   "metadata": {},
   "source": [
    "# Évaluation d'un système de recommandation sur Globocom\n",
    "\n",
    "Notebook autonome pour entraîner et comparer plusieurs approches de recommandation sur le dataset Kaggle **news-portal-user-interactions-by-globocom**."
   ]
  },
  {
   "cell_type": "code",
   "id": "43aa8131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.606683Z",
     "start_time": "2025-12-15T16:16:51.636113Z"
    }
   },
   "source": [
    "\n",
    "# Imports & Config\n",
    "from __future__ import annotations\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"clicks_path\": \"data/clicks_sample.csv\",\n",
    "    \"metadata_path\": \"data/articles_metadata.csv\",\n",
    "    \"artifacts_dir\": \"artifacts/evaluation\",\n",
    "    \"k\": 5,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"recent_window_days\": 7,\n",
    "    \"random_seed\": 42,\n",
    "    \"svd_components\": 64,\n",
    "    \"content_pca_components\": None,\n",
    "}\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "Path(CONFIG[\"artifacts_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config ready\", CONFIG)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready {'clicks_path': 'data/clicks_sample.csv', 'metadata_path': 'data/articles_metadata.csv', 'artifacts_dir': 'artifacts/evaluation', 'k': 5, 'train_ratio': 0.8, 'recent_window_days': 7, 'random_seed': 42, 'svd_components': 64, 'content_pca_components': None}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cc6be6a7",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Ce notebook compare plusieurs stratégies de recommandation pour choisir un Top-5 d'articles par utilisateur. Les textes sont en français tandis que le code et les commentaires restent en anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8cfa",
   "metadata": {},
   "source": [
    "## Données\n\nLes fichiers attendus sont situés dans `data/`. Si un fichier n'est pas trouvé, un jeu de clics synthétique minimal est généré automatiquement pour que le notebook reste exécutable (les messages expliquent comment remplacer par les données Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "id": "116743e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.642709Z",
     "start_time": "2025-12-15T16:16:52.614193Z"
    }
   },
   "source": [
    "# Load data utilities\n\ndef detect_timestamp_column(df: pd.DataFrame) -> str:\n    \"\"\"Detect the timestamp-like column name.\"\"\"\n    candidates = [\"timestamp\", \"click_timestamp\", \"event_time\", \"ts\", \"time\"]\n    for col in df.columns:\n        if col.lower() in candidates:\n            return col\n    raise ValueError(\"No timestamp-like column found. Expected one of: \" + \",\".join(candidates))\n\n\ndef detect_article_column(df: pd.DataFrame) -> str:\n    \"\"\"Detect the article/item column name.\"\"\"\n    candidates = [\"article_id\", \"click_article_id\", \"item_id\", \"content_id\"]\n    for col in df.columns:\n        if col in candidates:\n            return col\n    raise ValueError(\"No article id column found. Expected one of: \" + \",\".join(candidates))\n\n\ndef create_synthetic_clicks(path: str, n_users: int = 50, n_items: int = 120, days: int = 30, interactions_per_user: int = 25) -> pd.DataFrame:\n    \"\"\"Create a small synthetic clicks dataset to keep the notebook runnable.\"\"\"\n    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n    start = pd.Timestamp(\"2022-01-01\")\n    records = []\n    for user in range(1, n_users + 1):\n        offsets = rng.integers(0, days, size=interactions_per_user)\n        timestamps = [start + pd.Timedelta(int(o), unit=\"D\") for o in sorted(offsets.tolist())]\n        articles = rng.integers(1, n_items + 1, size=interactions_per_user)\n        for ts, art in zip(timestamps, articles):\n            records.append({\"user_id\": int(user), \"article_id\": int(art), \"timestamp\": ts})\n    df = pd.DataFrame(records).sort_values(\"timestamp\").reset_index(drop=True)\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    df.to_csv(path, index=False)\n    print(\n        f\"Synthetic clicks dataset created at {path} \"\n        f\"(users={n_users}, items={n_items}, interactions={len(df)})\"\n    )\n    return df\n\n\ndef load_clicks(path: str) -> pd.DataFrame:\n    \"\"\"Load clicks data with robust timestamp parsing; auto-generate if missing.\"\"\"\n    if not os.path.exists(path):\n        print(f\"Clicks file not found at {path}. Generating a synthetic sample for demonstration.\")\n        return create_synthetic_clicks(path)\n    df = pd.read_csv(path)\n    ts_col = detect_timestamp_column(df)\n    article_col = detect_article_column(df)\n    df[ts_col] = pd.to_datetime(df[ts_col])\n    required_cols = {\"user_id\", article_col, ts_col}\n    if not required_cols.issubset(df.columns):\n        raise ValueError(f\"Missing columns. Found {df.columns}, expected at least {required_cols}\")\n    df = df.rename(columns={ts_col: \"timestamp\", article_col: \"article_id\"})\n    df = df[[\"user_id\", \"article_id\", \"timestamp\"]]\n    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n    return df\n\n\ndef load_metadata(path: str) -> Optional[pd.DataFrame]:\n    \"\"\"Load article metadata if available.\"\"\"\n    if not os.path.exists(path):\n        print(f\"Metadata file not found at {path}. Falling back to co-visitation content model.\")\n        return None\n    meta = pd.read_csv(path)\n    if \"article_id\" not in meta.columns:\n        print(\"Metadata missing 'article_id' column. Ignoring metadata.\")\n        return None\n    return meta\n\n\nclicks = load_clicks(CONFIG[\"clicks_path\"])\nmetadata = load_metadata(CONFIG[\"metadata_path\"])\nprint(clicks.head())\nprint(\"Metadata loaded:\", metadata is not None)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks file not found at data/clicks_sample.csv. Generating a synthetic sample for demonstration.\n",
      "Synthetic clicks dataset created at data/clicks_sample.csv (users=50, items=120, interactions=1250)\n",
      "Metadata file not found at data/articles_metadata.csv. Falling back to co-visitation content model.\n",
      "   user_id  article_id  timestamp\n",
      "0        6          58 2022-01-01\n",
      "1       17          11 2022-01-01\n",
      "2       17          82 2022-01-01\n",
      "3       38          15 2022-01-01\n",
      "4        7          28 2022-01-01\n",
      "Metadata loaded: False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "a26424a2",
   "metadata": {},
   "source": [
    "## Protocole\n",
    "\n",
    "1. Tri des interactions par horodatage.\n",
    "2. Split temporel train/test selon `train_ratio`.\n",
    "3. Profil utilisateur: interactions de train.\n",
    "4. Ground truth: articles cliqués en test pour chaque utilisateur (au moins 1).\n",
    "5. Recommandations Top-5 en excluant les articles déjà vus en train.\n",
    "6. Calcul des métriques de ranking (Precision@5, Recall@5, MAP@5, NDCG@5, Coverage@5) et estimation de la latence moyenne sur un échantillon de 500 utilisateurs max."
   ]
  },
  {
   "cell_type": "code",
   "id": "a99221f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.687399Z",
     "start_time": "2025-12-15T16:16:52.663752Z"
    }
   },
   "source": [
    "\n",
    "# Split and utility functions\n",
    "\n",
    "def temporal_train_test_split(df: pd.DataFrame, train_ratio: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split interactions chronologically according to the train_ratio.\"\"\"\n",
    "    cutoff = int(len(df) * train_ratio)\n",
    "    train = df.iloc[:cutoff].copy()\n",
    "    test = df.iloc[cutoff:].copy()\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def build_user_histories(df: pd.DataFrame) -> Dict[int, List[int]]:\n",
    "    \"\"\"Create mapping user -> list of articles in chronological order.\"\"\"\n",
    "    histories: Dict[int, List[int]] = {}\n",
    "    for user_id, group in df.groupby(\"user_id\"):\n",
    "        histories[int(user_id)] = group.sort_values(\"timestamp\")[\"article_id\"].tolist()\n",
    "    return histories\n",
    "\n",
    "\n",
    "def get_candidate_items(df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Return unique article ids.\"\"\"\n",
    "    return df[\"article_id\"].unique().tolist()\n",
    "\n",
    "\n",
    "def make_ground_truth(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[Dict[int, List[int]], Dict[int, List[int]]]:\n",
    "    \"\"\"Build user histories and ground truth for evaluation.\"\"\"\n",
    "    train_hist = build_user_histories(train)\n",
    "    test_hist = build_user_histories(test)\n",
    "    eligible_users = {u: items for u, items in test_hist.items() if u in train_hist and len(items) > 0}\n",
    "    return train_hist, eligible_users\n",
    "\n",
    "\n",
    "train_df, test_df = temporal_train_test_split(clicks, CONFIG[\"train_ratio\"])\n",
    "train_histories, ground_truth = make_ground_truth(train_df, test_df)\n",
    "candidate_items = get_candidate_items(train_df)\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}, Users for eval: {len(ground_truth)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1000, Test size: 250, Users for eval: 50\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7127e6bb",
   "metadata": {},
   "source": [
    "## Modèles évalués\n",
    "\n",
    "* **Baseline A** : popularité globale (Top-K des articles les plus cliqués en train).\n",
    "* **Baseline B** : popularité récente (Top-K sur les N derniers jours de train).\n",
    "* **Modèle C** : item-to-item basé sur le contenu si disponible, sinon co-visitation (co-occurrence dans l'historique).\n",
    "* **Modèle D** : filtrage collaboratif implicite (factorisation via SVD sur matrice user-item binaire)."
   ]
  },
  {
   "cell_type": "code",
   "id": "2bb3c598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.730761Z",
     "start_time": "2025-12-15T16:16:52.724261Z"
    }
   },
   "source": [
    "\n",
    "# Metrics\n",
    "\n",
    "def precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Precision@k for a single user.\"\"\"\n",
    "    if not recommended:\n",
    "        return 0.0\n",
    "    rec_k = recommended[:k]\n",
    "    hits = len(set(rec_k) & set(relevant))\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Recall@k for a single user.\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    rec_k = recommended[:k]\n",
    "    hits = len(set(rec_k) & set(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "\n",
    "def average_precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"MAP@k for a single user.\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k)\n",
    "\n",
    "\n",
    "def dcg_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Discounted cumulative gain.\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            dcg += 1 / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    \"\"\"Normalized DCG.\"\"\"\n",
    "    ideal_dcg = dcg_at_k(relevant[:k], relevant, k)\n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(recommended, relevant, k) / ideal_dcg\n",
    "\n",
    "\n",
    "def coverage_at_k(all_recommendations: List[List[int]], candidate_items: List[int], k: int) -> float:\n",
    "    \"\"\"Coverage of unique recommended items over candidates.\"\"\"\n",
    "    rec_items = set()\n",
    "    for rec in all_recommendations:\n",
    "        rec_items.update(rec[:k])\n",
    "    if not candidate_items:\n",
    "        return 0.0\n",
    "    return len(rec_items) / len(candidate_items)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "82f04b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.793877Z",
     "start_time": "2025-12-15T16:16:52.782098Z"
    }
   },
   "source": [
    "\n",
    "# Recommenders\n",
    "\n",
    "def build_global_popularity(train: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Return items sorted by global click counts.\"\"\"\n",
    "    return train.groupby(\"article_id\").size().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "def build_recent_popularity(train: pd.DataFrame, window_days: int) -> List[int]:\n",
    "    \"\"\"Return popular items over the last window_days of training data.\"\"\"\n",
    "    max_time = train[\"timestamp\"].max()\n",
    "    window_start = max_time - pd.Timedelta(days=window_days)\n",
    "    recent = train[train[\"timestamp\"] >= window_start]\n",
    "    return recent.groupby(\"article_id\").size().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "def build_covisit_graph(train: pd.DataFrame) -> Dict[int, Dict[int, int]]:\n",
    "    \"\"\"Build co-visitation counts based on user histories.\"\"\"\n",
    "    graph: Dict[int, Dict[int, int]] = {}\n",
    "    for _, group in train.groupby(\"user_id\"):\n",
    "        items = group.sort_values(\"timestamp\")[\"article_id\"].tolist()\n",
    "        unique_items = list(dict.fromkeys(items))\n",
    "        for i, item_i in enumerate(unique_items):\n",
    "            graph.setdefault(item_i, {})\n",
    "            for item_j in unique_items[i + 1 :]:\n",
    "                graph[item_i][item_j] = graph[item_i].get(item_j, 0) + 1\n",
    "                graph.setdefault(item_j, {})\n",
    "                graph[item_j][item_i] = graph[item_j].get(item_i, 0) + 1\n",
    "    return graph\n",
    "\n",
    "\n",
    "def build_content_embeddings(metadata: pd.DataFrame, pca_components: Optional[int] = None):\n",
    "    \"\"\"Create TF-IDF embeddings from textual columns with optional PCA reduction.\"\"\"\n",
    "    text_cols = [c for c in metadata.columns if metadata[c].dtype == object and c != \"article_id\"]\n",
    "    if not text_cols:\n",
    "        raise ValueError(\"No textual columns in metadata\")\n",
    "    corpus = metadata[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    if pca_components and pca_components < tfidf.shape[1]:\n",
    "        svd = TruncatedSVD(n_components=pca_components, random_state=CONFIG[\"random_seed\"])\n",
    "        reduced = svd.fit_transform(tfidf)\n",
    "        embeddings = normalize(reduced)\n",
    "    else:\n",
    "        embeddings = normalize(tfidf)\n",
    "    ids = metadata[\"article_id\"].tolist()\n",
    "    return embeddings, ids\n",
    "\n",
    "\n",
    "def build_item_similarity(train: pd.DataFrame, metadata: Optional[pd.DataFrame]):\n",
    "    \"\"\"Build item-to-item similarity either from content or co-visitation.\"\"\"\n",
    "    if metadata is not None:\n",
    "        try:\n",
    "            embeddings, ids = build_content_embeddings(metadata, CONFIG[\"content_pca_components\"])\n",
    "            similarity: Dict[int, Dict[int, float]] = {}\n",
    "            for i, aid in enumerate(ids):\n",
    "                sims = embeddings @ embeddings[i].T\n",
    "                sims = np.asarray(sims).flatten()\n",
    "                top_idx = np.argsort(-sims)[1:51]\n",
    "                similarity[aid] = {ids[j]: float(sims[j]) for j in top_idx if sims[j] > 0}\n",
    "            return similarity, \"content\"\n",
    "        except Exception as exc:\n",
    "            print(f\"Content embeddings failed ({exc}). Falling back to co-visitation.\")\n",
    "    graph = build_covisit_graph(train)\n",
    "    similarity = {item: {nbr: float(cnt) for nbr, cnt in neigh.items()} for item, neigh in graph.items()}\n",
    "    return similarity, \"covisitation\"\n",
    "\n",
    "\n",
    "def recommend_from_similarity(user_id: int, train_histories: Dict[int, List[int]], similarity: Dict[int, Dict[int, float]], candidate_items: List[int], k: int) -> List[int]:\n",
    "    \"\"\"Aggregate similarity scores from user's history.\"\"\"\n",
    "    seen = set(train_histories.get(user_id, []))\n",
    "    scores: Dict[int, float] = {}\n",
    "    for item in seen:\n",
    "        for neighbor, sim in similarity.get(item, {}).items():\n",
    "            if neighbor in seen:\n",
    "                continue\n",
    "            scores[neighbor] = scores.get(neighbor, 0.0) + sim\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    recs = [it for it, _ in ranked if it not in seen]\n",
    "    if len(recs) < k:\n",
    "        for c in candidate_items:\n",
    "            if c not in seen and c not in recs:\n",
    "                recs.append(c)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "    return recs[:k]\n",
    "\n",
    "\n",
    "def build_collaborative_svd(train: pd.DataFrame, n_components: int):\n",
    "    \"\"\"Train a simple implicit SVD recommender returning a recommend function.\"\"\"\n",
    "    users = train[\"user_id\"].unique().tolist()\n",
    "    items = train[\"article_id\"].unique().tolist()\n",
    "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_to_idx = {it: i for i, it in enumerate(items)}\n",
    "\n",
    "    rows = [user_to_idx[u] for u in train[\"user_id\"]]\n",
    "    cols = [item_to_idx[it] for it in train[\"article_id\"]]\n",
    "    data = np.ones(len(rows))\n",
    "    mat = sparse.coo_matrix((data, (rows, cols)), shape=(len(users), len(items))).tocsr()\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=CONFIG[\"random_seed\"])\n",
    "    user_factors = svd.fit_transform(mat)\n",
    "    item_factors = svd.components_.T\n",
    "\n",
    "    user_norm = normalize(user_factors)\n",
    "    item_norm = normalize(item_factors)\n",
    "\n",
    "    def recommend(user_id: int, seen: set, k: int) -> List[int]:\n",
    "        if user_id not in user_to_idx:\n",
    "            popularity = build_global_popularity(train)\n",
    "            return [it for it in popularity if it not in seen][:k]\n",
    "        u_vec = user_norm[user_to_idx[user_id]]\n",
    "        scores = item_norm @ u_vec\n",
    "        ranked_items = [items[i] for i in np.argsort(-scores)]\n",
    "        return [it for it in ranked_items if it not in seen][:k]\n",
    "\n",
    "    meta = {\"users\": len(users), \"items\": len(items), \"components\": n_components}\n",
    "    return recommend, meta\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a2a633fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.847635Z",
     "start_time": "2025-12-15T16:16:52.841560Z"
    }
   },
   "source": [
    "\n",
    "# Evaluation pipeline\n",
    "\n",
    "def evaluate_model(\n",
    "    name: str,\n",
    "    recommend_func: Callable[[int, set, int], List[int]],\n",
    "    train_histories: Dict[int, List[int]],\n",
    "    ground_truth: Dict[int, List[int]],\n",
    "    candidate_items: List[int],\n",
    "    k: int,\n",
    "    latency_sample: int = 500,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a recommender with ranking metrics and latency estimation.\"\"\"\n",
    "    precisions: List[float] = []\n",
    "    recalls: List[float] = []\n",
    "    maps: List[float] = []\n",
    "    ndcgs: List[float] = []\n",
    "    all_recs: List[List[int]] = []\n",
    "\n",
    "    users = list(ground_truth.keys())\n",
    "    for user_id in users:\n",
    "        seen = set(train_histories.get(user_id, []))\n",
    "        recs = recommend_func(user_id, seen, k)\n",
    "        gt = ground_truth[user_id]\n",
    "        all_recs.append(recs)\n",
    "        precisions.append(precision_at_k(recs, gt, k))\n",
    "        recalls.append(recall_at_k(recs, gt, k))\n",
    "        maps.append(average_precision_at_k(recs, gt, k))\n",
    "        ndcgs.append(ndcg_at_k(recs, gt, k))\n",
    "\n",
    "    coverage = coverage_at_k(all_recs, candidate_items, k)\n",
    "\n",
    "    sample_users = users[: min(latency_sample, len(users))]\n",
    "    start = time.perf_counter()\n",
    "    for user_id in sample_users:\n",
    "        seen = set(train_histories.get(user_id, []))\n",
    "        _ = recommend_func(user_id, seen, k)\n",
    "    latency = (time.perf_counter() - start) / max(1, len(sample_users))\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"users\": len(users),\n",
    "        \"precision@k\": float(np.mean(precisions)),\n",
    "        \"recall@k\": float(np.mean(recalls)),\n",
    "        \"map@k\": float(np.mean(maps)),\n",
    "        \"ndcg@k\": float(np.mean(ndcgs)),\n",
    "        \"coverage@k\": coverage,\n",
    "        \"latency_per_user_s\": latency,\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "5a4ab146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:52.962189Z",
     "start_time": "2025-12-15T16:16:52.911981Z"
    }
   },
   "source": [
    "\n",
    "# Train recommenders\n",
    "K = CONFIG[\"k\"]\n",
    "\n",
    "popularity_rank = build_global_popularity(train_df)\n",
    "\n",
    "def popularity_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return [it for it in popularity_rank if it not in seen][:k]\n",
    "\n",
    "recent_rank = build_recent_popularity(train_df, CONFIG[\"recent_window_days\"])\n",
    "\n",
    "def recent_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return [it for it in recent_rank if it not in seen][:k]\n",
    "\n",
    "item_similarity, sim_mode = build_item_similarity(train_df, metadata)\n",
    "\n",
    "def content_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return recommend_from_similarity(user_id, train_histories, item_similarity, candidate_items, k)\n",
    "\n",
    "collab_recommend, collab_meta = build_collaborative_svd(train_df, CONFIG[\"svd_components\"])\n",
    "\n",
    "def collaborative_recommender(user_id: int, seen: set, k: int) -> List[int]:\n",
    "    return collab_recommend(user_id, seen, k)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b4694e82",
   "metadata": {},
   "source": [
    "## Résultats\n",
    "\n",
    "Les métriques sont calculées sur les utilisateurs présents en test avec au moins un clic et un historique en train."
   ]
  },
  {
   "cell_type": "code",
   "id": "4fce6efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:53.023341Z",
     "start_time": "2025-12-15T16:16:52.982209Z"
    }
   },
   "source": [
    "\n",
    "results = []\n",
    "results.append(evaluate_model(\"Baseline A - Popularité globale\", popularity_recommender, train_histories, ground_truth, candidate_items, K))\n",
    "results.append(evaluate_model(f\"Baseline B - Popularité {CONFIG['recent_window_days']}j\", recent_recommender, train_histories, ground_truth, candidate_items, K))\n",
    "results.append(evaluate_model(f\"Modèle C - Item2Item ({sim_mode})\", content_recommender, train_histories, ground_truth, candidate_items, K))\n",
    "results.append(evaluate_model(\"Modèle D - Collaborative SVD\", collaborative_recommender, train_histories, ground_truth, candidate_items, K))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values([\"ndcg@k\", \"map@k\"], ascending=False).reset_index(drop=True)\n",
    "print(results_df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 model  users  precision@k  recall@k  \\\n",
      "0      Baseline A - Popularité globale     50        0.048  0.045357   \n",
      "1  Modèle C - Item2Item (covisitation)     50        0.048  0.038103   \n",
      "2           Baseline B - Popularité 7j     50        0.044  0.042579   \n",
      "3         Modèle D - Collaborative SVD     50        0.024  0.023079   \n",
      "\n",
      "      map@k    ndcg@k  coverage@k  latency_per_user_s  \n",
      "0  0.028633  0.053961    0.083333            0.000007  \n",
      "1  0.021067  0.044166    0.258333            0.000220  \n",
      "2  0.019967  0.044085    0.091667            0.000006  \n",
      "3  0.016667  0.029087    0.783333            0.000027  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "2deac6ae",
   "metadata": {},
   "source": [
    "## Analyse & choix du modèle MVP\n",
    "\n",
    "Comparer les scores NDCG@5, MAP@5, couverture et latence pour sélectionner le meilleur compromis pour un déploiement Azure Functions. La justification est stockée dans `artifacts/evaluation/model_choice.md`."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbe168bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:53.046716Z",
     "start_time": "2025-12-15T16:16:53.033662Z"
    }
   },
   "source": [
    "\n",
    "best_row = results_df.iloc[0]\n",
    "justification = f\"\"\"\n",
    "## Choix du modèle MVP\n",
    "\n",
    "Modèle retenu : **{best_row['model']}**\n",
    "\n",
    "Motifs principaux :\n",
    "- NDCG@5 = {best_row['ndcg@k']:.4f}, MAP@5 = {best_row['map@k']:.4f}, Precision@5 = {best_row['precision@k']:.4f}, Recall@5 = {best_row['recall@k']:.4f}\n",
    "- Couverture = {best_row['coverage@k']:.4f} sur {len(candidate_items)} articles candidats.\n",
    "- Latence moyenne par utilisateur = {best_row['latency_per_user_s']:.6f} s (CPU).\n",
    "- Complexité : implémentation {('légère (contenu/co-visitation)' if 'Item2Item' in best_row['model'] else 'linéaire en dimensions SVD')} compatible avec Azure Functions.\n",
    "- Gestion du cold-start utilisateur via popularité globale.\n",
    "\n",
    "Note : ajuster `content_pca_components` pour réduire la taille des embeddings en production si nécessaire.\n",
    "\"\"\"\n",
    "choice_path = Path(CONFIG[\"artifacts_dir\"]) / \"model_choice.md\"\n",
    "choice_path.write_text(justification)\n",
    "print(justification)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Choix du modèle MVP\n",
      "\n",
      "Modèle retenu : **Baseline A - Popularité globale**\n",
      "\n",
      "Motifs principaux :\n",
      "- NDCG@5 = 0.0540, MAP@5 = 0.0286, Precision@5 = 0.0480, Recall@5 = 0.0454\n",
      "- Couverture = 0.0833 sur 120 articles candidats.\n",
      "- Latence moyenne par utilisateur = 0.000007 s (CPU).\n",
      "- Complexité : implémentation linéaire en dimensions SVD compatible avec Azure Functions.\n",
      "- Gestion du cold-start utilisateur via popularité globale.\n",
      "\n",
      "Note : ajuster `content_pca_components` pour réduire la taille des embeddings en production si nécessaire.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "33525096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:16:53.098167Z",
     "start_time": "2025-12-15T16:16:53.092438Z"
    }
   },
   "source": [
    "\n",
    "results_path_csv = Path(CONFIG[\"artifacts_dir\"]) / \"results.csv\"\n",
    "results_path_json = Path(CONFIG[\"artifacts_dir\"]) / \"results.json\"\n",
    "results_df.to_csv(results_path_csv, index=False)\n",
    "results_df.to_json(results_path_json, orient=\"records\", lines=True)\n",
    "print(f\"Résultats sauvegardés dans {results_path_csv} et {results_path_json}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sauvegardés dans artifacts/evaluation/results.csv et artifacts/evaluation/results.json\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
