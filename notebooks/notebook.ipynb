{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773debcd",
   "metadata": {},
   "source": [
    "# Évaluation d'un système de recommandation My Content\n",
    "\n",
    "Notebook pour entraîner et comparer plusieurs approches de recommandation sur le dataset Kaggle **news-portal-user-interactions-by-globocom**. L'objectif est de montrer clairement chaque étape (du chargement des données jusqu'au choix final du modèle).\n",
    "\n",
    "> Ce notebook aligne désormais **toutes les approches de recommandation sur la bibliothèque Surprise** (https://surprise.readthedocs.io/) afin de bénéficier d'algorithmes collaboratifs standardisés et faciles à déployer."
   ]
  },
  {
   "cell_type": "code",
   "id": "43aa8131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:53.682258Z",
     "iopub.status.busy": "2025-12-17T10:07:53.681987Z",
     "iopub.status.idle": "2025-12-17T10:07:55.146794Z",
     "shell.execute_reply": "2025-12-17T10:07:55.145979Z"
    }
   },
   "source": [
    "# Imports & Config\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Ensure the project root is importable\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"clicks_dir\": \"../data/news-portal-user-interactions-by-globocom/clicks\",\n",
    "    \"metadata_path\": \"../data/news-portal-user-interactions-by-globocom/articles_metadata.csv\",\n",
    "    \"embeddings_path\": \"../data/news-portal-user-interactions-by-globocom/articles_embeddings.pickle\",\n",
    "    \"max_click_files\": None,\n",
    "    \"artifacts_dir\": \"../artifacts/evaluation\",\n",
    "    \"k\": 5,\n",
    "    \"random_seed\": 42,\n",
    "    \"min_user_interactions\": 3,\n",
    "}\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "Path(CONFIG[\"artifacts_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Config ready\", CONFIG)\n",
    "\n",
    "# Context columns provided in the clicks dataset\n",
    "CONTEXT_COLUMNS = [\n",
    "    \"click_environment\",\n",
    "    \"click_deviceGroup\",\n",
    "    \"click_os\",\n",
    "    \"click_country\",\n",
    "    \"click_region\",\n",
    "    \"click_referrer_type\",\n",
    "]\n",
    "\n",
    "from surprise import Dataset, Reader, NormalPredictor, SVDpp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc6be6a7",
   "metadata": {},
   "source": [
    "## Contexte\n",
    "\n",
    "Nous voulons proposer à chaque lecteur un Top-5 d'articles susceptibles de l'intéresser. Le notebook illustre la démarche de A à Z : préparation des données, construction de différentes familles de modèles puis comparaison à l'aide de métriques de ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f8cfa",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Les fichiers attendus sont situés dans `/data/*`."
   ]
  },
  {
   "cell_type": "code",
   "id": "116743e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.150007Z",
     "iopub.status.busy": "2025-12-17T10:07:55.149650Z",
     "iopub.status.idle": "2025-12-17T10:07:55.178749Z",
     "shell.execute_reply": "2025-12-17T10:07:55.177740Z"
    }
   },
   "source": [
    "\n",
    "# Load data utilities\n",
    "\n",
    "\n",
    "def detect_timestamp_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the timestamp-like column name.\"\"\"\n",
    "    candidates = [\"click_timestamp\", \"timestamp\", \"event_time\", \"ts\", \"time\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates or col.lower() in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No timestamp-like column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def detect_article_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Detect the article/item column name.\"\"\"\n",
    "    candidates = [\"click_article_id\", \"clicked_article_id\", \"article_id\", \"item_id\", \"content_id\"]\n",
    "    for col in df.columns:\n",
    "        if col in candidates:\n",
    "            return col\n",
    "    raise ValueError(\"No article id column found. Expected one of: \" + \",\".join(candidates))\n",
    "\n",
    "\n",
    "def infer_unix_unit(values: pd.Series) -> str:\n",
    "    numeric = pd.to_numeric(values, errors=\"coerce\").dropna()\n",
    "    if numeric.empty:\n",
    "        return \"s\"\n",
    "    max_abs = numeric.abs().max()\n",
    "    if max_abs >= 1e14:\n",
    "        return \"ns\"\n",
    "    if max_abs >= 1e11:\n",
    "        return \"ms\"\n",
    "    return \"s\"\n",
    "\n",
    "\n",
    "def to_timestamp(series: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return pd.to_datetime(series)\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        unit = infer_unix_unit(series)\n",
    "        return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "    converted = pd.to_datetime(series, errors=\"coerce\")\n",
    "    if converted.notna().any():\n",
    "        return converted\n",
    "\n",
    "    unit = infer_unix_unit(series)\n",
    "    return pd.to_datetime(series, unit=unit, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def list_click_files(path: Union[str, Path]) -> List[Path]:\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.is_file():\n",
    "        return [path_obj]\n",
    "    if path_obj.is_dir():\n",
    "        return sorted(path_obj.glob(\"clicks_hour_*.csv\"))\n",
    "    return []\n",
    "\n",
    "\n",
    "def ensure_context_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure session_size and context columns exist with safe defaults.\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"session_size\" not in df.columns:\n",
    "        df[\"session_size\"] = 1\n",
    "    for col in CONTEXT_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_synthetic_clicks(path: str, n_users: int = 50, n_items: int = 120, days: int = 30, interactions_per_user: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"Create a small synthetic clicks dataset to keep the notebook runnable.\"\"\"\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "    start = pd.Timestamp(\"2022-01-01\")\n",
    "    envs = [\"web\", \"app\"]\n",
    "    devices = [\"mobile\", \"desktop\"]\n",
    "    oss = [\"ios\", \"android\", \"linux\"]\n",
    "    referrers = [\"direct\", \"search\", \"social\"]\n",
    "    records = []\n",
    "    for user in range(1, n_users + 1):\n",
    "        offsets = rng.integers(0, days, size=interactions_per_user)\n",
    "        timestamps = [start + pd.Timedelta(int(o), unit=\"D\") for o in sorted(offsets.tolist())]\n",
    "        articles = rng.integers(1, n_items + 1, size=interactions_per_user)\n",
    "        for ts, art in zip(timestamps, articles):\n",
    "            records.append({\n",
    "                \"user_id\": int(user),\n",
    "                \"article_id\": int(art),\n",
    "                \"timestamp\": ts,\n",
    "                \"session_size\": int(rng.integers(1, 6)),\n",
    "                \"click_environment\": rng.choice(envs),\n",
    "                \"click_deviceGroup\": rng.choice(devices),\n",
    "                \"click_os\": rng.choice(oss),\n",
    "                \"click_country\": rng.choice([\"fr\", \"us\", \"br\"]),\n",
    "                \"click_region\": rng.choice([\"idf\", \"sp\", \"ca\"]),\n",
    "                \"click_referrer_type\": rng.choice(referrers),\n",
    "            })\n",
    "    df = pd.DataFrame(records).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\n",
    "        f\"Synthetic clicks dataset created at {path} \"\n",
    "        f\"(users={n_users}, items={n_items}, interactions={len(df)})\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_clicks(path: str, max_files: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load clicks data from the Globo hourly files, with a safety cap.\"\"\"\n",
    "    files = list_click_files(path)\n",
    "    total_files = len(files)\n",
    "    if not files:\n",
    "        print(f\"Clicks directory not found at {path}. Generating a synthetic sample for demonstration.\")\n",
    "        return ensure_context_columns(create_synthetic_clicks(Path(path) / \"clicks_hour_000.csv\"))\n",
    "\n",
    "    if max_files is not None:\n",
    "        print(f\"Limite explicite max_files={max_files}, total détecté={total_files}\")\n",
    "        files = files[:max_files]\n",
    "\n",
    "    print(f\"Chargement de {len(files)} fichiers clicks (total détecté={total_files}, limite={max_files if max_files is not None else 'aucune'})\")\n",
    "    frames = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        ts_col = detect_timestamp_column(df)\n",
    "        article_col = detect_article_column(df)\n",
    "        df[ts_col] = to_timestamp(df[ts_col])\n",
    "        df = df.rename(columns={ts_col: \"timestamp\", article_col: \"article_id\"})\n",
    "        df = ensure_context_columns(df)\n",
    "        keep_cols = [col for col in [\n",
    "            \"user_id\",\n",
    "            \"article_id\",\n",
    "            \"timestamp\",\n",
    "            \"session_size\",\n",
    "            *CONTEXT_COLUMNS,\n",
    "        ] if col in df.columns]\n",
    "        frames.append(df[keep_cols])\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined = combined.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"Clicks agrégés : {len(combined)} lignes, {combined['user_id'].nunique()} utilisateurs uniques, {combined['article_id'].nunique()} articles uniques.\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_metadata(path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load article metadata if available.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Metadata file not found at {path}. Utilisation du pipeline Surprise uniquement si les métadonnées sont absentes.\")\n",
    "        return None\n",
    "    meta = pd.read_csv(path)\n",
    "    if \"article_id\" not in meta.columns:\n",
    "        print(\"Metadata missing 'article_id' column. Ignoring metadata.\")\n",
    "        return None\n",
    "    return meta\n",
    "\n",
    "\n",
    "clicks = load_clicks(CONFIG[\"clicks_dir\"], max_files=CONFIG[\"max_click_files\"])\n",
    "metadata = load_metadata(CONFIG[\"metadata_path\"])\n",
    "print(clicks.head())\n",
    "print(\"Metadata loaded:\", metadata is not None)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eab18f0f",
   "metadata": {},
   "source": [
    "## Analyse exploratoire des données\n",
    "\n",
    "Courte photographie des fichiers sources immédiatement après le chargement :\n",
    "- nombre de lignes et noms de colonnes des clics\n",
    "- volumes et intégrité des métadonnées articles\n",
    "- dimensions et structure du fichier d'`articles_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "id": "45108db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T10:07:55.182450Z",
     "iopub.status.busy": "2025-12-17T10:07:55.182242Z",
     "iopub.status.idle": "2025-12-17T10:07:55.260458Z",
     "shell.execute_reply": "2025-12-17T10:07:55.259678Z"
    }
   },
   "source": [
    "# EDA rapide sur les données sources\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "\n",
    "\n",
    "def summarize_timestamps(series: pd.Series):\n",
    "    series = pd.to_datetime(series)\n",
    "    daily = series.dt.date.value_counts().sort_index().rename_axis(\"date\").reset_index(name=\"nb_clicks\")\n",
    "    hourly = series.dt.hour.value_counts().sort_index().rename_axis(\"hour\").reset_index(name=\"nb_clicks\")\n",
    "    return series.min(), series.max(), daily, hourly\n",
    "\n",
    "\n",
    "def describe_structure(obj, prefix=\"embeddings\", max_depth=4):\n",
    "    entries = []\n",
    "\n",
    "    def add_entry(path, value, note=None):\n",
    "        entry = {\"chemin\": path, \"type\": type(value).__name__}\n",
    "        if hasattr(value, \"shape\"):\n",
    "            entry[\"shape\"] = tuple(getattr(value, \"shape\"))\n",
    "        elif hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)):\n",
    "            entry[\"len\"] = len(value)\n",
    "        if hasattr(value, \"dtype\"):\n",
    "            entry[\"dtype\"] = str(getattr(value, \"dtype\"))\n",
    "        if note:\n",
    "            entry[\"note\"] = note\n",
    "        if isinstance(value, np.ndarray) and value.dtype.names:\n",
    "            entry[\"dtype_fields\"] = list(value.dtype.names)\n",
    "        if isinstance(value, np.ndarray) and value.ndim == 1 and len(value) > 0 and not isinstance(value[0], (np.ndarray, list, tuple, Mapping)):\n",
    "            entry[\"exemple\"] = repr(value[:3].tolist())\n",
    "        entries.append(entry)\n",
    "\n",
    "    def walk(value, path, depth):\n",
    "        add_entry(path, value)\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "        if isinstance(value, Mapping):\n",
    "            for k, v in value.items():\n",
    "                walk(v, f\"{path}.{k}\", depth + 1)\n",
    "        elif isinstance(value, (list, tuple, np.ndarray)) and not isinstance(value, (str, bytes)):\n",
    "            if len(value) > 0:\n",
    "                walk(value[0], f\"{path}[0]\", depth + 1)\n",
    "\n",
    "    walk(obj, prefix, 0)\n",
    "    return entries\n",
    "\n",
    "\n",
    "click_files = list_click_files(CONFIG[\"clicks_dir\"])\n",
    "print(f\"Nombre total de fichiers clicks détectés: {len(click_files)}\")\n",
    "if not click_files:\n",
    "    print(\"Aucun fichier clicks trouvé au chemin configuré. Vérifiez le téléchargement des données.\")\n",
    "\n",
    "files_for_eda = click_files[:2]\n",
    "per_file_stats = []\n",
    "for file in files_for_eda:\n",
    "    df_file = pd.read_csv(file)\n",
    "    ts_col = detect_timestamp_column(df_file)\n",
    "    article_col = detect_article_column(df_file)\n",
    "    timestamps = to_timestamp(df_file[ts_col])\n",
    "    per_file_stats.append(\n",
    "        {\n",
    "            \"fichier\": file.name,\n",
    "            \"nb_lignes\": len(df_file),\n",
    "            \"colonnes\": \", \".join(df_file.columns),\n",
    "            \"articles_uniques\": df_file[article_col].nunique(),\n",
    "            \"horodatage_min\": timestamps.min(),\n",
    "            \"horodatage_max\": timestamps.max(),\n",
    "        }\n",
    "    )\n",
    "if per_file_stats:\n",
    "    display(pd.DataFrame(per_file_stats))\n",
    "else:\n",
    "    print(\"Pas assez de fichiers pour réaliser une EDA détaillée par fichier.\")\n",
    "\n",
    "print(\"=== Clicks (agrégés) ===\")\n",
    "if clicks.empty:\n",
    "    print(\"Aucun clic chargé. Vérifier le chemin ou augmenter max_click_files.\")\n",
    "else:\n",
    "    clicks_summary = {\n",
    "        \"nb_lignes\": len(clicks),\n",
    "        \"colonnes\": \", \".join(clicks.columns),\n",
    "        \"utilisateurs_uniques\": clicks['user_id'].nunique() if 'user_id' in clicks else None,\n",
    "        \"articles_uniques\": clicks['article_id'].nunique() if 'article_id' in clicks else None,\n",
    "    }\n",
    "    display(pd.DataFrame([clicks_summary]))\n",
    "\n",
    "    total_articles = None\n",
    "    if metadata is not None and 'article_id' in metadata:\n",
    "        total_articles = metadata['article_id'].nunique()\n",
    "    elif 'article_id' in clicks:\n",
    "        total_articles = clicks['article_id'].nunique()\n",
    "\n",
    "    total_clients = clicks['user_id'].nunique() if 'user_id' in clicks else None\n",
    "    print(\"Synthèse globale (articles / clients)\")\n",
    "    display(pd.DataFrame([{\n",
    "        'nombre_total_articles': total_articles,\n",
    "        'nombre_total_clients': total_clients,\n",
    "    }]))\n",
    "\n",
    "    ts_min, ts_max, daily, hourly = summarize_timestamps(clicks['timestamp'])\n",
    "    display(pd.DataFrame([\n",
    "        {\n",
    "            'horodatage_min': ts_min,\n",
    "            'horodatage_max': ts_max,\n",
    "            'fenetre_jours': (ts_max - ts_min).days + 1,\n",
    "        }\n",
    "    ]))\n",
    "    print(\"Répartition par jour (jusqu'à 10 premières valeurs)\")\n",
    "    display(daily.head(10))\n",
    "    print(\"Répartition par heure (0-23)\")\n",
    "    display(hourly)\n",
    "\n",
    "print(\"=== Métadonnées des articles ===\")\n",
    "if metadata is None:\n",
    "    print(\"Aucun fichier metadata chargé.\")\n",
    "else:\n",
    "    meta_summary = {\n",
    "        \"nb_articles\": len(metadata),\n",
    "        \"colonnes\": \", \".join(metadata.columns),\n",
    "        \"articles_uniques\": metadata['article_id'].nunique() if 'article_id' in metadata else None,\n",
    "    }\n",
    "    display(pd.DataFrame([meta_summary]))\n",
    "    missing = metadata.isna().sum().sort_values(ascending=False)\n",
    "    display(missing.to_frame('valeurs_manquantes'))\n",
    "    if 'created_at_ts' in metadata.columns:\n",
    "        created = to_timestamp(metadata['created_at_ts'])\n",
    "        display(pd.DataFrame([{'premier_article': created.min(), 'dernier_article': created.max()}]))\n",
    "    if 'article_id' in metadata.columns:\n",
    "        overlap = set(clicks['article_id'].unique()) if 'article_id' in clicks.columns else set()\n",
    "        coverage = len(overlap & set(metadata['article_id'].unique()))\n",
    "        print(f\"Articles présents dans clicks et metadata: {coverage}\")\n",
    "\n",
    "\n",
    "print(\"=== Embeddings d'articles ===\")\n",
    "embeddings_path = Path(CONFIG['embeddings_path'])\n",
    "if embeddings_path.exists():\n",
    "    with embeddings_path.open('rb') as f:\n",
    "        embeddings_obj = pickle.load(f)\n",
    "    print(f\"Type chargé: {type(embeddings_obj)}\")\n",
    "\n",
    "    def summarize_matrix(mat):\n",
    "        stats = {\n",
    "            'shape': getattr(mat, 'shape', None),\n",
    "            'dtype': getattr(mat, 'dtype', None),\n",
    "        }\n",
    "\n",
    "        dim_values = []\n",
    "        shape = getattr(mat, 'shape', None)\n",
    "        if shape is not None and len(shape) >= 2:\n",
    "            dim_values.append(shape[1])\n",
    "        elif isinstance(mat, (list, tuple, np.ndarray)):\n",
    "            for row in mat:\n",
    "                if hasattr(row, '__len__') and not isinstance(row, (str, bytes)):\n",
    "                    try:\n",
    "                        dim_values.append(len(row))\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "\n",
    "        if dim_values:\n",
    "            stats.update({\n",
    "                'profondeur_min': min(dim_values),\n",
    "                'profondeur_moyenne': float(np.mean(dim_values)),\n",
    "                'profondeur_max': max(dim_values),\n",
    "            })\n",
    "\n",
    "        if hasattr(mat, 'shape') and len(getattr(mat, 'shape', [])) == 2:\n",
    "            norms = np.linalg.norm(mat, axis=1)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'nb_vectors': mat.shape[0],\n",
    "                    'dim': mat.shape[1],\n",
    "                    'norm_min': norms.min(),\n",
    "                    'norm_max': norms.max(),\n",
    "                    'norm_moyenne': norms.mean(),\n",
    "                }\n",
    "            )\n",
    "        return stats\n",
    "\n",
    "    base_structure = describe_structure(embeddings_obj, max_depth=4)\n",
    "\n",
    "    if isinstance(embeddings_obj, dict):\n",
    "        keys = list(embeddings_obj.keys())\n",
    "        print(f\"Clés disponibles: {keys}\")\n",
    "        matrix = embeddings_obj.get('embeddings')\n",
    "        ids = embeddings_obj.get('articles_ids') or embeddings_obj.get('article_ids')\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_ids',\n",
    "                'type': type(ids).__name__,\n",
    "                'len': len(ids),\n",
    "                'note': \"Identifiants d'articles fournis dans le fichier\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if matrix is not None:\n",
    "            stats = summarize_matrix(matrix)\n",
    "            stats.update(\n",
    "                {\n",
    "                    'colonnes': \", \".join(keys),\n",
    "                    'nb_articles_ids': len(ids) if ids is not None else None,\n",
    "                    'ids_uniques': len(set(ids)) if ids is not None else None,\n",
    "                    'couverture_metadata': len(set(ids) & set(metadata['article_id']))\n",
    "                    if (metadata is not None and ids is not None and 'article_id' in metadata)\n",
    "                    else None,\n",
    "                    'couverture_clicks': len(set(ids) & set(clicks['article_id']))\n",
    "                    if (not clicks.empty and ids is not None and 'article_id' in clicks)\n",
    "                    else None,\n",
    "                }\n",
    "            )\n",
    "            display(pd.DataFrame([stats]))\n",
    "\n",
    "            if ids is not None:\n",
    "                sample_ids = ids[:5] if len(ids) >= 5 else ids\n",
    "                print(\"Aperçu des premiers article_id liés aux embeddings:\")\n",
    "                display(pd.DataFrame({'article_id': sample_ids}))\n",
    "\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, matrix.shape[1] if hasattr(matrix, 'shape') else 0))]\n",
    "            if preview_cols:\n",
    "                preview = pd.DataFrame(matrix[:5, : len(preview_cols)], columns=preview_cols)\n",
    "                if ids is not None:\n",
    "                    preview.insert(0, 'article_id', ids[: len(preview)])\n",
    "                print(\"Aperçu des embeddings (quelques colonnes et premières lignes):\")\n",
    "                display(preview)\n",
    "                print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "                print(\", \".join(preview.columns))\n",
    "\n",
    "                if ids is not None and metadata is not None and 'article_id' in metadata:\n",
    "                    meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                    meta_sample = (\n",
    "                        preview[['article_id']]\n",
    "                        .merge(metadata[['article_id'] + meta_cols], on='article_id', how='left')\n",
    "                    )\n",
    "                    if 'created_at_ts' in meta_sample.columns:\n",
    "                        meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                    print(\"Exemple de liaison embedding -> metadata sur article_id (5 premières lignes):\")\n",
    "                    display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Aucune matrice d'embeddings explicite trouvée dans l'objet chargé.\")\n",
    "    elif hasattr(embeddings_obj, 'shape'):\n",
    "        stats = summarize_matrix(embeddings_obj)\n",
    "\n",
    "        inferred_ids = None\n",
    "        mapping_note = None\n",
    "        if metadata is not None and 'article_id' in metadata and hasattr(embeddings_obj, 'shape'):\n",
    "            if embeddings_obj.shape[0] == len(metadata):\n",
    "                inferred_ids = metadata['article_id'].reset_index(drop=True)\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id explicite fourni ; association supposée alignée sur l'ordre des metadata.\"\n",
    "                )\n",
    "            else:\n",
    "                mapping_note = (\n",
    "                    \"Aucun article_id dans le fichier d'embeddings et la taille ne correspond pas aux metadata : \"\n",
    "                    f\"{embeddings_obj.shape[0]} vecteurs vs {len(metadata)} lignes de metadata.\"\n",
    "                )\n",
    "        else:\n",
    "            mapping_note = (\n",
    "                \"Aucun identifiant d'article n'est présent dans le fichier d'embeddings (mapping externe requis).\"\n",
    "            )\n",
    "\n",
    "        structure = base_structure.copy()\n",
    "        if inferred_ids is not None:\n",
    "            structure.insert(0, {\n",
    "                'chemin': 'embeddings.article_id (inféré)',\n",
    "                'type': type(inferred_ids).__name__,\n",
    "                'len': len(inferred_ids),\n",
    "                'note': \"Alignement supposé sur metadata.article_id (index identique).\",\n",
    "            })\n",
    "        if structure:\n",
    "            print(\"Structure détaillée de l'objet d'embeddings (par chemin de clé):\")\n",
    "            display(pd.DataFrame(structure))\n",
    "\n",
    "        if mapping_note:\n",
    "            print(mapping_note)\n",
    "\n",
    "        if inferred_ids is not None:\n",
    "            stats.update(\n",
    "                {\n",
    "                    'ids_source': 'metadata.article_id (alignement par index)',\n",
    "                    'ids_uniques': inferred_ids.nunique(),\n",
    "                    'couverture_metadata': len(set(inferred_ids) & set(metadata['article_id'])),\n",
    "                    'couverture_clicks': len(set(inferred_ids) & set(clicks['article_id'])) if not clicks.empty else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        display(pd.DataFrame([stats]))\n",
    "        if len(getattr(embeddings_obj, 'shape', [])) >= 2 and embeddings_obj.shape[1] > 0:\n",
    "            preview_cols = [f\"emb_{i}\" for i in range(min(5, embeddings_obj.shape[1]))]\n",
    "            preview = pd.DataFrame(embeddings_obj[:5, : len(preview_cols)], columns=preview_cols)\n",
    "            if inferred_ids is not None:\n",
    "                preview.insert(0, 'article_id', inferred_ids.iloc[: len(preview)].values)\n",
    "            print(\"Aperçu direct de la matrice d'embeddings:\")\n",
    "            display(preview)\n",
    "            print(\"Colonnes affichées pour l'aperçu des embeddings:\")\n",
    "            print(\", \".join(preview.columns))\n",
    "\n",
    "            if inferred_ids is not None and metadata is not None:\n",
    "                meta_cols = [c for c in ['title', 'category_id', 'created_at_ts', 'publisher'] if c in metadata.columns]\n",
    "                meta_sample = preview[['article_id']].merge(\n",
    "                    metadata[['article_id'] + meta_cols], on='article_id', how='left'\n",
    "                )\n",
    "                if 'created_at_ts' in meta_sample.columns:\n",
    "                    meta_sample['created_at_ts'] = to_timestamp(meta_sample['created_at_ts'])\n",
    "                print(\"Exemple de liaison embedding -> metadata sur article_id (inféré):\")\n",
    "                display(meta_sample.head())\n",
    "        else:\n",
    "            print(\"Objet chargé non structuré, utilisez type/len pour investiguer.\")\n",
    "else:\n",
    "    print(f\"Fichier d'embeddings introuvable à {embeddings_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bac08d578ddc9073",
   "metadata": {},
   "source": [
    "# Article Embeddings\n",
    "\n",
    "Ce fichier contient les **embeddings des articles**, c’est-à-dire une **représentation numérique du contenu textuel** permettant de comparer les articles entre eux sur le plan sémantique.\n",
    "\n",
    "* **Format** : matrice NumPy `(N, 250)` en `float32`\n",
    "* **1 ligne = 1 article**\n",
    "* **250 colonnes = dimensions latentes**\n",
    "* Les valeurs individuelles n’ont pas de signification directe\n",
    "\n",
    "L’`article_id` n’est **pas stocké explicitement** : il est **déduit de l’ordre des lignes**, qui doit rester aligné avec les métadonnées des articles.\n",
    "\n",
    "La variable `words_count` indique le **nombre de mots du texte source** et sert uniquement d’indicateur de qualité du contenu.\n",
    "\n",
    "Les embeddings **ne sont pas normalisés** : la **similarité cosinus** est la mesure recommandée pour comparer les articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données et split temporel\n",
    "\n",
    "Les splits sont réalisés **par utilisateur**, en conservant l'ordre chronologique :\n",
    "- historique utilisateur ordonné\n",
    "- 1 interaction la plus récente pour le test\n",
    "- 1 interaction juste avant pour la validation\n",
    "- le reste pour l'entraînement\n",
    "\n",
    "Cette stratégie garantit l'absence de fuite d'information tout en restant robuste pour des historiques courts.\n"
   ],
   "id": "c856514aa2797fb1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data preparation utilities\n",
    "\n",
    "def clean_interactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"user_id\"] = pd.to_numeric(df[\"user_id\"], errors=\"coerce\")\n",
    "    df[\"article_id\"] = pd.to_numeric(df[\"article_id\"], errors=\"coerce\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"user_id\", \"article_id\", \"timestamp\"])\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(int)\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_user_histories(df: pd.DataFrame) -> Dict[int, List[int]]:\n",
    "    return (\n",
    "        df.sort_values([\"user_id\", \"timestamp\"])\n",
    "        .groupby(\"user_id\")[\"article_id\"]\n",
    "        .apply(list)\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "def temporal_split_per_user(df: pd.DataFrame, min_interactions: int = 3) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_rows = []\n",
    "    val_rows = []\n",
    "    test_rows = []\n",
    "    for user_id, group in df.groupby(\"user_id\"):\n",
    "        group = group.sort_values(\"timestamp\")\n",
    "        if len(group) < min_interactions:\n",
    "            continue\n",
    "        test_rows.append(group.iloc[-1])\n",
    "        val_rows.append(group.iloc[-2])\n",
    "        if len(group) > 2:\n",
    "            train_rows.append(group.iloc[:-2])\n",
    "    train_df = pd.concat(train_rows, ignore_index=True) if train_rows else pd.DataFrame(columns=df.columns)\n",
    "    val_df = pd.DataFrame(val_rows) if val_rows else pd.DataFrame(columns=df.columns)\n",
    "    test_df = pd.DataFrame(test_rows) if test_rows else pd.DataFrame(columns=df.columns)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "clicks_clean = clean_interactions(clicks)\n",
    "user_histories = build_user_histories(clicks_clean)\n",
    "train_df, val_df, test_df = temporal_split_per_user(clicks_clean, CONFIG[\"min_user_interactions\"])\n",
    "\n",
    "print(f\"Interactions totales après nettoyage: {len(clicks_clean)}\")\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Candidate items are derived from training data only\n",
    "candidate_items = sorted(train_df[\"article_id\"].unique().tolist())\n",
    "print(f\"Catalog (train only): {len(candidate_items)} items\")\n",
    "\n",
    "# Leakage checks\n",
    "def assert_no_leakage(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Ensure temporal ordering per user without forbidding repeated items.\"\"\"\n",
    "    latest_train = train_df.groupby(\"user_id\")[\"timestamp\"].max()\n",
    "    earliest_val = val_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "    earliest_test = test_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "    for user_id in latest_train.index:\n",
    "        if user_id in earliest_val.index:\n",
    "            assert latest_train[user_id] <= earliest_val[user_id], \"Temporal leakage train->val\"\n",
    "        if user_id in earliest_test.index:\n",
    "            assert latest_train[user_id] <= earliest_test[user_id], \"Temporal leakage train->test\"\n",
    "\n",
    "    if not val_df.empty and not test_df.empty:\n",
    "        latest_val = val_df.groupby(\"user_id\")[\"timestamp\"].max()\n",
    "        earliest_test = test_df.groupby(\"user_id\")[\"timestamp\"].min()\n",
    "        for user_id in latest_val.index:\n",
    "            if user_id in earliest_test.index:\n",
    "                assert latest_val[user_id] <= earliest_test[user_id], \"Temporal leakage val->test\"\n",
    "\n",
    "assert_no_leakage(train_df, val_df, test_df)\n",
    "\n",
    "train_user_items = train_df.groupby(\"user_id\")[\"article_id\"].apply(set).to_dict()\n",
    "\n",
    "print(f\"Users in train: {len(train_user_items)}\")\n",
    "\n",
    "display(train_df.head())\n"
   ],
   "id": "78caefc0d42e539a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles et métriques de ranking\n",
    "\n",
    "Les modèles sont entraînés **indépendamment** et évalués sur le même jeu de candidats (articles vus en train).\n",
    "Pour le modèle avec pondération de session, nous transformons le signal implicite en :\n",
    "`rating = rating_base + log1p(session_size)` afin de renforcer légèrement les sessions plus longues.\n",
    "\n",
    "Le **rating_base** est défini comme `1 + 0.01 * rang_normalisé` (rang temporel par utilisateur),\n",
    "ce qui introduit une légère variance nécessaire au calcul des prédictions sans changer la logique implicite.\n",
    "\n",
    "**Note** : les modèles SVD++ exploitent à la fois les notes implicites et l'historique d'interactions\n",
    "pour apprendre des facteurs latents (pas de similarité de contenu textuel direct).\n",
    "\n",
    "---\n"
   ],
   "id": "f40da61d7585693"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Modeling and evaluation\n",
    "\n",
    "\n",
    "def build_surprise_trainset(df: pd.DataFrame, rating_col: str) -> Tuple[Dataset, object]:\n",
    "    rating_min = float(df[rating_col].min())\n",
    "    rating_max = float(df[rating_col].max())\n",
    "    reader = Reader(rating_scale=(rating_min, rating_max))\n",
    "    data = Dataset.load_from_df(df[[\"user_id\", \"article_id\", rating_col]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    return data, trainset\n",
    "\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Callable, Optional\n",
    "import heapq\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Candidate generation\n",
    "# ----------------------------\n",
    "TOP_N_CANDIDATES = 5000  # tune: 2_000 / 5_000 / 10_000 / 20_000\n",
    "popular_candidates: List[int] = (\n",
    "    train_df.groupby(\"article_id\")\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(TOP_N_CANDIDATES)\n",
    "    .index.astype(int)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "if not popular_candidates:\n",
    "    popular_candidates = train_df[\"article_id\"].dropna().astype(int).unique().tolist()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Fast recommenders\n",
    "# ----------------------------\n",
    "\n",
    "def recommend_random(rng: np.random.Generator, user_id: int, k: int) -> List[int]:\n",
    "    seen = train_user_items.get(int(user_id), set())\n",
    "    candidates = [item for item in popular_candidates if item not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "    rng.shuffle(candidates)\n",
    "    return candidates[:k]\n",
    "\n",
    "\n",
    "def recommend_surprise_fast(model: Any, user_id: int, k: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Score only a limited candidate pool (top-N popular items) to avoid O(all_items).\n",
    "    Uses heapq.nlargest to avoid sorting the whole candidate list.\n",
    "    \"\"\"\n",
    "    seen = train_user_items.get(int(user_id), set())\n",
    "    candidates = [i for i in popular_candidates if i not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    scored: List[Tuple[int, float]] = []\n",
    "    for item_id in candidates:\n",
    "        est = float(model.predict(int(user_id), int(item_id)).est)\n",
    "        scored.append((int(item_id), est))\n",
    "\n",
    "    topk = heapq.nlargest(k, scored, key=lambda t: t[1])\n",
    "    return [item_id for item_id, _ in topk]\n",
    "\n",
    "\n",
    "def recommend_surprise(model: Any, user_id: int, k: int) -> List[int]:\n",
    "    return recommend_surprise_fast(model, user_id, k)\n",
    "\n",
    "\n",
    "class HybridSvdppKnnModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        svdpp_weighted_model,\n",
    "        knn_model,\n",
    "        svdpp_weight: float = 0.6,\n",
    "        knn_weight: float = 0.4,\n",
    "    ) -> None:\n",
    "        self.svdpp_weighted_model = svdpp_weighted_model\n",
    "        self.knn_model = knn_model\n",
    "        self.svdpp_weight = svdpp_weight\n",
    "        self.knn_weight = knn_weight\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        prediction = self.svdpp_weighted_model.predict(user_id, item_id)\n",
    "        knn_score = self.knn_model.predict(user_id, item_id).est\n",
    "        blended_score = self.svdpp_weight * prediction.est + self.knn_weight * knn_score\n",
    "        return prediction._replace(est=blended_score)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Metrics\n",
    "# ----------------------------\n",
    "\n",
    "def precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not recommended:\n",
    "        return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for item in recommended_k if item in relevant_set)\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum(1 for item in recommended_k if item in relevant_set)\n",
    "    return hits / len(relevant_set)\n",
    "\n",
    "\n",
    "def average_precision_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    relevant_set = set(relevant)\n",
    "    ap = 0.0\n",
    "    hits = 0\n",
    "    for idx, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant_set:\n",
    "            hits += 1\n",
    "            ap += hits / idx\n",
    "    return ap / min(len(relevant_set), k)\n",
    "\n",
    "\n",
    "def ndcg_at_k(recommended: List[int], relevant: List[int], k: int) -> float:\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    relevant_set = set(relevant)\n",
    "    dcg = 0.0\n",
    "    for idx, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant_set:\n",
    "            dcg += 1.0 / np.log2(idx + 1)\n",
    "    ideal_hits = min(len(relevant_set), k)\n",
    "    idcg = sum(1.0 / np.log2(idx + 1) for idx in range(1, ideal_hits + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def rmse_from_predictions(predictions: List[float], targets: List[float]) -> float:\n",
    "    if not predictions:\n",
    "        return 0.0\n",
    "    errors = np.subtract(np.array(predictions, dtype=float), np.array(targets, dtype=float))\n",
    "    return float(np.sqrt(np.mean(errors ** 2)))\n",
    "\n",
    "\n",
    "def score_rmse(\n",
    "    predict_fn: Callable[[int, int], float],\n",
    "    eval_df: pd.DataFrame,\n",
    "    rating_col: str = \"rating\",\n",
    ") -> float:\n",
    "    if eval_df.empty or rating_col not in eval_df.columns:\n",
    "        return 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for row in eval_df.itertuples(index=False):\n",
    "        predictions.append(float(predict_fn(int(row.user_id), int(row.article_id))))\n",
    "        targets.append(float(getattr(row, rating_col)))\n",
    "    return rmse_from_predictions(predictions, targets)\n",
    "\n",
    "\n",
    "def make_predict_fn(model: Any) -> Callable[[int, int], float]:\n",
    "    def _predict(user_id: int, item_id: int) -> float:\n",
    "        prediction = model.predict(int(user_id), int(item_id))\n",
    "        return float(getattr(prediction, \"est\", prediction))\n",
    "\n",
    "    return _predict\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Parallel evaluation helpers\n",
    "# ----------------------------\n",
    "\n",
    "_EVAL_RECOMMEND_FN = None\n",
    "_EVAL_USER_RELEVANT = None\n",
    "_EVAL_K = None\n",
    "\n",
    "\n",
    "def _set_eval_globals(recommend_fn, user_relevant, k):\n",
    "    global _EVAL_RECOMMEND_FN, _EVAL_USER_RELEVANT, _EVAL_K\n",
    "    _EVAL_RECOMMEND_FN = recommend_fn\n",
    "    _EVAL_USER_RELEVANT = user_relevant\n",
    "    _EVAL_K = k\n",
    "\n",
    "\n",
    "def _evaluate_user(user_id: int):\n",
    "    relevant = _EVAL_USER_RELEVANT.get(int(user_id), [])\n",
    "    user_start = time.perf_counter()\n",
    "    recommended = _EVAL_RECOMMEND_FN(int(user_id), int(_EVAL_K))\n",
    "    latency = time.perf_counter() - user_start\n",
    "    return (\n",
    "        {\n",
    "            \"precision\": precision_at_k(recommended, relevant, _EVAL_K),\n",
    "            \"recall\": recall_at_k(recommended, relevant, _EVAL_K),\n",
    "            \"map\": average_precision_at_k(recommended, relevant, _EVAL_K),\n",
    "            \"ndcg\": ndcg_at_k(recommended, relevant, _EVAL_K),\n",
    "            \"latency\": float(latency),\n",
    "        },\n",
    "        recommended,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model_name: str,\n",
    "    recommend_fn: Callable[[int, int], List[int]],\n",
    "    eval_df: pd.DataFrame,\n",
    "    k: int,\n",
    "    predict_fn: Optional[Callable[[int, int], float]] = None,\n",
    "    rating_col: str = \"rating\",\n",
    "    *,\n",
    "    log_every: int = 2000,\n",
    "    show_progress: bool = True,\n",
    "    coverage_denominator: Optional[int] = None,\n",
    "    parallel: bool = True,\n",
    "    n_jobs: Optional[int] = 9,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Retrocompatible replacement of your evaluate_model:\n",
    "    - Same required signature and returned dict keys.\n",
    "    - Adds optional progress logging (disabled by show_progress=False).\n",
    "    - Allows overriding coverage denominator (defaults to len(popular_candidates) if available).\n",
    "    - Enables multi-core evaluation when supported (parallel=True, n_jobs=None uses all cores).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    per_user_metrics: List[Dict[str, float]] = []\n",
    "    all_recommended: List[int] = []\n",
    "\n",
    "    users = eval_df[\"user_id\"].dropna().astype(int).unique().tolist()\n",
    "    n_users = len(users)\n",
    "\n",
    "    if coverage_denominator is None:\n",
    "        coverage_denominator = max(len(popular_candidates), 1)\n",
    "\n",
    "    if show_progress:\n",
    "        print(f\"▶ Evaluating: {model_name} | users={n_users} | k={k}\", flush=True)\n",
    "\n",
    "\n",
    "    rmse = score_rmse(predict_fn, eval_df, rating_col) if predict_fn is not None else 0.0\n",
    "\n",
    "    user_relevant = (\n",
    "        eval_df.groupby(\"user_id\")[\"article_id\"]\n",
    "        .apply(lambda s: s.dropna().astype(int).tolist())\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    use_parallel = parallel and n_users > 1\n",
    "    ctx = None\n",
    "    if use_parallel:\n",
    "        available_methods = mp.get_all_start_methods()\n",
    "        if \"fork\" in available_methods:\n",
    "            ctx = mp.get_context(\"fork\")\n",
    "        else:\n",
    "            use_parallel = False\n",
    "            if show_progress:\n",
    "                print(\"⚠️ Multiprocessing fork not available; falling back to sequential evaluation.\")\n",
    "\n",
    "    if use_parallel and ctx is not None:\n",
    "        if n_jobs is None:\n",
    "            n_jobs = ctx.cpu_count()\n",
    "        n_jobs = max(1, min(int(n_jobs), n_users))\n",
    "        chunksize = max(1, n_users // (n_jobs * 4))\n",
    "\n",
    "        _set_eval_globals(recommend_fn, user_relevant, k)\n",
    "        with ctx.Pool(\n",
    "            processes=n_jobs,\n",
    "        ) as pool:\n",
    "            for idx, (metrics, recommended) in enumerate(\n",
    "                pool.imap_unordered(_evaluate_user, users, chunksize=chunksize), start=1\n",
    "            ):\n",
    "                per_user_metrics.append(metrics)\n",
    "                all_recommended.extend(recommended)\n",
    "\n",
    "                if show_progress and (\n",
    "                    idx == 1 or idx == n_users or (log_every > 0 and idx % log_every == 0)\n",
    "                ):\n",
    "                    elapsed = time.perf_counter() - start_time\n",
    "                    avg_per_user = elapsed / max(idx, 1)\n",
    "                    eta = avg_per_user * (n_users - idx)\n",
    "                    print(\n",
    "                        f\"[{model_name}] {idx}/{n_users} ({idx/n_users:.1%}) \"\n",
    "                        f\"elapsed={elapsed/60:.1f}min ETA={eta/60:.1f}min \"\n",
    "                        f\"avg_latency={avg_per_user:.4f}s/user\",\n",
    "                        flush=True,\n",
    "                    )\n",
    "    else:\n",
    "        for idx, user_id in enumerate(users, start=1):\n",
    "            relevant = user_relevant.get(int(user_id), [])\n",
    "\n",
    "            user_start = time.perf_counter()\n",
    "            recommended = recommend_fn(int(user_id), int(k))\n",
    "            latency = time.perf_counter() - user_start\n",
    "\n",
    "            all_recommended.extend(recommended)\n",
    "\n",
    "            per_user_metrics.append(\n",
    "                {\n",
    "                    \"precision\": precision_at_k(recommended, relevant, k),\n",
    "                    \"recall\": recall_at_k(recommended, relevant, k),\n",
    "                    \"map\": average_precision_at_k(recommended, relevant, k),\n",
    "                    \"ndcg\": ndcg_at_k(recommended, relevant, k),\n",
    "                    \"latency\": float(latency),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if show_progress and (\n",
    "                idx == 1 or idx == n_users or (log_every > 0 and idx % log_every == 0)\n",
    "            ):\n",
    "                elapsed = time.perf_counter() - start_time\n",
    "                avg_per_user = elapsed / max(idx, 1)\n",
    "                eta = avg_per_user * (n_users - idx)\n",
    "                print(\n",
    "                    f\"[{model_name}] {idx}/{n_users} ({idx/n_users:.1%}) \"\n",
    "                    f\"elapsed={elapsed/60:.1f}min ETA={eta/60:.1f}min \"\n",
    "                    f\"avg_latency={avg_per_user:.4f}s/user\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "\n",
    "    if not per_user_metrics:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"precision@k\": 0.0,\n",
    "            \"recall@k\": 0.0,\n",
    "            \"map@k\": 0.0,\n",
    "            \"ndcg@k\": 0.0,\n",
    "            \"coverage@k\": 0.0,\n",
    "            \"rmse\": float(rmse),\n",
    "            \"latency_per_user_s\": 0.0,\n",
    "            \"total_eval_time_s\": float(total_time),\n",
    "        }\n",
    "\n",
    "    metrics_df = pd.DataFrame(per_user_metrics)\n",
    "    coverage = len(set(all_recommended)) / max(int(coverage_denominator), 1)\n",
    "\n",
    "    if show_progress:\n",
    "        print(\n",
    "            f\"✔ Done: {model_name} in {total_time/60:.1f}min | \"\n",
    "            f\"mean_latency={metrics_df['latency'].mean():.4f}s/user | \"\n",
    "            f\"coverage={coverage:.4f}\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"precision@k\": float(metrics_df[\"precision\"].mean()),\n",
    "        \"recall@k\": float(metrics_df[\"recall\"].mean()),\n",
    "        \"map@k\": float(metrics_df[\"map\"].mean()),\n",
    "        \"ndcg@k\": float(metrics_df[\"ndcg\"].mean()),\n",
    "        \"coverage@k\": float(coverage),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"latency_per_user_s\": float(metrics_df[\"latency\"].mean()),\n",
    "        \"total_eval_time_s\": float(total_time),\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare implicit ratings\n",
    "train_df = train_df.copy()\n",
    "train_df[\"interaction_rank\"] = train_df.groupby(\"user_id\").cumcount()\n",
    "max_rank = train_df[\"interaction_rank\"].max()\n",
    "if max_rank > 0:\n",
    "    train_df[\"rating\"] = 1.0 + 0.01 * (train_df[\"interaction_rank\"] / max_rank)\n",
    "else:\n",
    "    train_df[\"rating\"] = 1.0\n",
    "train_df[\"rating_weighted\"] = train_df[\"rating\"] + np.log1p(train_df[\"session_size\"].astype(float))\n",
    "\n",
    "\n",
    "# Ratings for RMSE on held-out interactions\n",
    "test_df = test_df.copy()\n",
    "test_df[\"rating\"] = 1.0\n",
    "rating_min = float(train_df[\"rating\"].min())\n",
    "rating_max = float(train_df[\"rating\"].max())\n",
    "random_rmse_rng = np.random.default_rng(CONFIG[\"random_seed\"] + 1)\n",
    "random_predict_fn = lambda user_id, item_id: float(random_rmse_rng.uniform(rating_min, rating_max))\n",
    "\n",
    "_, trainset_basic = build_surprise_trainset(train_df, rating_col=\"rating\")\n",
    "_, trainset_weighted = build_surprise_trainset(train_df, rating_col=\"rating_weighted\")\n",
    "\n",
    "# Models\n",
    "rng = np.random.default_rng(CONFIG[\"random_seed\"])\n"
   ],
   "id": "a4db9eac36403577",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train NormalPredictor baseline\n",
    "normal_model = NormalPredictor()\n",
    "normal_model.fit(trainset_basic)\n"
   ],
   "id": "32b98c6f3759c0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train content-based TF-IDF cosine model\n",
    "from collections import namedtuple\n",
    "\n",
    "articles_df = metadata.copy() if metadata is not None else pd.DataFrame(columns=[\"article_id\"])\n",
    "if \"article_id\" not in articles_df.columns:\n",
    "    articles_df[\"article_id\"] = pd.Series(dtype=int)\n",
    "\n",
    "text_columns = [col for col in [\"title\", \"text\", \"keywords\"] if col in articles_df.columns]\n",
    "if text_columns:\n",
    "    articles_df[\"content_text\"] = (\n",
    "        articles_df[text_columns]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "else:\n",
    "    articles_df[\"content_text\"] = \"\"\n",
    "\n",
    "articles_df = articles_df[articles_df[\"article_id\"].isin(candidate_items)].drop_duplicates(\"article_id\")\n",
    "articles_df = articles_df.reset_index(drop=True)\n",
    "\n",
    "if articles_df.empty or articles_df[\"content_text\"].str.strip().eq(\"\").all():\n",
    "    item_vectors = np.zeros((len(articles_df), 1), dtype=float)\n",
    "else:\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "    item_tfidf = tfidf_vectorizer.fit_transform(articles_df[\"content_text\"])\n",
    "\n",
    "    if item_tfidf.shape[1] > 2:\n",
    "        n_components = min(256, item_tfidf.shape[1] - 1)\n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=CONFIG[\"random_seed\"])\n",
    "        item_vectors = svd.fit_transform(item_tfidf)\n",
    "    else:\n",
    "        item_vectors = item_tfidf.toarray()\n",
    "\n",
    "item_vectors = normalize(item_vectors, norm=\"l2\")\n",
    "item_ids = articles_df[\"article_id\"].astype(int).tolist()\n",
    "item_index = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "user_profiles = {}\n",
    "for user_id, group in train_df.groupby(\"user_id\"):\n",
    "    clicked_ids = [item for item in group[\"article_id\"].tolist() if item in item_index]\n",
    "    if not clicked_ids:\n",
    "        continue\n",
    "    vectors = item_vectors[[item_index[item] for item in clicked_ids]]\n",
    "    mean_vector = vectors.mean(axis=0, keepdims=True)\n",
    "    user_profiles[int(user_id)] = normalize(mean_vector, norm=\"l2\")[0]\n",
    "\n",
    "popular_items = train_df[\"article_id\"].value_counts().index.tolist()\n",
    "\n",
    "Prediction = namedtuple(\"Prediction\", [\"est\"])\n",
    "\n",
    "\n",
    "class ContentBasedModel:\n",
    "    def __init__(self, user_profiles, item_vectors, item_index):\n",
    "        self.user_profiles = user_profiles\n",
    "        self.item_vectors = item_vectors\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def score(self, user_id, item_id) -> float:\n",
    "        user_vector = self.user_profiles.get(int(user_id))\n",
    "        idx = self.item_index.get(int(item_id))\n",
    "        if user_vector is None or idx is None:\n",
    "            return 0.0\n",
    "        return float(self.item_vectors[idx].dot(user_vector))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        return Prediction(est=self.score(user_id, item_id))\n",
    "\n",
    "\n",
    "content_model = ContentBasedModel(user_profiles, item_vectors, item_index)\n",
    "\n",
    "\n",
    "def recommend_content_based(user_id: int, k: int) -> List[int]:\n",
    "    seen = train_user_items.get(user_id, set())\n",
    "    user_vector = user_profiles.get(user_id)\n",
    "    if user_vector is None or not item_index:\n",
    "        return [item for item in popular_items if item not in seen][:k]\n",
    "\n",
    "    candidate_ids = [item for item in item_ids if item not in seen]\n",
    "    if not candidate_ids:\n",
    "        return []\n",
    "\n",
    "    candidate_idx = [item_index[item] for item in candidate_ids]\n",
    "    scores = item_vectors[candidate_idx].dot(user_vector)\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    return [candidate_ids[i] for i in top_idx]\n"
   ],
   "id": "1e51bc31ec92816e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train SVD++ collaborative filtering model\n",
    "svdpp_model = SVDpp(random_state=CONFIG[\"random_seed\"])\n",
    "svdpp_model.fit(trainset_basic)\n"
   ],
   "id": "aa7ea9cc0096e36e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train SVD++ model with session weighting\n",
    "svdpp_weighted_model = SVDpp(random_state=CONFIG[\"random_seed\"])\n",
    "svdpp_weighted_model.fit(trainset_weighted)"
   ],
   "id": "d4be10d435fdc09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"NormalPredictor (topN popular candidates)\",\n",
    "        lambda user_id, k: recommend_surprise_fast(normal_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "        predict_fn=make_predict_fn(normal_model),\n",
    "    )\n",
    ")"
   ],
   "id": "6e342ac8c299c3d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train hybrid model (SVD++ session-weighted + item-item KNN)\n",
    "hybrid_svdpp_knn_model = HybridSvdppKnnModel(\n",
    "    svdpp_weighted_model,\n",
    "    content_model,\n",
    "    svdpp_weight=0.6,\n",
    "    knn_weight=0.4,\n",
    ")"
   ],
   "id": "a55cbc59fcf9ce03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Random recommender\",\n",
    "        lambda user_id, k: recommend_random(rng, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "        predict_fn=random_predict_fn,\n",
    "    )\n",
    ")"
   ],
   "id": "bfe95f77e07d7c11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Content-Based (TF-IDF cosine)\",\n",
    "        lambda user_id, k: recommend_content_based(user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "        predict_fn=make_predict_fn(content_model),\n",
    "    )\n",
    ")\n"
   ],
   "id": "a20a4727cfdbada2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "if False:\n",
    "    results.append(\n",
    "        evaluate_model(\n",
    "            \"SVD++\",\n",
    "            lambda user_id, k: recommend_surprise(svdpp_model, user_id, k),\n",
    "            test_df,\n",
    "            CONFIG[\"k\"],\n",
    "            predict_fn=make_predict_fn(svdpp_model),\n",
    "        )\n",
    "    )"
   ],
   "id": "cb89ebdb8b24d3e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"SVD++ + session weighting\",\n",
    "        lambda user_id, k: recommend_surprise(svdpp_weighted_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "        predict_fn=make_predict_fn(svdpp_weighted_model),\n",
    "    )\n",
    ")"
   ],
   "id": "776567c3b7a5dde5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results.append(\n",
    "    evaluate_model(\n",
    "        \"Hybrid SVD++ (session weighting) + Content-Based (TF-IDF cosine)\",\n",
    "        lambda user_id, k: recommend_surprise(hybrid_svdpp_knn_model, user_id, k),\n",
    "        test_df,\n",
    "        CONFIG[\"k\"],\n",
    "        predict_fn=make_predict_fn(hybrid_svdpp_knn_model),\n",
    "    )\n",
    ")\n"
   ],
   "id": "38a81e0ec4f751bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(by=\"ndcg@k\", ascending=False))"
   ],
   "id": "752707c5fc3b6593",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétation rapide\n",
    "\n",
    "- **Random recommender** : sert de base sanity-check ; il ignore les préférences et affiche les métriques les plus faibles.\n",
    "- **NormalPredictor** : exploite la distribution globale des interactions, utile comme baseline non personnalisée mais limitée en couverture personnelle.\n",
    "- **KNNBasic cosine (item-item)** : capte des co-occurrences simples, efficace quand la popularité locale est un bon signal.\n",
    "- **SVD++** : modèle factoriel collaboratif qui combine notes implicites et historique d'interactions pour affiner les recommandations.\n",
    "- **SVD++ + session weighting** : renforce les sessions longues, ce qui peut mieux refléter l'engagement utilisateur pour le contexte news.\n",
    "\n",
    "---\n"
   ],
   "id": "e640b2f140ef5398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d08ada1b0ce34061",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
